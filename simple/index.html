<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>DataStream API - Quokka</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "DataStream API";
        var mkdocs_page_input_path = "simple.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Quokka
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../started/">Cartoons</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">Installation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cloud/">Setting Up Cloud Cluster</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../different/">How is Quokka different from ...?</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Tutorials</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">DataStream API</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#lesson-1-things">Lesson -1: Things</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lesson-0-reading-things">Lesson 0: Reading Things</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lesson-1-doing-things">Lesson 1: Doing Things</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lesson-2-writing-things">Lesson 2: Writing Things</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lesson-3-things-you-cant-do">Lesson 3: Things you can't do.</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tutorial/">TaskGraph API</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Dataframe API reference</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../quokka_context/">QuokkaContext</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../datastream/">DataStream</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Quokka</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Tutorials &raquo;</li>
      <li>DataStream API</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/marsupialtail/quokka/edit/master/docs/simple.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="tutorials">Tutorials</h1>
<p>This section is for learning how to use Quokka's DataStream API. <strong>Quokka's DataStream API is basically a dataframe API.</strong> It takes heavy inspiration from SparkSQL and Polars, and adopts a lazy execution model. This means that in contrast to Pandas, your operations are not executed immediately after you define them. Instead, Quokka builds a logical plan under the hood and executes it only when the user wants to "collect" the result, just like Spark. </p>
<p>For the first part of our tutorial, we are going to go through implementing a few SQL queries in the TPC-H benchmark suite. You can download the data <a href="https://drive.google.com/file/d/19hgYxZ4u28Cxe0s616Q3yAfkuRdQlmvO/view?usp=sharing">here</a>. It is about 1GB unzipped. Please download the data (should take 2 minutes) and extract it to some directory locally. If you are testing this on a VM where clicking the link can't work, try this command after pip installing gdown: <code>~/.local/bin/gdown https://drive.google.com/uc?id=19hgYxZ4u28Cxe0s616Q3yAfkuRdQlmvO</code>. The SQL queries themselves can be found on this awesome <a href="https://umbra-db.com/interface/">interface</a>.</p>
<p>These tutorials will use your local machine. They shouldn't take too long to run. It would be great if you can follow along, not just for fun -- <strong>if you find a bug in this tutorial I will buy you a cup of coffee!</strong></p>
<p>For an extensive API reference, please refer to <a href="../datastream/">here</a>.</p>
<h2 id="lesson-1-things">Lesson -1: Things</h2>
<p>Please read the <a href="../started/">Getting Started</a> section. I spent way too much time making the cartoons on that page.</p>
<h2 id="lesson-0-reading-things">Lesson 0: Reading Things</h2>
<p>For every Quokka program, we need to set up a <code>QuokkaContext</code> object. This is similar to the Spark <code>SQLContext</code>. This can easily be done by running the following two lines of code in your Python terminal.</p>
<pre><code class="language-python">from pyquokka.df import * 
qc = QuokkaContext()
</code></pre>
<p>Once we have the <code>QuokkaContext</code> object, we can start reading data to obtain DataStreams. Quokka can read data on disk and on the cloud (currently S3). For the purposes of this tutorial we will be reading data from disk. Quokka currently reads CSV and Parquet, with plans to add JSON soon. </p>
<p>Here is how you would read a CSV file <strong>if you know the schema</strong>:</p>
<pre><code class="language-python"># the last column is called NULL, because the TPC-H data generator likes to put a | at the end of each row, making it appear as if there is a final column
# with no values. Don't worry, we can drop this column. 
lineitem_scheme = [&quot;l_orderkey&quot;,&quot;l_partkey&quot;,&quot;l_suppkey&quot;,&quot;l_linenumber&quot;,&quot;l_quantity&quot;,&quot;l_extendedprice&quot;, &quot;l_discount&quot;,&quot;l_tax&quot;,&quot;l_returnflag&quot;,&quot;l_linestatus&quot;,&quot;l_shipdate&quot;,&quot;l_commitdate&quot;,&quot;l_receiptdate&quot;,&quot;l_shipinstruct&quot;,&quot;l_shipmode&quot;,&quot;l_comment&quot;, &quot;null&quot;]
lineitem = qc.read_csv(disk_path + &quot;lineitem.tbl&quot;, lineitem_scheme, sep=&quot;|&quot;)
</code></pre>
<p>And if you don't know the schema but there is a header row where column names are <strong>separated with the same separator as the data</strong>:</p>
<pre><code class="language-python">lineitem = qc.read_csv(disk_path + &quot;lineitem.tbl&quot;, sep=&quot;|&quot;, has_header=True)
</code></pre>
<p>The test files you just downloaded are of this form. No need to specify the schema for those. In this case Quokka will just use the header row for the schema.</p>
<p>You can also read a directory of CSV files:</p>
<pre><code class="language-python">lineitem = qc.read_csv(disk_path + &quot;lineitem/*&quot;, lineitem_scheme, sep=&quot;|&quot;, has_header = True)
</code></pre>
<p>Now let's read all the tables of the TPC-H benchmark suite. Set <code>disk_path</code> to where you unzipped the files.</p>
<pre><code class="language-python">lineitem = qc.read_csv(disk_path + &quot;lineitem.tbl&quot;, sep=&quot;|&quot;, has_header=True)
orders = qc.read_csv(disk_path + &quot;orders.tbl&quot;, sep=&quot;|&quot;, has_header=True)
customer = qc.read_csv(disk_path + &quot;customer.tbl&quot;,sep = &quot;|&quot;, has_header=True)
part = qc.read_csv(disk_path + &quot;part.tbl&quot;, sep = &quot;|&quot;, has_header=True)
supplier = qc.read_csv(disk_path + &quot;supplier.tbl&quot;, sep = &quot;|&quot;, has_header=True)
partsupp = qc.read_csv(disk_path + &quot;partsupp.tbl&quot;, sep = &quot;|&quot;, has_header=True)
nation = qc.read_csv(disk_path + &quot;nation.tbl&quot;, sep = &quot;|&quot;, has_header=True)
region = qc.read_csv(disk_path + &quot;region.tbl&quot;, sep = &quot;|&quot;, has_header=True)
</code></pre>
<p>If you want to read the Parquet files, you should first run this script to generate the Parquet files:</p>
<pre><code class="language-python">import polars as pl
disk_path = &quot;/home/ubuntu/tpc-h/&quot; #replace
files = [&quot;lineitem.tbl&quot;,&quot;orders.tbl&quot;,&quot;customer.tbl&quot;,&quot;part.tbl&quot;,&quot;supplier.tbl&quot;,&quot;partsupp.tbl&quot;,&quot;nation.tbl&quot;,&quot;region.tbl&quot;]
for file in files:
    df = pl.read_csv(disk_path + file,sep=&quot;|&quot;,has_header = True, parse_dates = True).drop(&quot;null&quot;)
    df.write_parquet(disk_path + file.replace(&quot;tbl&quot;, &quot;parquet&quot;), row_group_size=100000)
</code></pre>
<p>To read in a Parquet file, you don't have to worry about headers or schema, just do:</p>
<pre><code class="language-python">lineitem = qc.read_parquet(disk_path + &quot;lineitem.parquet&quot;)
</code></pre>
<p>Currently, <code>qc.read_csv</code> and <code>qc.read_parquet</code> will either return a DataStream or just a Polars DataFrame directly if the data size is small (set at 10 MB).</p>
<h2 id="lesson-1-doing-things">Lesson 1: Doing Things</h2>
<p>Now that we have read the data, let's do things with it. First, why don't we count how many rows there are in the <code>lineitem</code> table.</p>
<pre><code class="language-python">&gt;&gt;&gt; lineitem.count()
</code></pre>
<p>If you don't see the number 6001215 after a while, something is very wrong. Please send me an email, I will help you fix things (and buy you a coffee): zihengw@stanford.edu.</p>
<p>Feel free to type other random things and see if it's supported, but for those interested, let's follow a structured curriculum. Let's take a look at <a href="https://github.com/dragansah/tpch-dbgen/blob/master/tpch-queries/1.sql">TPC-H query 1</a>.</p>
<p>This is how you would write it in Quokka. This is very similar to how you'd write in another DataFrame library like Polars or Dask.</p>
<pre><code class="language-python">def do_1():
    d = lineitem.filter(&quot;l_shipdate &lt;= date '1998-12-01' - interval '90' day&quot;)
    d = d.with_column(&quot;disc_price&quot;, lambda x: x[&quot;l_extendedprice&quot;] * (1 - x[&quot;l_discount&quot;]), required_columns ={&quot;l_extendedprice&quot;, &quot;l_discount&quot;})
    d = d.with_column(&quot;charge&quot;, lambda x: x[&quot;l_extendedprice&quot;] * (1 - x[&quot;l_discount&quot;]) * (1 + x[&quot;l_tax&quot;]), required_columns={&quot;l_extendedprice&quot;, &quot;l_discount&quot;, &quot;l_tax&quot;})
    f = d.groupby([&quot;l_returnflag&quot;, &quot;l_linestatus&quot;], orderby=[&quot;l_returnflag&quot;,&quot;l_linestatus&quot;]).agg({&quot;l_quantity&quot;:[&quot;sum&quot;,&quot;avg&quot;], &quot;l_extendedprice&quot;:[&quot;sum&quot;,&quot;avg&quot;], &quot;disc_price&quot;:&quot;sum&quot;, &quot;charge&quot;:&quot;sum&quot;, &quot;l_discount&quot;:&quot;avg&quot;,&quot;*&quot;:&quot;count&quot;})
    return f.collect()
</code></pre>
<p>Quokka supports filtering DataStreams by <code>DataStream.filter()</code>. Filters can be specified in SQL syntax. The columns in the SQL expression must exist in the schema of the DataStream. A more Pythonic way of doing this like <code>b = b[b.a &lt; 5]</code> isn't supported yet, mainly due to the finickiness surrounding date types etc. The result of a <code>filter()</code> is another DataStream whose Polars DataFrames will only contain rows that respect the predicate.</p>
<p>On the plus side, Quokka uses the amazing <a href="https://github.com/tobymao/sqlglot">SQLGlot</a> library to support most ANSI-SQL compliant predicates, including dates, between, IN, even arithmetic in conditions. Try out some different <a href="../datastream/#filter">predicates</a>! Please give SQLGlot a star when you're at it. For example, you can specify this super complicated predicate for <a href="https://github.com/dragansah/tpch-dbgen/blob/master/tpch-queries/6.sql">TPC-H query 6</a>:</p>
<pre><code class="language-python">def do_6():
    d = lineitem.filter(&quot;l_shipdate &gt;= date '1994-01-01' and l_shipdate &lt; date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01 and l_quantity &lt; 24&quot;)
    d = d.with_column(&quot;revenue&quot;, lambda x: x[&quot;l_extendedprice&quot;] * x[&quot;l_discount&quot;], required_columns={&quot;l_extendedprice&quot;, &quot;l_discount&quot;})
    f = d.aggregate({&quot;revenue&quot;:[&quot;sum&quot;]})
    return f.collect()
</code></pre>
<p>Quokka supports creating new columns in DataStreams with <code>with_column</code>. Read more about how this works <a href="../datastream/#with_column">here</a>. This is in principle similar to Spark <code>df.with_column</code> and Pandas UDFs. The main thing to keep in mind is that the function you supply will be applied to each batch in the DataStream, instead of row by row. As a result, you can make use of fast vectorized execution with Polars. The mental model here is that we have a DataStream <code>d</code> of Polars DataFrames, each of which have rows from the lineitem table satisfying the filter predicate. Then, each Polars DataFrame is transformed by our functions to add the columns <code>disk_price</code> and <code>charge</code>. </p>
<p>Like most Quokka operations, <code>with_column</code> will produce a new DataStream with an added column and is not inplace. This means that the command is lazy, and won't trigger the runtime to produce the actual data. It simply builds a logical plan of what to do in the background, which can be optimized when the user specifically ask for the result.</p>
<p>Finally, we can group the DataStream and aggregate it to get the result. Read more about aggregation syntax <a href="../datastream/#agg">here</a>. The aggregation will produce another DataStream, which we call <code>collect()</code> on, to convert it to a Polars DataFrame in your Python terminal.</p>
<p>When you call <code>.collect()</code>, the logical plan you have built is actually optimized and executed. This is exactly how Spark works. To view the optimized logical plan and learn more about what Quokka is doing, you can do <code>f.explain()</code> which will produce a graph, or <code>f.explain(mode="text")</code> which will produce a textual explanation.</p>
<p>Joins work very intuitively. For example, this is how to do <a href="https://github.com/dragansah/tpch-dbgen/blob/master/tpch-queries/12.sql">TPC-H query 12</a>.</p>
<pre><code class="language-python">def do_12():
    d = lineitem.join(orders,left_on=&quot;l_orderkey&quot;, right_on=&quot;o_orderkey&quot;)
    d = d.filter(&quot;l_shipmode IN ('MAIL','SHIP') and l_commitdate &lt; l_receiptdate and l_shipdate &lt; l_commitdate and \
        l_receiptdate &gt;= date '1994-01-01' and l_receiptdate &lt; date '1995-01-01'&quot;)
    d = d.with_column(&quot;high&quot;, lambda x: (x[&quot;o_orderpriority&quot;] == &quot;1-URGENT&quot;) | (x[&quot;o_orderpriority&quot;] == &quot;2-HIGH&quot;), required_columns={&quot;o_orderpriority&quot;})
    d = d.with_column(&quot;low&quot;, lambda x: (x[&quot;o_orderpriority&quot;] != &quot;1-URGENT&quot;) &amp; (x[&quot;o_orderpriority&quot;] != &quot;2-HIGH&quot;), required_columns={&quot;o_orderpriority&quot;})
    f = d.groupby(&quot;l_shipmode&quot;).aggregate(aggregations={'high':['sum'], 'low':['sum']})
    return f.collect()
</code></pre>
<p>Note it does not matter if you filter after the join or before the join, Quokka will automatically push them down during the logical plan optimization. The <code>join</code> operator on a DataStream takes in either another DataStream or a Polars DataFrame in your Python session. In the latter case, this Polars DataFrame will be broadcasted to different workers similar to Spark's broadcast join. Here is another example, <a href="https://github.com/dragansah/tpch-dbgen/blob/master/tpch-queries/3.sql">TPC-H query 3</a>.</p>
<pre><code class="language-python">def do_3():
    d = lineitem.join(orders,left_on=&quot;l_orderkey&quot;, right_on=&quot;o_orderkey&quot;)
    d = customer.join(d,left_on=&quot;c_custkey&quot;, right_on=&quot;o_custkey&quot;)
    d = d.filter(&quot;c_mktsegment = 'BUILDING' and o_orderdate &lt; date '1995-03-15' and l_shipdate &gt; date '1995-03-15'&quot;)
    d = d.with_column(&quot;revenue&quot;, lambda x: x[&quot;l_extendedprice&quot;] * ( 1 - x[&quot;l_discount&quot;]) , required_columns={&quot;l_extendedprice&quot;, &quot;l_discount&quot;})
    f = d.groupby([&quot;l_orderkey&quot;,&quot;o_orderdate&quot;,&quot;o_shippriority&quot;]).agg({&quot;revenue&quot;:[&quot;sum&quot;]})
    return f.collect()
</code></pre>
<p>Note unlike some SQL engines, Quokka currently will not try to figure out the optimal join ordering between the specified three-way join between lineitem, orders and customer tables. You are responsible for figuring that out at the moment -- try to join smaller tables first and then join them against larger tables, or try to minimize the intermeidate result size from those joins.</p>
<p>An important thing to note is that Quokka currently only support inner joins. Other kinds of joins are coming soon.</p>
<p>Feel free to look at some other queries in the Quokka <a href="https://github.com/marsupialtail/quokka/tree/master/apps">github</a>, or browse the <a href="../datastream/">API reference</a>. While you are there, please give Quokka a star!</p>
<h2 id="lesson-2-writing-things">Lesson 2: Writing Things</h2>
<p>So far, we have just learned about how to read things into DataStreams and do things to DataStreams. You can also write out DataStreams to persistent storage like disk or S3 to record all the amazing things we did with them.</p>
<p>Quokka currently operates like Spark and by default writes a directory of files, with a default maximum file size for different file formats. This makes it easy to perform parallel writing.</p>
<p>To write out a DataStream to CSV or Parquet to a local directory (you must specify a valid absolute path), simply do:</p>
<pre><code class="language-python">d.write_csv(&quot;/home/ubuntu/test-path/&quot;)
d.write_parquet(&quot;/home/ubuntu/test-path/&quot;)
</code></pre>
<p>To write out a DataStream to S3, you should specify an S3 bucket and prefix like this:</p>
<pre><code class="language-python">d.write_csv(&quot;s3://bucket/prefix/&quot;)
d.write_parquet(&quot;s3://bucket/prefix/&quot;)
</code></pre>
<p>Writing out a DataStream is a blocking API and will automatically call a <code>collect()</code> for you. The collected Polars DataFrame at the end is just a column of filenames produced.</p>
<h2 id="lesson-3-things-you-cant-do">Lesson 3: Things you can't do.</h2>
<p>Here is a brief discussion of what Quokka is not great for. Quokka's main advantage stems from the fact it can pipeline the execution of DataStreams. Once a partition (typically a Polars DataFrame) in a DataStream has been generated, it can be immediately consumed by a downstream user. This means downstream processing of this partition and upstream generation of the next partition can be overlapped. </p>
<p>Now, if an operator processing a DataStream cannot emit any partitions downstream until it has seen all of the partitions in its input DataStreams, the pipeline breaks. An example of this is an aggregation. You cannot safely emit the result of a sum of a column of a table until you have seen every row! The main examples of this in data processing are groupby-aggregations and distributed sorts. </p>
<p>Currently, calling <code>groupby().agg()</code> or just <code>agg()</code> on a DataStream will produce another DataStream. However that DataStream will consist of exactly one batch, which holds the final result, emitted when it's computed. It is recommended to just call <code>collect()</code> or <code>compute()</code> on that result. </p>
<p>Quokka currently does not support distributed sort -- indeed a sort heavy workload is really great for Spark. Distributed sorting is not exactly needed for many analytical SQL workloads since you typically do the aggregation before the order by, which greatly reduce the number of rows you have to sort. You can then sort after you have done <code>collect()</code>. However for many other workloads distributed sorting is critical, and Quokka aims to support this as soon as possible.</p>
<p>Things that Quokka can do and doesn't do yet: fine grained placement of UDFs or UDAFs on GPUs or CPUs, core-count-control, Docker support, reading JSON, etc. Most of these can be easily implemented (and some already are) in the graph level API, however it takes effort to figure out what's the best abstractions to expose in the DataStream API. If you want to make this list shorter, I welcome contributions: zihengw@stanford.edu.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../different/" class="btn btn-neutral float-left" title="How is Quokka different from ...?"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tutorial/" class="btn btn-neutral float-right" title="TaskGraph API">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/marsupialtail/quokka" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../different/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tutorial/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
