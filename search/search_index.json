{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"If you like, please : Introduction Quokka is a lightweight distributed dataflow engine written completely in Python targeting modern data science use cases involving 100GBs to TBs of data. At its core, Quokka manipulates streams of data with stateful actors. Quokka offers a stream-centric, Python-native perspective to Spark. Instead of logical DataFrames where partitions are present across the cluster at the same time, Quokka operates on DataStreams , where partitions may be materialized in sequence. Please see the Getting Started for more details. Quokka's DataStream interface is highly inspired by the Polars API. One simple way to think about Quokka is distributed Polars on Ray . Quokka currently supports a large percentage of Polars APIs, though keeping up with Polars' rapid development is a challenge. Quokka also offers SQL analogs to some of the Polars APIs, e.g. filter_sql or with_columns_sql that are helpful to people with more of a SQL background. This streaming paradigm inspired by high performance databases such as DuckDB and Snowflake allows Quokka to greatly outperform Apache Spark performance on SQL type workloads reading from cloud storage for formats like CSV and Parquet. Fineprint: benchmark done with TPC-H Scale Factor 100 in Parquet format using four r6id.2xlarge instances for Quokka and EMR 6.8.0 with five r6id.2xlarge instances for Spark where one instance is used as a coordinator. Ignores the time to start up the SparkSQL or Quokka runtimes, which is comparable. Did not compare against Modin or Dask since they are typically much slower than SparkSQL. What's even better than being cheap and fast is the fact that since Quokka is Python native, you can easily use your favorite machine learning libraries like Scikit-Learn and Pytorch with Quokka inside of arbitrary Python functions to transform your DataStreams. Another great advantage is that a streaming data paradigm is more in line with how data arrives in the real world, making it easy to bridge your data application to production, or conduct time-series backfilling on your historical data. You develop with Quokka locally, and deploy to cloud (currently AWS) with a single line of code change. You can spin up a new cluster or connect to an existing Ray cluster. Kubernetes can be supported through setting up a Ray cluster with KubeRay . Like Spark, Quokka can recover from worker failures , but expects a reliable coordinator node that doesn't fail throughout the duration of the job. Like Spark, Quokka benefits from having plenty of RAM, and fast SSDs for spilling if needed. Roadmap Ordered data support. Quokka's goal is to become a TB-scale feature framework for time series data. Currently Quokka DataStreams are unordered, but that is going to change in the next few months! Expect to see blazing fast asof_join and window_transforms on TBs of time-series data. The operators have already been impelemented, mostly working out the high level API. If you are interested in this, please contact me. Better SQL support. Currently Quokka has very rudimentary SQL support in the form of qc.sql(SQL_QUERY). It will loudly complain if it sees some SQL that it doesn't understand, e.g. complicated nested subqueries etc. I am working with SQLGlot to enhancing SQL support. Target pass TPC-H and say 75% of TPC-DS Q2 2023. Bug Bounty If you see any bugs, please raise a Github issue. There is a $10 reward if you find a bug that I can't fix within a week. If you fix the bug yourself and make a PR, you will get another reward: $50 for a bug that results in a runtime error. $100 for a bug that results in a silent error, e.g. produces wrong results. I am a PhD student and have finite resources. So this program will be shut down after I lose $2000. Contact If you are interested in trying out Quokka or hit any problems at all, please contact me at zihengw@stanford.edu or Discord . I will try my best to make Quokka work for you.","title":"Home"},{"location":"#if-you-like-please","text":"","title":"If you like, please: "},{"location":"#introduction","text":"Quokka is a lightweight distributed dataflow engine written completely in Python targeting modern data science use cases involving 100GBs to TBs of data. At its core, Quokka manipulates streams of data with stateful actors. Quokka offers a stream-centric, Python-native perspective to Spark. Instead of logical DataFrames where partitions are present across the cluster at the same time, Quokka operates on DataStreams , where partitions may be materialized in sequence. Please see the Getting Started for more details. Quokka's DataStream interface is highly inspired by the Polars API. One simple way to think about Quokka is distributed Polars on Ray . Quokka currently supports a large percentage of Polars APIs, though keeping up with Polars' rapid development is a challenge. Quokka also offers SQL analogs to some of the Polars APIs, e.g. filter_sql or with_columns_sql that are helpful to people with more of a SQL background. This streaming paradigm inspired by high performance databases such as DuckDB and Snowflake allows Quokka to greatly outperform Apache Spark performance on SQL type workloads reading from cloud storage for formats like CSV and Parquet. Fineprint: benchmark done with TPC-H Scale Factor 100 in Parquet format using four r6id.2xlarge instances for Quokka and EMR 6.8.0 with five r6id.2xlarge instances for Spark where one instance is used as a coordinator. Ignores the time to start up the SparkSQL or Quokka runtimes, which is comparable. Did not compare against Modin or Dask since they are typically much slower than SparkSQL. What's even better than being cheap and fast is the fact that since Quokka is Python native, you can easily use your favorite machine learning libraries like Scikit-Learn and Pytorch with Quokka inside of arbitrary Python functions to transform your DataStreams. Another great advantage is that a streaming data paradigm is more in line with how data arrives in the real world, making it easy to bridge your data application to production, or conduct time-series backfilling on your historical data. You develop with Quokka locally, and deploy to cloud (currently AWS) with a single line of code change. You can spin up a new cluster or connect to an existing Ray cluster. Kubernetes can be supported through setting up a Ray cluster with KubeRay . Like Spark, Quokka can recover from worker failures , but expects a reliable coordinator node that doesn't fail throughout the duration of the job. Like Spark, Quokka benefits from having plenty of RAM, and fast SSDs for spilling if needed.","title":"Introduction"},{"location":"#roadmap","text":"Ordered data support. Quokka's goal is to become a TB-scale feature framework for time series data. Currently Quokka DataStreams are unordered, but that is going to change in the next few months! Expect to see blazing fast asof_join and window_transforms on TBs of time-series data. The operators have already been impelemented, mostly working out the high level API. If you are interested in this, please contact me. Better SQL support. Currently Quokka has very rudimentary SQL support in the form of qc.sql(SQL_QUERY). It will loudly complain if it sees some SQL that it doesn't understand, e.g. complicated nested subqueries etc. I am working with SQLGlot to enhancing SQL support. Target pass TPC-H and say 75% of TPC-DS Q2 2023.","title":"Roadmap"},{"location":"#bug-bounty","text":"If you see any bugs, please raise a Github issue. There is a $10 reward if you find a bug that I can't fix within a week. If you fix the bug yourself and make a PR, you will get another reward: $50 for a bug that results in a runtime error. $100 for a bug that results in a silent error, e.g. produces wrong results. I am a PhD student and have finite resources. So this program will be shut down after I lose $2000.","title":"Bug Bounty"},{"location":"#contact","text":"If you are interested in trying out Quokka or hit any problems at all, please contact me at zihengw@stanford.edu or Discord . I will try my best to make Quokka work for you.","title":"Contact"},{"location":"cloud/","text":"Setting up Quokka for EC2 To use Quokka for EC2, you need to at minimum have an AWS account with permissions to launch instances. You will probably run into issues since everybody's AWS setup is a little bit different, so please email: zihengw@stanford.edu or Discord . First if you haven't already, you must run aws configure on your local machine, i.e. the machine you are using to spin up a Quokka cluster and submit jobs to the cluster. Connecting to an existing Ray cluster This is the easiest way. It assumes you have already started a cluster with a Ray cluster YAML, and the cluster is already in a running state . You can use the QuokkaClusterManager in pyquokka.utils to connect to the cluster. You need to specify the path to the Ray cluster YAML, your AWS access key and secret key. You can optionally specify a spill directory on the machines in the cluster. from pyquokka.utils import * from pyquokka.df import * manager = QuokkaClusterManager() # Assume you created a Ray cluster with my_cluster.yaml. You need to pick a spill directory on the machines in the cluster. If you don't know what this is, just use /data. If /data doesn't work, try something else random like /data1. You can also specify pip requirements here for things you think you will need, like numpy. cluster = manager.get_cluster_from_ray(\"my_cluster.yaml\", aws_access_key, aws_access_id, requirements = [\"numpy\", \"pandas\"], spill_dir = \"/data\") from pyquokka.df import QuokkaContext qc = QuokkaContext(cluster) cluster.to_json(\"my_cluster.json\") It is recommended to do the above only once and save the cluster object to a json file using EC2Cluster.to_json and then use QuokkaClusterManager.get_cluster_from_json to connect to the cluster. Now you can start a new Python session and just use the json. cluster = manager.get_cluster_from_json(\"my_cluster.json\") qc = QuokkaContext(cluster) lineitem = qc.read_parquet(\"s3://my-bucket/lineitem.parquet\") ... If this doesn't work, please join the Discord and ask for help, or raise a Github issue. Creating an EC2 cluster without using ray up Quokka requires a security group that allows inbound and outbound connections for Arrow Flight, Ray, Redis and SSH. For simplicity, you can just enable all inbound and outbound connections from all IP addresses. You can make a security group like this: #!/bin/bash # Set the name and description of the security group GROUP_NAME=random-group GROUP_DESCRIPTION=\"Custom security group for Quokka\" # Create the security group aws ec2 create-security-group --group-name \"$GROUP_NAME\" --description \"$GROUP_DESCRIPTION\" # Get the ID of the security group GROUP_ID=$(aws ec2 describe-security-groups --group-names \"$GROUP_NAME\" --query \"SecurityGroups[0].GroupId\" --output text) echo $GROUP_ID # write this down! # Allow some inbound TCP traffic aws ec2 authorize-security-group-ingress --group-id \"$GROUP_ID\" --protocol tcp --port 0-65535 --cidr 0.0.0.0/0 # Allow all outbound TCP traffic aws ec2 authorize-security-group-egress --group-id \"$GROUP_ID\" --protocol tcp --port 0-65535 --cidr 0.0.0.0/0 You also need to generate a pem key pair. The easiest way to do this is if you don't already have one, is to start a t2.micro instance in the AWS console and create a new keypair. Remember the name of the key pair and where it lives on your computer. After you have the security group ID (sg-XXXXXX), you can use the QuokkaClusterManager in pyquokka.utils to spin up a cluster. You can optionally specify an AMI_ID as the base AMI for the machines in the cluster. The first requirement is that it's running Ubuntu. The second requirement for this AMI is that the Python version on it must match the Python version on your laptop, or whatever machine that will be running Quokka code and submitting jobs to this cluster. If you don't specify anything, the default will be AWS Ubuntu Server 20.04 AMI, which assume you have Python3.8. Now you can spin up a cluster with four lines of code: from pyquokka.utils import QuokkaClusterManager manager = QuokkaClusterManager(key_name = YOUR_KEY, key_location = ABSOLUTE_PATH_TO_KEY, security_group= SECURITY_GROUP_ID) cluster = manager.create_cluster(aws_access_secret_key, aws_access_id, num_instances = 4, instance_type = \"i3.2xlarge\", ami=AMI_ID, requirements = [\"sklearn\"]) cluster.to_json(\"config.json\") This would spin up four i3.2xlarge instances and install Sklearn on each of them. This takes around three minutes for me. The QuokkaClusterManager also has other utilities such as launch_all , terminate_cluster and get_cluster_from_json . Importantly, currently only on-demand instances are supported. This will change in the near future. The most interesting utility is probably manager.launch_all(command) , which basically runs a custom command on each machine. You can use this command to massage your cluster into your desired state. In general, all of the machines in your cluster must have all the Python packages you need installed with pip . Importantly, if you are using on demand instances, creating a cluster only needs to happen once. Once you have saved the cluster configs to a json, the next time you want to run a job and use this cluster, you can just do: from pyquokka.utils import QuokkaClusterManager manager = QuokkaClusterManager(key_name = YOUR_KEY, key_location = ABSOLUTE_PATH_TO_KEY, security_group= SECURITY_GROUP_ID) cluster = manager.get_cluster_from_json(\"config.json\") This will work if the cluster is either fully stopped or fully running, i.e. every machine must be in either stopped or running state. If the cluster is running, this assumes it was started by running the get_cluster_from_json command! Please do not manually start the instances and try to use get_cluster_from_json to connect to a cluster. Quokka also plans to extend support to Docker/Kubernetes based deployments based on KubeRay. (Contributions welcome!) Of course, there are plans to support GCP and Azure. The best way to make sure that happens is by sending me a message on email or Discord . The hard way, DIY Of course, you might wonder if you can set up the cluster yourself without using pyquokka.utils . Indeed you might not trust my setup -- am I stealing your data? Apart from reassuring you that I have little interest in your data, you can also try to manually setup the cluster yourself. Well it shouldn't be so hard to do this. These are the steps you have to follow: I assume you have your own security group and AMI image that abide by the requirements listed above. Feel free to open only specific ports, but Quokka might not work if you do. A telling sign there's a firewall problem is if Quokka hangs at launch. Please do make sure the Python version across the cluster is the same as the Python version on your laptop or whatever machine that will be submitting jobs to this cluster. Now launch a Ray cluster with the security group and AMI image. It's quite simple. Just install Ray on each machine in the cluster, and run ray start on the machine you choose to be the master. Now it will spit out a command you should run on the remaining machines. Go run that command on each remaining machine to setup the workers. Important: the Ray version across the cluster must also match the Ray version on your laptop. You must install Redis server on the master machine. curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list sudo apt-get update sudo apt-get install redis Now pip install pyquokka on all the machines in your cluster. That should automatically install all the dependencies you need. Of course if your workflow requires other packages like PyTorch , you need to pip install those as well. Now start the Redis server on the master. Quokka requires some interesting configs for this, which is in Quokka's github repo under pyquokka folder, redis.conf. Copy that file onto the master and start the Redis server: redis-server redis.conf --port 6800 --protected-mode no& You also need to copy the Flight Server file to each worker machine. You can find flight.py in pyquokka folder and copy that to each worker machine in the home directory. Now run aws configure on each of the machines across the cluster assuming you will be reading data from S3. If your instance comes with NVME SSDs, mount the NVME SSD onto /data : sudo mkfs.ext4 -E nodiscard /dev/nvme0n1; sudo mount /dev/nvme0n1 /data; Otherwise just make a directory called /data . Give read and write privleges to this folder: sudo chmod -R a+rw /data/ . You should be done. You should now make a json file that describes the cluster: {\"instance_ids\": {\"0\": \"i-0cb3e260d80acf9e1\", \"1\": \"i-02f307f57aa9217aa\", \"2\": \"i-09a5a93e7726b2eee\", \"3\": \"i-0af7018eba5c4bf17\"}, \"cpu_count_per_instance\": 8} Now you can use this Json file to get a Quokka cluster as described above. The hard way is not expected to work right away. In fact I would expect it not: definitely send me a message on Discord or email me if this doesn't work.","title":"Setting Up Cloud Cluster"},{"location":"cloud/#setting-up-quokka-for-ec2","text":"To use Quokka for EC2, you need to at minimum have an AWS account with permissions to launch instances. You will probably run into issues since everybody's AWS setup is a little bit different, so please email: zihengw@stanford.edu or Discord . First if you haven't already, you must run aws configure on your local machine, i.e. the machine you are using to spin up a Quokka cluster and submit jobs to the cluster.","title":"Setting up Quokka for EC2"},{"location":"cloud/#connecting-to-an-existing-ray-cluster","text":"This is the easiest way. It assumes you have already started a cluster with a Ray cluster YAML, and the cluster is already in a running state . You can use the QuokkaClusterManager in pyquokka.utils to connect to the cluster. You need to specify the path to the Ray cluster YAML, your AWS access key and secret key. You can optionally specify a spill directory on the machines in the cluster. from pyquokka.utils import * from pyquokka.df import * manager = QuokkaClusterManager() # Assume you created a Ray cluster with my_cluster.yaml. You need to pick a spill directory on the machines in the cluster. If you don't know what this is, just use /data. If /data doesn't work, try something else random like /data1. You can also specify pip requirements here for things you think you will need, like numpy. cluster = manager.get_cluster_from_ray(\"my_cluster.yaml\", aws_access_key, aws_access_id, requirements = [\"numpy\", \"pandas\"], spill_dir = \"/data\") from pyquokka.df import QuokkaContext qc = QuokkaContext(cluster) cluster.to_json(\"my_cluster.json\") It is recommended to do the above only once and save the cluster object to a json file using EC2Cluster.to_json and then use QuokkaClusterManager.get_cluster_from_json to connect to the cluster. Now you can start a new Python session and just use the json. cluster = manager.get_cluster_from_json(\"my_cluster.json\") qc = QuokkaContext(cluster) lineitem = qc.read_parquet(\"s3://my-bucket/lineitem.parquet\") ... If this doesn't work, please join the Discord and ask for help, or raise a Github issue.","title":"Connecting to an existing Ray cluster"},{"location":"cloud/#creating-an-ec2-cluster-without-using-ray-up","text":"Quokka requires a security group that allows inbound and outbound connections for Arrow Flight, Ray, Redis and SSH. For simplicity, you can just enable all inbound and outbound connections from all IP addresses. You can make a security group like this: #!/bin/bash # Set the name and description of the security group GROUP_NAME=random-group GROUP_DESCRIPTION=\"Custom security group for Quokka\" # Create the security group aws ec2 create-security-group --group-name \"$GROUP_NAME\" --description \"$GROUP_DESCRIPTION\" # Get the ID of the security group GROUP_ID=$(aws ec2 describe-security-groups --group-names \"$GROUP_NAME\" --query \"SecurityGroups[0].GroupId\" --output text) echo $GROUP_ID # write this down! # Allow some inbound TCP traffic aws ec2 authorize-security-group-ingress --group-id \"$GROUP_ID\" --protocol tcp --port 0-65535 --cidr 0.0.0.0/0 # Allow all outbound TCP traffic aws ec2 authorize-security-group-egress --group-id \"$GROUP_ID\" --protocol tcp --port 0-65535 --cidr 0.0.0.0/0 You also need to generate a pem key pair. The easiest way to do this is if you don't already have one, is to start a t2.micro instance in the AWS console and create a new keypair. Remember the name of the key pair and where it lives on your computer. After you have the security group ID (sg-XXXXXX), you can use the QuokkaClusterManager in pyquokka.utils to spin up a cluster. You can optionally specify an AMI_ID as the base AMI for the machines in the cluster. The first requirement is that it's running Ubuntu. The second requirement for this AMI is that the Python version on it must match the Python version on your laptop, or whatever machine that will be running Quokka code and submitting jobs to this cluster. If you don't specify anything, the default will be AWS Ubuntu Server 20.04 AMI, which assume you have Python3.8. Now you can spin up a cluster with four lines of code: from pyquokka.utils import QuokkaClusterManager manager = QuokkaClusterManager(key_name = YOUR_KEY, key_location = ABSOLUTE_PATH_TO_KEY, security_group= SECURITY_GROUP_ID) cluster = manager.create_cluster(aws_access_secret_key, aws_access_id, num_instances = 4, instance_type = \"i3.2xlarge\", ami=AMI_ID, requirements = [\"sklearn\"]) cluster.to_json(\"config.json\") This would spin up four i3.2xlarge instances and install Sklearn on each of them. This takes around three minutes for me. The QuokkaClusterManager also has other utilities such as launch_all , terminate_cluster and get_cluster_from_json . Importantly, currently only on-demand instances are supported. This will change in the near future. The most interesting utility is probably manager.launch_all(command) , which basically runs a custom command on each machine. You can use this command to massage your cluster into your desired state. In general, all of the machines in your cluster must have all the Python packages you need installed with pip . Importantly, if you are using on demand instances, creating a cluster only needs to happen once. Once you have saved the cluster configs to a json, the next time you want to run a job and use this cluster, you can just do: from pyquokka.utils import QuokkaClusterManager manager = QuokkaClusterManager(key_name = YOUR_KEY, key_location = ABSOLUTE_PATH_TO_KEY, security_group= SECURITY_GROUP_ID) cluster = manager.get_cluster_from_json(\"config.json\") This will work if the cluster is either fully stopped or fully running, i.e. every machine must be in either stopped or running state. If the cluster is running, this assumes it was started by running the get_cluster_from_json command! Please do not manually start the instances and try to use get_cluster_from_json to connect to a cluster. Quokka also plans to extend support to Docker/Kubernetes based deployments based on KubeRay. (Contributions welcome!) Of course, there are plans to support GCP and Azure. The best way to make sure that happens is by sending me a message on email or Discord .","title":"Creating an EC2 cluster without using ray up"},{"location":"cloud/#the-hard-way-diy","text":"Of course, you might wonder if you can set up the cluster yourself without using pyquokka.utils . Indeed you might not trust my setup -- am I stealing your data? Apart from reassuring you that I have little interest in your data, you can also try to manually setup the cluster yourself. Well it shouldn't be so hard to do this. These are the steps you have to follow: I assume you have your own security group and AMI image that abide by the requirements listed above. Feel free to open only specific ports, but Quokka might not work if you do. A telling sign there's a firewall problem is if Quokka hangs at launch. Please do make sure the Python version across the cluster is the same as the Python version on your laptop or whatever machine that will be submitting jobs to this cluster. Now launch a Ray cluster with the security group and AMI image. It's quite simple. Just install Ray on each machine in the cluster, and run ray start on the machine you choose to be the master. Now it will spit out a command you should run on the remaining machines. Go run that command on each remaining machine to setup the workers. Important: the Ray version across the cluster must also match the Ray version on your laptop. You must install Redis server on the master machine. curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list sudo apt-get update sudo apt-get install redis Now pip install pyquokka on all the machines in your cluster. That should automatically install all the dependencies you need. Of course if your workflow requires other packages like PyTorch , you need to pip install those as well. Now start the Redis server on the master. Quokka requires some interesting configs for this, which is in Quokka's github repo under pyquokka folder, redis.conf. Copy that file onto the master and start the Redis server: redis-server redis.conf --port 6800 --protected-mode no& You also need to copy the Flight Server file to each worker machine. You can find flight.py in pyquokka folder and copy that to each worker machine in the home directory. Now run aws configure on each of the machines across the cluster assuming you will be reading data from S3. If your instance comes with NVME SSDs, mount the NVME SSD onto /data : sudo mkfs.ext4 -E nodiscard /dev/nvme0n1; sudo mount /dev/nvme0n1 /data; Otherwise just make a directory called /data . Give read and write privleges to this folder: sudo chmod -R a+rw /data/ . You should be done. You should now make a json file that describes the cluster: {\"instance_ids\": {\"0\": \"i-0cb3e260d80acf9e1\", \"1\": \"i-02f307f57aa9217aa\", \"2\": \"i-09a5a93e7726b2eee\", \"3\": \"i-0af7018eba5c4bf17\"}, \"cpu_count_per_instance\": 8} Now you can use this Json file to get a Quokka cluster as described above. The hard way is not expected to work right away. In fact I would expect it not: definitely send me a message on Discord or email me if this doesn't work.","title":"The hard way, DIY"},{"location":"different/","text":"How is Quokka different from ... ? Spark First I have to say Matei is somewhat of a God, and Spark's design choices are simply ingenious in many cases. Most of its ingenuity is not apparent until you try to design your own system to beat its performance, which I had the good fortune of stumbling upon doing. Now that I have paid homage to my forebearers, let me say that Quokka and Spark are very similar in terms of what they do, but there are some important differences. Spark's core abstraction is a collection of data partitions. You operate on those data partitions in stages. One stage must complete before the next one starts. Quokka's core abstraction is a stream of data partitions. You can consume a data partition as soon as it's produced. As a result, multiple \"stages\" can be overlapped and pipelined in Quokka, leading to higher performance. Quokka's DataStream API resembles Spark's DataFrame API. However it is not feature complete yet. Importantly, Quokka doesn't yet have mature support for SQL inputs. Like Spark, Quokka's API is lazy. Like Spark, Quokka has a logical plan optimizer, though it is truly a baby compared to the gorilla-sized Spark Catalyst Optimizer. Quokka is written in Python completely on top of Ray, and integrates with Ray Data. Like Spark, Quokka is fault tolerant and can handle worker failures, but not coordinator failures. Finally, Quokka is written by one Stanford PhD student, while Spark has billions of dollars behind it. Obviously Quokka in its current state doesn't seek to displace Spark. Eventually, Quokka aims to be synergistic to Spark by supporting workloads the SparkSQL engine doesn't do too well, like time series or feature backfilling, on the same data lake based on open-source formats like Parquet. Quokka can do these a lot more efficiently due to its streaming-execution model and Python-based flexibility. Modin/Dask Quokka is a lot faster, or aims to be. I don't have benchmark numbers here, though I have found these systems to be slower than Spark. On the other hand, Quokka does not aim to support things like machine learning training (Dask), or dataframe pivots (Modin). Quokka also doesn't seek to religiously obey the Pandas API, whose eager execution model I think is incompatible with performance in modern systems. Pandas/Polars/DuckDB You should be using these solutions if you have less than 100GB of data. Pandas is the starter pack for data scientists, but I really encorage people to check out Polars, which is a Rust/Arrow-based implementation with pretty much the same API that's A LOT FASTER . Of course if all you want to do is SQL, then DuckDB can be a good choice. Quokka is heavily integrated with Polars. It also uses a lot of DuckDB for single node computation. Ray Data/DaFt/PetaStorm Recently there has been several attempts to bring data lake computing to unstructured datasets like images or natural language. Most prominent are probably DaFt by Eventual AI and PetaStorm by Uber. They define their own extension types for unstructured data, and try to make executing machine learning models in data pipelines efficient. Although you can certainly use Quokka to do what those libraries do, Quokka does not focus on this application. Instead Quokka seeks to integrate with those libraries by handling the upstream structured data ETL, like joining feature tables to observations tables etc. Of course, if your architecture is such that you are using a separate inference server with its own compute resources to conduct the machine learning, and all you have to do in your data pipeline is making RPC calls, then Quokka can definitely fulfill your needs for \"unstructured ETL\". Quokka just doesn't prioritize executing these deep learning functions natively inside your data pipeline.","title":"How is Quokka different from ...?"},{"location":"different/#how-is-quokka-different-from","text":"","title":"How is Quokka different from ... ?"},{"location":"different/#spark","text":"First I have to say Matei is somewhat of a God, and Spark's design choices are simply ingenious in many cases. Most of its ingenuity is not apparent until you try to design your own system to beat its performance, which I had the good fortune of stumbling upon doing. Now that I have paid homage to my forebearers, let me say that Quokka and Spark are very similar in terms of what they do, but there are some important differences. Spark's core abstraction is a collection of data partitions. You operate on those data partitions in stages. One stage must complete before the next one starts. Quokka's core abstraction is a stream of data partitions. You can consume a data partition as soon as it's produced. As a result, multiple \"stages\" can be overlapped and pipelined in Quokka, leading to higher performance. Quokka's DataStream API resembles Spark's DataFrame API. However it is not feature complete yet. Importantly, Quokka doesn't yet have mature support for SQL inputs. Like Spark, Quokka's API is lazy. Like Spark, Quokka has a logical plan optimizer, though it is truly a baby compared to the gorilla-sized Spark Catalyst Optimizer. Quokka is written in Python completely on top of Ray, and integrates with Ray Data. Like Spark, Quokka is fault tolerant and can handle worker failures, but not coordinator failures. Finally, Quokka is written by one Stanford PhD student, while Spark has billions of dollars behind it. Obviously Quokka in its current state doesn't seek to displace Spark. Eventually, Quokka aims to be synergistic to Spark by supporting workloads the SparkSQL engine doesn't do too well, like time series or feature backfilling, on the same data lake based on open-source formats like Parquet. Quokka can do these a lot more efficiently due to its streaming-execution model and Python-based flexibility.","title":"Spark"},{"location":"different/#modindask","text":"Quokka is a lot faster, or aims to be. I don't have benchmark numbers here, though I have found these systems to be slower than Spark. On the other hand, Quokka does not aim to support things like machine learning training (Dask), or dataframe pivots (Modin). Quokka also doesn't seek to religiously obey the Pandas API, whose eager execution model I think is incompatible with performance in modern systems.","title":"Modin/Dask"},{"location":"different/#pandaspolarsduckdb","text":"You should be using these solutions if you have less than 100GB of data. Pandas is the starter pack for data scientists, but I really encorage people to check out Polars, which is a Rust/Arrow-based implementation with pretty much the same API that's A LOT FASTER . Of course if all you want to do is SQL, then DuckDB can be a good choice. Quokka is heavily integrated with Polars. It also uses a lot of DuckDB for single node computation.","title":"Pandas/Polars/DuckDB"},{"location":"different/#ray-datadaftpetastorm","text":"Recently there has been several attempts to bring data lake computing to unstructured datasets like images or natural language. Most prominent are probably DaFt by Eventual AI and PetaStorm by Uber. They define their own extension types for unstructured data, and try to make executing machine learning models in data pipelines efficient. Although you can certainly use Quokka to do what those libraries do, Quokka does not focus on this application. Instead Quokka seeks to integrate with those libraries by handling the upstream structured data ETL, like joining feature tables to observations tables etc. Of course, if your architecture is such that you are using a separate inference server with its own compute resources to conduct the machine learning, and all you have to do in your data pipeline is making RPC calls, then Quokka can definitely fulfill your needs for \"unstructured ETL\". Quokka just doesn't prioritize executing these deep learning functions natively inside your data pipeline.","title":"Ray Data/DaFt/PetaStorm"},{"location":"install/","text":"Installation If you plan on trying out Quokka for whatever reason, I'd love to hear from you. Please send an email to zihengw@stanford.edu or join the Discord . Quokka can be installed as a pip package: pip3 install pyquokka You should also install the latest version of Redis if you are using Quokka locally: curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list sudo apt-get update sudo apt-get install redis Quokka is currently tested to work on x86 Linux/Mac environments. If you only plan on running Quokka locally, you are done. Here is a 10 min lesson on how it works. If you are planning on reading files from S3, you need to install the awscli and have your credentials set up. If you plan on using Quokka for cloud by launching EC2 clusters, there's a bit more setup that needs to be done. Currently Quokka only provides support for AWS. Quokka provides a utility library under pyquokka.utils which allows you to manager clusters and connect to them. It assumes that awscli is configured locally and you have a keypair and a security group with the proper configurations. To set these things up, you can follow the AWS guide . Alternatively, if you already have a Ray cluster setup, Quokka can connect to it. It will need to install a couple more things on the Ray cluster (pyquokka and Redis), but it will not launch any new nodes. More detailed instructions can be found in Setting Up Cloud Cluster .","title":"Installation"},{"location":"install/#installation","text":"If you plan on trying out Quokka for whatever reason, I'd love to hear from you. Please send an email to zihengw@stanford.edu or join the Discord . Quokka can be installed as a pip package: pip3 install pyquokka You should also install the latest version of Redis if you are using Quokka locally: curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list sudo apt-get update sudo apt-get install redis Quokka is currently tested to work on x86 Linux/Mac environments. If you only plan on running Quokka locally, you are done. Here is a 10 min lesson on how it works. If you are planning on reading files from S3, you need to install the awscli and have your credentials set up. If you plan on using Quokka for cloud by launching EC2 clusters, there's a bit more setup that needs to be done. Currently Quokka only provides support for AWS. Quokka provides a utility library under pyquokka.utils which allows you to manager clusters and connect to them. It assumes that awscli is configured locally and you have a keypair and a security group with the proper configurations. To set these things up, you can follow the AWS guide . Alternatively, if you already have a Ray cluster setup, Quokka can connect to it. It will need to install a couple more things on the Ray cluster (pyquokka and Redis), but it will not launch any new nodes. More detailed instructions can be found in Setting Up Cloud Cluster .","title":"Installation"},{"location":"multiregional_clusters/","text":"Spinning up Multiregional AWS Clusters using Quokka How to connect instances from multiple regions in Quokka Quokka now provides an interface which allows users to spin up a connected cluster of AWS EC2 instances across multiple regions. To do so, we first need to create a config.yaml file that specifies the details of the instances that will be included in the cluster. This config.yaml file needs to be set up according to the specifications outlined here . In order to spin up a certain number of workers in addition to the head node, the min_workers should be set to the desired number. Make sure that the specified number of max_workers is greater than the specified number of min_workers. In order to spin up instances in different regions, we need to create a new config file for each region in which we want to launch instances. Once we have our config files, we call ray up /path/to/config_region1.yaml from the command line. This will spin up the instances in region1. To spin up instances in other regions, we need to call ray up /path/to/config_regionX.yaml on each regional config file respectively. Once all instances are up and running, we can then create a QuokkaClusterManager and subsequently call the get_multiple_clusters_from_yaml method. Here is an example of how to do this: from pyquokka.utils import * manager = QuokkaClusterManager(key_name=\u201dmy_key\u201d, key_location=\u201d/path/to/key\u201d, security_group=\u201dabc\u201d) cluster_list = [\"config_region1.yaml\", \"config_region2.yaml\"] results = manager.get_multiple_clusters_from_yaml(cluster_list, aws_access_key, aws_access_id, requirements = [\"numpy\", \"pandas\"], spill_dir = \"/data\") from pyquokka.df import QuokkaContext qc = QuokkaContext(results[0]) It is important to note that results is a tuple of values: an instance of a Quokka EC2Cluster class at index 0 and a dictionary containing more regional information about the instances in the cluster at index 1. Important Information about Multiregional Clusters AMIs AWS AMIs are region specific. Therefore, you should use region-appropriate AMIs for the instances that you are spinning up in each region. If you have AMIs in one region that you want to copy to another region, you can do as described in this post . VPC Peering for Data Transfer In order to spin up a multi-region cluster, Quokka requires the VPCs of all involved instances to be peered. The idea of VPC peering is explained in more detail here . It is easiest to create a VPC peering connection from the AWS console. The steps to establish a VPC peering connection are described in the AWS docs here . Since we are talking about setting up a multiregional cluster here, you most likely will want to peer two VPC on the same account, but for different regions. An in-depth tutorial on how to do this can be found here . At the end of the peering process, you will need to update your route tables for the peering. The AWS docs on how to do this can be found here . Security Groups and VPCs When spinning up instances in different regions with boto3, issues can arise from the security group specified in the QuokkaClusterManager. An example would be the following error: botocore.exceptions.ClientError: An error occurred (InvalidGroup.NotFound) when calling the RunInstances operation: The security group 'xyz\u2019 does not exist in VPC 'abc\u2019 Make sure to update the security group in order to allow VPC from other regions to access it. The process of how to do this is described in the AWS docs here . Use of AWS Keys across Multiple regions When using different key pairs for different regions, it is likely that the following error occurs: botocore.exceptions.ClientError: An error occurred (InvalidKeyPair.NotFound) when calling the RunInstances operation: The key pair xxxxx does not exist In order for this to work, we need to import the key that we want to use across the instances to all the regions that we want to connect using the multi-region cluster. An article on how to import AWS keys to other regions can be found here . When importing the key to a different region, make sure to give it the same name as the key in the original region. The QuokkaClusterManager class does not distinguish between key names. This means that it will not be able to handle various key names.","title":"Spinning up Multiregional AWS Clusters using Quokka"},{"location":"multiregional_clusters/#spinning-up-multiregional-aws-clusters-using-quokka","text":"","title":"Spinning up Multiregional AWS Clusters using Quokka"},{"location":"multiregional_clusters/#how-to-connect-instances-from-multiple-regions-in-quokka","text":"Quokka now provides an interface which allows users to spin up a connected cluster of AWS EC2 instances across multiple regions. To do so, we first need to create a config.yaml file that specifies the details of the instances that will be included in the cluster. This config.yaml file needs to be set up according to the specifications outlined here . In order to spin up a certain number of workers in addition to the head node, the min_workers should be set to the desired number. Make sure that the specified number of max_workers is greater than the specified number of min_workers. In order to spin up instances in different regions, we need to create a new config file for each region in which we want to launch instances. Once we have our config files, we call ray up /path/to/config_region1.yaml from the command line. This will spin up the instances in region1. To spin up instances in other regions, we need to call ray up /path/to/config_regionX.yaml on each regional config file respectively. Once all instances are up and running, we can then create a QuokkaClusterManager and subsequently call the get_multiple_clusters_from_yaml method. Here is an example of how to do this: from pyquokka.utils import * manager = QuokkaClusterManager(key_name=\u201dmy_key\u201d, key_location=\u201d/path/to/key\u201d, security_group=\u201dabc\u201d) cluster_list = [\"config_region1.yaml\", \"config_region2.yaml\"] results = manager.get_multiple_clusters_from_yaml(cluster_list, aws_access_key, aws_access_id, requirements = [\"numpy\", \"pandas\"], spill_dir = \"/data\") from pyquokka.df import QuokkaContext qc = QuokkaContext(results[0]) It is important to note that results is a tuple of values: an instance of a Quokka EC2Cluster class at index 0 and a dictionary containing more regional information about the instances in the cluster at index 1.","title":"How to connect instances from multiple regions in Quokka"},{"location":"multiregional_clusters/#important-information-about-multiregional-clusters","text":"","title":"Important Information about Multiregional Clusters"},{"location":"multiregional_clusters/#amis","text":"AWS AMIs are region specific. Therefore, you should use region-appropriate AMIs for the instances that you are spinning up in each region. If you have AMIs in one region that you want to copy to another region, you can do as described in this post .","title":"AMIs"},{"location":"multiregional_clusters/#vpc-peering-for-data-transfer","text":"In order to spin up a multi-region cluster, Quokka requires the VPCs of all involved instances to be peered. The idea of VPC peering is explained in more detail here . It is easiest to create a VPC peering connection from the AWS console. The steps to establish a VPC peering connection are described in the AWS docs here . Since we are talking about setting up a multiregional cluster here, you most likely will want to peer two VPC on the same account, but for different regions. An in-depth tutorial on how to do this can be found here . At the end of the peering process, you will need to update your route tables for the peering. The AWS docs on how to do this can be found here .","title":"VPC Peering for Data Transfer"},{"location":"multiregional_clusters/#security-groups-and-vpcs","text":"When spinning up instances in different regions with boto3, issues can arise from the security group specified in the QuokkaClusterManager. An example would be the following error: botocore.exceptions.ClientError: An error occurred (InvalidGroup.NotFound) when calling the RunInstances operation: The security group 'xyz\u2019 does not exist in VPC 'abc\u2019 Make sure to update the security group in order to allow VPC from other regions to access it. The process of how to do this is described in the AWS docs here .","title":"Security Groups and VPCs"},{"location":"multiregional_clusters/#use-of-aws-keys-across-multiple-regions","text":"When using different key pairs for different regions, it is likely that the following error occurs: botocore.exceptions.ClientError: An error occurred (InvalidKeyPair.NotFound) when calling the RunInstances operation: The key pair xxxxx does not exist In order for this to work, we need to import the key that we want to use across the instances to all the regions that we want to connect using the multi-region cluster. An article on how to import AWS keys to other regions can be found here . When importing the key to a different region, make sure to give it the same name as the key in the original region. The QuokkaClusterManager class does not distinguish between key names. This means that it will not be able to handle various key names.","title":"Use of AWS Keys across Multiple regions"},{"location":"runtime/","text":"Quokka Runtime API documentation Programming Model A note about the name: the name is inspired by the Apache Flink icon, which is a chipmunk. A quokka is a marsupial that resembles a chipmunk. Motivation Popular big data processing frameworks such as Spark and Dask rely on bulk-synchronous execution on distributed datasets. Often, a map-reduce style model is adopted, where mappers perform functions on partitions of the input, the mapper outputs are shuffled into groups, and after the shuffle has fully/mostly completed , reducers start working on each group. Typically this is implemented as a pull-based model where reducers pull required data from the mappers, who persist their output in some kind of external storage (disk or network) when fault tolerance is desired. There are a couple problems with this approach. The first, as recent works such as LinkedIn Magnet and Uber Zeus have identified, is that when each mapper doesn't have too much data for each reducer, the pull operation amounts to a bunch of random disk/network reads. This is horrible. The solution is push-based shuffles, where mappers push data to the reducers. Data can now be persisted on the reducer side for fault tolerance. However, this only addresses part of the problem. In a synchronous shuffle, even when mapper output is pushed to the reducers as soon as they are generated, the reducers can't start operating on said data until they have received near everything. This is because the current Map-Reduce paradigm stipulates that the reduction function is a function on all the data assigned to it from the mappers. This forces the reducers to start only after most of the mappers have completely executed, making any kind of pipelined parallel execution between the two impossible. This is unfortunate, because mappers and reducers often use very different resources (network I/O bound mappers + compute bound reducers), and can often be scheduled for parallel execution on the same physical instances without compromising too much the performance of either. Quokka's solution is to support two different kinds of reducer functions. Blocking reducers are similar to classic Map-Reduce reducers and block until they receive all mapper outputs. However, non-blocking reducers can start executing on mapper outputs as soon as they arrive, producing some output of its own and updating some local state. For example, sort, count and aggregation are blocking reducer functions because their output depend on all the data. However, join, filter and projection can be implemented in a non-blocking fashion with streaming operators. Non-blocking reducers can be pipelined with other non-blocking reducers, while a blocking reducer breaks the pipeline. Mappers are treated as non-blocking reducers where the output already exists in network/disk storage. We impose some limitations on the kinds of non-blocking operators we support, which are described in detail later. Logically, one can view Quokka execution as a series of stages, where each stage start with the output produced by a blocking operator, ends with another blocking operator, and executes non-blocking operators in between. The entire stage is executed in a pipeline-parallel fashion, and can be viewed as a pure streaming system. The stage inputs/outputs use Spark's lineage tracking based fault-tolerance and persistence mechanism. Since each Quokka stage now corresponds to a few Spark stages, Quokka also implements intra-stage fault tolerance based on checkpointing. The checkpointing recovery mechanism in Quokka conveniently avoids global asynchronous rollbacks, the bane of streaming systems, thanks to the restrictions we impose on the non-blocking operators. Quokka also aims to support autoscaling. (I have a plan to do this, but likely will not get to this until after the rotation.) Execution Model The Quokka runtime API allows you to construct a task graph of nodes , which corresponds to a Quokka stage. This is very similar to other DAG-based processing frameworks such as Apache Spark or Tensorflow . For example, you can write the following code in the runtime API to execute TPC-H query 6: task_graph = TaskGraph() lineitem = task_graph.new_input_csv(bucket,key,lineitem_scheme,8,batch_func=lineitem_filter, sep=\"|\") agg_executor = AggExecutor() agged = task_graph.new_blocking_node({0:lineitem}, agg_executor, 1, {0:None}) task_graph.initialize() task_graph.run() There are perhaps a couple of things to note here. Firstly, there are two types of nodes in the runtime API. There are input nodes , declared with APIs such as new_input_csv or new_input_parquet , which interface with the external world (you can define where they will read their data), and task nodes , declared with new_non_blocking_node or new_blocking_node , which take as input the outputs generated from another node in the task graph, either an input node or another task node. Secondly, we see that the task node agged depends on the outputs from the input node lineitem . We will describe what exactly are the types of lineitem and agged later (the former is a stream and the latter is a dataset). Finally, note that the task graph ends with a blocking node. This is currently required, if you want to be able to interact with the results of the task graph execution. Multiple stages are implemented with multiple task graphs, with the first node of stage 2 reading from the output of stage 1, like the following: task_graph = TaskGraph() a = task_graph.new_input_csv(\"bump\",\"a-big.csv\",[\"key\"] + [\"avalue\" + str(i) for i in range(100)],{'localhost':2}) b = task_graph.new_input_csv(\"bump\",\"b-big.csv\",[\"key\"] + [\"bvalue\" + str(i) for i in range(100)],{'localhost':2}) join_executor = OOCJoinExecutor(on=\"key\") output = task_graph.new_blocking_node({0:quotes,1:trades},None, join_executor,{'localhost':4},{0:\"key\", 1:\"key\"}) task_graph.initialize() task_graph.run() del task_graph task_graph2 = TaskGraph() count_executor = CountExecutor() joined_stream = task_graph2.new_input_from_dataset(output,{'localhost':4}) final = task_graph2.new_blocking_node({0:joined_stream}, None, count_executor, {'localhost':4}, {0:'key'}) task_graph2.initialize() task_graph2.run() Note that since the output of a stage is persisted as in Spark, one can delete the first task graph and still access its outputs. Since a task graph represents one Quokka stage, it strictly follows push-based execution. This means that a node does not wait for its downstream dependencies to ask for data, but instead actively pushes data to its downstream dependencies whenever some intermediate results become available. In short, execution proceeds as follows : input nodes read batches of data from a specified source, which might be an external data source or the outputs of a previous stage, and pushes those batches to downstream task nodes. A task node exposes a handler to process incoming batches as they arrive, possibly updating some internal state, and for each input batch possibly produces an output batch for its own downstream children. The programmer is expected to supply this handler function as an executor object (e.g. OOCJoinExecutor , AggExecutor ). Quokka provides a library of pre-implemented executor objects that the programmer can use for SQL, ML and graph analytics. Each task node can have multiple physical executors, referred to as channels . This is a form of intra-operator data parallelism, as opposed to the inter-operator pipeline parallelism that results from all task nodes executing at the same time. These physical executors all execute the same handler function, but on different portions of the input batch, partitioned by a user-specified partition function. A Map-Reduce job with M mappers and R reducers would be implemented in Quokka as a single mapper task node and a single reducer task node, where the mapper task node has M channels and the reducer task node has R channels. In the example above, we specified that the input node lineitem has 8 channels, and the task node agged has only 1 channel. The partition key was not specified ( {0:None} ) since there is no parallelism, thus no need for partitioning. The situation looks something like the following picture: Quokka keeps track of all the channels and schedules them onto physical computing resources. For the engine, two channels from different task nodes are on more or less equal footing -- they can be scheduled on the same hardware or different hardware. A channel from an input node completes execution when there's no more inputs to be read or if all of its downstream dependencies have completed execution. A channel from a task node completes execution when: all of its upstream sources have completed execution if its execution handler decides to terminate early based on the input batch and its state (e.g. for a task node that executes the limit operator in a limit query, it might keep as local state the buffered output, and decide to terminate when that output size surpasses the limit number) if all its downstream dependencies have completed execution. By default, all channels start execution at once. This does not necessarily mean that they will start processing data, this means that they will all start waiting for input batches from their upstream sources to arrive. One could specify that an input node delay execution until another input node has finished. For example to implement a hash join one might want to stream in one table to build the hash table, then stream in the other table for probing. The runtime API is meant to be very flexible and support all manners of batch and stream processing. For example, one could specify an input node that listens to a Kafka stream, some task nodes which processes batches of data from that stream, and an output node that writes to another Kafka stream. In this case, since the input node will never terminate, and assuming the other nodes do not trigger early termination, the task graph will always be running. As a result of this flexibility, it requires quite a lot of knowledge for efficient utilization. As a result, we aim to provide higher level APIs to support common batch and streaming tasks in SQL, machine learning and graph analytics. Most programmers are not expected to program at the runtime API level, but rather make use of the pre-packaged higher-level APIs. Stateful Actors Let's talk more about task nodes in Quokka. Channels in task nodes can be treated as stateful operators in an actor programming model. Quokka adopts the notion of channels in a task node to specify that a group of actors all execute the same code, for fault tolerance and autoscaling purposes. One could override default Quokka behavior by simply specifying different task nodes with one channel each, all executing the same code. The key property of stateful operators in Quokka is confluence : in the context of nondeterministic message delivery, an operation on a single machine is confluent if it produces the same set of outputs for any nondeterministic ordering and batching of a set of inputs. (Hellerstein, CALM) Note that the output itself can also be produced in any order. It\u2019s easy to see that any composition of confluent operators is still confluent. We relax the confluent definition somewhat here to accept potentially different output sets, assuming they are all semantically correct. For example an operator that implements the LIMIT N clause in SQL can admit any of N input records it sees. More importantly, for Quokka we allow operators to depend on intra-stream ordering, just not inter-stream ordering. This means that it might still expect the inputs produced by a certain stream to observe some order, while there are no restrictions on the relative orderings between different input streams. Quokka as a system enforces intra-stream message order, but makes zero gurantees about inter-stream message orders. Henceforth, confluence will refer to this narrow definition, not the one defined in the CALM paper. Confluence is a very nice property to have in general, more so for streaming systems. Let\u2019s imagine a stateful operator with two different upstream operators producing messages. It is very nice if the system\u2019s correctness does not depend on the order in which the two upstream operators produce the messages, which could depend on network delay, task scheduling, etc. This is critical for performance in a push-based framework since a node should never wait on any one of its input streams. In addition, it also greatly facilitates fault tolerance, as messages from different sources can be replayed in any order in regards to one another, as we will describe later. Confluence is perhaps the key difference between Quokka and streaming-centric systems like Flink. In Flink you can totally write pipelines where the outputs depend very strongly on the order the inputs are supplied. In Quokka it is not allowed. (Really at this point, it's only \"not recommended\" -- there are no checks in place to see if your actor is confluent or not. What's guaranteed is that all the operators in the libraries supplied follow this model. Enforcing this is future work.) What are some examples of confluent stateful operators? First let's categorize the world of stateful operators we'd like to implement in data analytics. As mentioned previosuly, there are two important cateogories: nonblocking and blocking . Blocking operators cannot emit any outputs to their downstream children until all of their inputs have been processed. Examples are any kind of aggregation and sort. For (naive) aggregation, the stateful operator does not know it has the final result for any of its aggregation keys until it has seen all of its inputs. For sorting, the stateful operator cannot guarantee that it would emit results in sorted order until it has received all its inputs. We call any operator that is not blocking non-blocking. Example non-blocking operators are map, filter, projection and join. Blocking operators are pipeline breakers, and negate the benefits of using a streaming framework like Quokka. Confluence is easy to reason about for blocking operators. The blocking operator emit only one output, at the very end. We just have to make sure that this output is the same regardless of the order in which we supply the operator's inputs. Since this operator is typically a function of the final state, we just have to ensure that the final state is the same. If we imagine that each incoming message changes the state of the operator by function f , then it's easy to see that as long as f is commutative this is true. For example, any kind of aggregation is commutative, the merge step in merge-sort is commutative, etc. Confluence is harder to reason about for nonblocking operators. We must guarantee that regardless of the order the input batches are supplied, the set of output batches do not change. Let\u2019s say we only have two incoming messages, m and n, to a node with starting state S. Then the outputs produced by giving m first to S, changing the state S to f(m, S), while producing output o(m, S) and then giving n to S, changing the state to f(n, f(m,S)) while producing output o(n, f(m,S)), which is {o(m,S), o(n,f(m,s))} is the same as if we gave the outputs in the reverse order. Note that this assumes that m and n are all the messages the node will see. Confluence is about eventual consistency. While in general there are many ways to achieve this kind of behavior as long as only monotonic operations are applied to the state at each input batch (Bloom), in Quokka all the stock non-blocking operators take the approach of setting the state as sets of immutable batches of data, that can only be added to. This is clearly monotonic. If you are writing a stateful operator for Quokka, this is the recommended approach. What this means is that it is impossible to perform operations that require a specific batch amongst the set of batches, such as list indexing, since ordering of the batches in a set in the state is undefined. Most meaningful operations take the incoming message and produce an output that depends on the entire set, or not at all. An example of a confluent stateful operator in Quokka is a join. The code can be roughly summarized as follows: state0 = set() state1 = set() for each input: if input from stream0: state0.add(input) emit set(input.join(i) for i in state1) else: state1.add(input) emit set(i.join(input) for i in state0) Note that there is in fact a non-monotonic domain-specific optimization we can make that will preserve confluence in the case of a primary key join. Any input streamed in from stream0 can guarantee that any future records from that table will not have the same key value. Thus all state1 related to the record\u2019s key can be safely deleted. Quokka currently does not implement this optimization. Datasets and Streams Let's talk more about how non-blocking and blocking operators work in Quokka. Blocking operators could be introduced by operations like aggregations and sort, or simply by user command when they wish to materialize data with .materialize() (similar to .cache() semantics in Spark or .compute() semantics in Dask). Such blocking operators will produce a Dataset in Quokka, while non-blocking operators will produce a Stream . Downstream operators could depend on both upstream datasets and streams. The difference is that the upstream dataset need to be completely materialized when an operator starts executing, while a stream is just a promise that batches of data will be produced at some point in the future in any order. In other words, from the perspective of the operator, it can pull data from an upstream dataset and expects data to be pushed to it from the stream. In the very first code listing for TPC-H query 6, agged is a dataset whereas lineitem is a stream. In practice, a Quokka DAG can consist of many blocking operators and non-blocking operators organized in complicated ways. For example, here is the DAG for a PageRank application: As previously described, Quokka decomposes the computation into stages, with each stage ending in the creation of a Dataset. In this case the computation will be broken into two stages, the first of which consists of the nonblocking input sparse matrix read and caching (the upper row). The second will be the bottom row. The second stage depends on the first one, so it will be launched after the first one has completed. This is very similar to how stages in Spark work. (Note that strictly speaking, every stage has to start from a Dataset too. In this case the input nodes depend on Datasets that are pre-created in S3 or Disk, and are abbreviated in this graph.) Similarly to an RDD, Quokka represents a Dataset as a collection of immutable objects, and some associated metadata on those objects, which is itself an immutable object. The objects are all stored on a shared-memory object store with persistence (currently RocksDB). When you use task_graph.add_blocking_node in Quokka, a Dataset object will be returned. You can use this Dataset object in downstream operators. Quokka guarantees that by the time the downstream operators execute, all the Datasets that they depend on would have been materialized in this object store. The stock Dataset class in Quokka exposes some convenience methods such as an iterator to iterate through the objects. The user could also interact directly with the object store after looking up metadata from the Dataset object. There are more specialized Dataset class implementations in Quokka like KVDataset or RangeDataset which corresponds to hash-based partitioning or range-based partitioning of objects that expose more methods. The user could also implement a custom Dataset class that descends from Dataset with even more methods. It is important to ensure that when using a Dataset in a downstream operator that also takes streaming inputs, the confluence property is respected. Unfortunately, Quokka currently does not enforce this and it's possible for you to mess this up when writing your code. Although it's not that easy to mess up, since you cannot change the objects you read from the Dataset. A downstream operator could treat the Dataset as a stream by simply invoking the iterator to iterate through the objects in the Dataset. However, for many downstream operations, it might be desirable to explicitly convert a Dataset into a Stream again (e.g. to use stock operators that only have stream-based implementations). You can do that by using the specialized task node add_input_dataset . Internally, this task node just calls the iterator repeatedly and produce a stream of batches corresponding to the objects in the Dataset. Fault tolerance (future work) The current theory is a bit complicated. I am still thinking through how this should work exactly, but hopefully the gist gets through. Given our group of confluent stateful operators, how do we achieve fault tolerance? A Quokka application can be thought of as a DAG, where each node corresponds to a channel, from one of the task nodes. Each node is assigned to a physical hardware instance. Quokka is designed to expect many nodes to be assigned to one physical instance. For example, let's imagine the following case, where the nodes circled belongs to machine A and the rest belong to machine B, and nodes 1 and 2 are channels of the input node. 3, 4 and 5 are non-blocking operators, 6 and 7 are blocking operators. Quokka follows a checkpoint-based system where each channel periodically asynchronously checkpoints its local state to persistent storage (AWS S3). Note that this is quite efficient given the types of states we typically have, such as (typically) small intermediate aggregation results and sets of batches that are monotonically added to. (This is definitely an area of future work) The problem is easy to spot: \"yes checkpoints are great, but you must turn off the entire system when a machine fails to sync it back to the latest good state, and then reapply all the inputs.\" Yes that is true for a general-purpose streaming system like Flink or Naiad. Coordinated global rollbacks really suck. But in Quokka where all the stateful operators are confluent, this need not happen. What happens when machine A dies? TLDR: machine B can keep doing work as if nothing is wrong, while machine A's workload eventually gets rescheduled. The gory details: nodes 1, 3, 6 and 7 carry on with life (they won't even know machine A just died). 1 will notice that it can no longer send messages to 4 and 5. That's ok, it will just buffer those messages. 3 and 6 will realize that they have fewer incoming messages now. 7 will notice that they have no more incoming messages. That's ok, they can work on their backlog. The system then goes about recovering 2, 4 and 5. It will request a new machine to schedule 2, 4 and 5, or simply schedule them to machine B. 2 is a channel of an input node, which has no state. In Quokka, all message sent between channels are tagged with a sequence number. The number starts from 0 and monotonically increases. This way, the task node discards messages with a wrong sequence number. The state of a stateful operator is also tagged with a number. The state number starts from 0 and monotonically increases every time. When an operator checkpoints, it writes its state, its state number, and the latest sequence number it expects from its inputs. A consistent checkpoint contains all this information. Quokka will look at the last consistent checkpoint of nodes 4 and 5, and find the minimum of all the latest sequence numbers across both consistent checkpoints. This is the batch that 2 will now have to start to produce. Let's say that node 4 had the smaller latest sequence number. Then node 4 will immediately start catching up. Node 5 will look at the incoming batches, find that their sequence numbers are smaller than expected, and proceed to ignore all of them. Eventually, node 5 will start recovering state as well. After both nodes catch up to the point where they died, node 6 and 7 will start accepting messages from node 4 and node 5 since now their messages have valid sequence numbers. What if in this example, node 2 was not an input node but a task node? Then the dead subgraph has no way of re-reading the input. Long story short, each node needs to buffer outgoing messages, until its children notify it that the state change affected by that outgoing message has been persisted to a checkpoint. This way, messages can be replayed when needed. All this while, machine B has been carrying on with its life. This means that if we started out in a load balanced way, then this fault recovery has introduced stragglers -- node 4 and 5 will now finish after node 3. This is actually okay from a resource-usage point of view. Note that nowhere in this process are we wasting resources across the cluster, as seen in global synchronous rollbacks. Only the lost states need to be recomputed, similar in vein to the fault-tolerance mechanism in Spark. In addition, fault recovery required minimal communication with workers that did not experience a fault, minimizing fault recovery overhead. Stragglers are okay for Quokka, we will mediate them through the dynamic scheduling mechanism described in the next section. Scheduling and Autoscaling (future work) There are two auto-scaling strategies in Quokka. The first is automatic, while the second might require some user input. Recall that Quokka is designed to expect many channels to be assigned to the same physical hardware. But first, let's talk about how Quokka schedules channels to hardware, assuming that the graph is static, and the number and type of machines are fixed. Firstly, in the current runtime API, when instantiating a task node or input node, the user manually specifies how many channels are there and where those channels go. Dynamic channel scheduling is done when programming in higher-level APIs. We observe that each channel is in effect an independent stateful oeprator that can be scheduled independently. However, different scheduling strategies entail different communication costs. If channel A sends a large volume of messages to channel B, then we should schedule them on the same machine. Note that contrary to intuition, there is no benefit at all in scheduling multiple channels from the same input node or task node on the same machine apart from parallelism, since they never talk to each other. Channel scheduling can be dynamic, in the sense that a channel can be moved from one physical machine to another in a very straight-forward way. The self-contained nature of an actor is an oft-quoted strength of the actor model. All that needs to happen is for Quokka to transfer the state of the actor to another node (which could be done asynchronously after the transfer decision is made), and change the partition function for the channel's parents so that the appropriate physical machine receives the incoming messages. The data transfer cost is the only cost in moving an actor. Different criteria can be used to decide if a channel should be moved to another physical machine. These could include machine specific characteristics, such as limited memory available or high CPU usage on the current machine, or the lack thereof on the other machine. Quokka can also use channel-specific information, for example if the system observes the channel transfering large amounts of data to another channel on another machine and determines that the cost in moving this channel can be overcame by the benefit in data locality achieved after the move. The stragglers introduced by fault recovery can be mediated in this fashion. Node 1 and 3 will finish before node 2 and 4/5, creating less resource usage on machine B. The system will then try to move one of node 4/5 onto machine B. Manual autoscaling using combiner functions To be written. Example Applications TPC-H query 12 Pagerank Let's talk about how PageRank works in the Quokka programming model. TaskGraph API new_input_csv (bucket, key, names, parallelism, ip='localhost',batch_func=None, sep = \",\", dependents = [], stride = 64 * 1024 * 1024) Currently, new_input_csv only supports reading a CSV in batches from an AWS S3 bucket. Required arguments in order: bucket : str. AWS S3 bucket key : str. AWS S3 key names : list of str. Column names. Note that if your rows ends with a delimiter value, such as in TPC-H, you will have to end this list with a placeholder such as \"null\". Look at the TPC-H code examples under apps. parallelism : int. the runtime API expects the programmer to explicitly state the amount of intra-op parallelism to expose. 8 is typically a good number. Keyword arguments: ip : str. the IP address of the physical machine the input node should be placed. Defaults to local execution. batch_func : function. the user can optionally pass in a function to execute on the input CSV chunk before it's passed off to downstream dependents. Currently the input CSV is parsed into a Pandas Dataframe, so batch_func can be any Python function that can take a Pandas Dataframe as input and produces a Pandas Dataframe. This can be done to perform predicate pushdown for SQL for example. sep : str. delimiter dependents : list of int. an input node can depend on other input nodes, i.e. only start once another input node is done. For example to implement as hash join where one input might depend on another, one could do the following: a = new_input_csv(...) b = new_input_csv(...,dependents=[a]) stide : int. how many bytes to read from the input S3 file to read at a time, default to 64 MB. Returns : a node id which is a handle to this input node, that can be used as the sources argument for task nodes or dependents arguments for other input nodes. new_input_parquet(bucket, key, names, parallelism, columns, skip_conditions, ip='localhost',batch_func=None, sep = \",\", dependents = [], stride = 64 * 1024 * 1024) Not yet implemented. new_task_node(sources, functionObject, parallelism, partition_key, ip='localhost') Instantiate a new task node with an executor object that defines the handler function which runs on each incoming batch. Required arguments in order: sources : dict of int -> int. the upstream sources that feed batches to this task node. Expects a dictionary, where the keys are integers and values are node ids (also stored as integers). This in effect names the source nodes. i.e. if you specify {0: source_node_id_x, 1:source_node_id_y} , from the perspective of this task node you are calling the batches coming from source_node_id_x source 0 and the batches coming from node_id_y source 1. You will make use of these identifiers writing the executor class's handler function for incoming batches. functionObject : an executor object which defines the input batch handler function. More details on this in the next section. You can write your own or use a pre-supplied one from the sql, ml or graph packages. parallelism : int. the runtime API expects the programmer to explicitly state the amount of intra-op parallelism to expose. Think carefully about this choice. Computationally intensive tasks might benefit from parallelism, while simple tasks such as aggregation might not. partition_key : dict of int -> in. This argument expects a dictionary with a key for each key in the sources dict. It describes how the input batches should be partitioned amongst the channels. If the value is None, then the input batch is copied and broadcast to all channels. Otherwise, currently each channel receives the sub-batch input_batch[input_batch.partition_key % parallelism == channel_id]. If this partition key is not in the input batch's columns from the specified source node, a runtime error would ensue. Keyword arguments: ip : str. the IP address of the physical machine the input node should be placed. Defaults to local execution. Writing Your Own (Stateless) Executor Object The best place to learn how to write your own executor object classes is by looking at the available executor object classes in the SQL library. In short, an executor class is simply a child class of this base class: class StatelessExecutor: def __init__(self) -> None: raise NotImplementedError def early_termination(self): self.early_termination = True def execute(self,batch,stream_id, executor_id): raise NotImplementedError def done(self,executor_id): raise NotImplementedError The Stateless","title":"Quokka Runtime API documentation"},{"location":"runtime/#quokka-runtime-api-documentation","text":"","title":"Quokka Runtime API documentation"},{"location":"runtime/#programming-model","text":"A note about the name: the name is inspired by the Apache Flink icon, which is a chipmunk. A quokka is a marsupial that resembles a chipmunk.","title":"Programming Model"},{"location":"runtime/#motivation","text":"Popular big data processing frameworks such as Spark and Dask rely on bulk-synchronous execution on distributed datasets. Often, a map-reduce style model is adopted, where mappers perform functions on partitions of the input, the mapper outputs are shuffled into groups, and after the shuffle has fully/mostly completed , reducers start working on each group. Typically this is implemented as a pull-based model where reducers pull required data from the mappers, who persist their output in some kind of external storage (disk or network) when fault tolerance is desired. There are a couple problems with this approach. The first, as recent works such as LinkedIn Magnet and Uber Zeus have identified, is that when each mapper doesn't have too much data for each reducer, the pull operation amounts to a bunch of random disk/network reads. This is horrible. The solution is push-based shuffles, where mappers push data to the reducers. Data can now be persisted on the reducer side for fault tolerance. However, this only addresses part of the problem. In a synchronous shuffle, even when mapper output is pushed to the reducers as soon as they are generated, the reducers can't start operating on said data until they have received near everything. This is because the current Map-Reduce paradigm stipulates that the reduction function is a function on all the data assigned to it from the mappers. This forces the reducers to start only after most of the mappers have completely executed, making any kind of pipelined parallel execution between the two impossible. This is unfortunate, because mappers and reducers often use very different resources (network I/O bound mappers + compute bound reducers), and can often be scheduled for parallel execution on the same physical instances without compromising too much the performance of either. Quokka's solution is to support two different kinds of reducer functions. Blocking reducers are similar to classic Map-Reduce reducers and block until they receive all mapper outputs. However, non-blocking reducers can start executing on mapper outputs as soon as they arrive, producing some output of its own and updating some local state. For example, sort, count and aggregation are blocking reducer functions because their output depend on all the data. However, join, filter and projection can be implemented in a non-blocking fashion with streaming operators. Non-blocking reducers can be pipelined with other non-blocking reducers, while a blocking reducer breaks the pipeline. Mappers are treated as non-blocking reducers where the output already exists in network/disk storage. We impose some limitations on the kinds of non-blocking operators we support, which are described in detail later. Logically, one can view Quokka execution as a series of stages, where each stage start with the output produced by a blocking operator, ends with another blocking operator, and executes non-blocking operators in between. The entire stage is executed in a pipeline-parallel fashion, and can be viewed as a pure streaming system. The stage inputs/outputs use Spark's lineage tracking based fault-tolerance and persistence mechanism. Since each Quokka stage now corresponds to a few Spark stages, Quokka also implements intra-stage fault tolerance based on checkpointing. The checkpointing recovery mechanism in Quokka conveniently avoids global asynchronous rollbacks, the bane of streaming systems, thanks to the restrictions we impose on the non-blocking operators. Quokka also aims to support autoscaling. (I have a plan to do this, but likely will not get to this until after the rotation.)","title":"Motivation"},{"location":"runtime/#execution-model","text":"The Quokka runtime API allows you to construct a task graph of nodes , which corresponds to a Quokka stage. This is very similar to other DAG-based processing frameworks such as Apache Spark or Tensorflow . For example, you can write the following code in the runtime API to execute TPC-H query 6: task_graph = TaskGraph() lineitem = task_graph.new_input_csv(bucket,key,lineitem_scheme,8,batch_func=lineitem_filter, sep=\"|\") agg_executor = AggExecutor() agged = task_graph.new_blocking_node({0:lineitem}, agg_executor, 1, {0:None}) task_graph.initialize() task_graph.run() There are perhaps a couple of things to note here. Firstly, there are two types of nodes in the runtime API. There are input nodes , declared with APIs such as new_input_csv or new_input_parquet , which interface with the external world (you can define where they will read their data), and task nodes , declared with new_non_blocking_node or new_blocking_node , which take as input the outputs generated from another node in the task graph, either an input node or another task node. Secondly, we see that the task node agged depends on the outputs from the input node lineitem . We will describe what exactly are the types of lineitem and agged later (the former is a stream and the latter is a dataset). Finally, note that the task graph ends with a blocking node. This is currently required, if you want to be able to interact with the results of the task graph execution. Multiple stages are implemented with multiple task graphs, with the first node of stage 2 reading from the output of stage 1, like the following: task_graph = TaskGraph() a = task_graph.new_input_csv(\"bump\",\"a-big.csv\",[\"key\"] + [\"avalue\" + str(i) for i in range(100)],{'localhost':2}) b = task_graph.new_input_csv(\"bump\",\"b-big.csv\",[\"key\"] + [\"bvalue\" + str(i) for i in range(100)],{'localhost':2}) join_executor = OOCJoinExecutor(on=\"key\") output = task_graph.new_blocking_node({0:quotes,1:trades},None, join_executor,{'localhost':4},{0:\"key\", 1:\"key\"}) task_graph.initialize() task_graph.run() del task_graph task_graph2 = TaskGraph() count_executor = CountExecutor() joined_stream = task_graph2.new_input_from_dataset(output,{'localhost':4}) final = task_graph2.new_blocking_node({0:joined_stream}, None, count_executor, {'localhost':4}, {0:'key'}) task_graph2.initialize() task_graph2.run() Note that since the output of a stage is persisted as in Spark, one can delete the first task graph and still access its outputs. Since a task graph represents one Quokka stage, it strictly follows push-based execution. This means that a node does not wait for its downstream dependencies to ask for data, but instead actively pushes data to its downstream dependencies whenever some intermediate results become available. In short, execution proceeds as follows : input nodes read batches of data from a specified source, which might be an external data source or the outputs of a previous stage, and pushes those batches to downstream task nodes. A task node exposes a handler to process incoming batches as they arrive, possibly updating some internal state, and for each input batch possibly produces an output batch for its own downstream children. The programmer is expected to supply this handler function as an executor object (e.g. OOCJoinExecutor , AggExecutor ). Quokka provides a library of pre-implemented executor objects that the programmer can use for SQL, ML and graph analytics. Each task node can have multiple physical executors, referred to as channels . This is a form of intra-operator data parallelism, as opposed to the inter-operator pipeline parallelism that results from all task nodes executing at the same time. These physical executors all execute the same handler function, but on different portions of the input batch, partitioned by a user-specified partition function. A Map-Reduce job with M mappers and R reducers would be implemented in Quokka as a single mapper task node and a single reducer task node, where the mapper task node has M channels and the reducer task node has R channels. In the example above, we specified that the input node lineitem has 8 channels, and the task node agged has only 1 channel. The partition key was not specified ( {0:None} ) since there is no parallelism, thus no need for partitioning. The situation looks something like the following picture: Quokka keeps track of all the channels and schedules them onto physical computing resources. For the engine, two channels from different task nodes are on more or less equal footing -- they can be scheduled on the same hardware or different hardware. A channel from an input node completes execution when there's no more inputs to be read or if all of its downstream dependencies have completed execution. A channel from a task node completes execution when: all of its upstream sources have completed execution if its execution handler decides to terminate early based on the input batch and its state (e.g. for a task node that executes the limit operator in a limit query, it might keep as local state the buffered output, and decide to terminate when that output size surpasses the limit number) if all its downstream dependencies have completed execution. By default, all channels start execution at once. This does not necessarily mean that they will start processing data, this means that they will all start waiting for input batches from their upstream sources to arrive. One could specify that an input node delay execution until another input node has finished. For example to implement a hash join one might want to stream in one table to build the hash table, then stream in the other table for probing. The runtime API is meant to be very flexible and support all manners of batch and stream processing. For example, one could specify an input node that listens to a Kafka stream, some task nodes which processes batches of data from that stream, and an output node that writes to another Kafka stream. In this case, since the input node will never terminate, and assuming the other nodes do not trigger early termination, the task graph will always be running. As a result of this flexibility, it requires quite a lot of knowledge for efficient utilization. As a result, we aim to provide higher level APIs to support common batch and streaming tasks in SQL, machine learning and graph analytics. Most programmers are not expected to program at the runtime API level, but rather make use of the pre-packaged higher-level APIs.","title":"Execution Model"},{"location":"runtime/#stateful-actors","text":"Let's talk more about task nodes in Quokka. Channels in task nodes can be treated as stateful operators in an actor programming model. Quokka adopts the notion of channels in a task node to specify that a group of actors all execute the same code, for fault tolerance and autoscaling purposes. One could override default Quokka behavior by simply specifying different task nodes with one channel each, all executing the same code. The key property of stateful operators in Quokka is confluence : in the context of nondeterministic message delivery, an operation on a single machine is confluent if it produces the same set of outputs for any nondeterministic ordering and batching of a set of inputs. (Hellerstein, CALM) Note that the output itself can also be produced in any order. It\u2019s easy to see that any composition of confluent operators is still confluent. We relax the confluent definition somewhat here to accept potentially different output sets, assuming they are all semantically correct. For example an operator that implements the LIMIT N clause in SQL can admit any of N input records it sees. More importantly, for Quokka we allow operators to depend on intra-stream ordering, just not inter-stream ordering. This means that it might still expect the inputs produced by a certain stream to observe some order, while there are no restrictions on the relative orderings between different input streams. Quokka as a system enforces intra-stream message order, but makes zero gurantees about inter-stream message orders. Henceforth, confluence will refer to this narrow definition, not the one defined in the CALM paper. Confluence is a very nice property to have in general, more so for streaming systems. Let\u2019s imagine a stateful operator with two different upstream operators producing messages. It is very nice if the system\u2019s correctness does not depend on the order in which the two upstream operators produce the messages, which could depend on network delay, task scheduling, etc. This is critical for performance in a push-based framework since a node should never wait on any one of its input streams. In addition, it also greatly facilitates fault tolerance, as messages from different sources can be replayed in any order in regards to one another, as we will describe later. Confluence is perhaps the key difference between Quokka and streaming-centric systems like Flink. In Flink you can totally write pipelines where the outputs depend very strongly on the order the inputs are supplied. In Quokka it is not allowed. (Really at this point, it's only \"not recommended\" -- there are no checks in place to see if your actor is confluent or not. What's guaranteed is that all the operators in the libraries supplied follow this model. Enforcing this is future work.) What are some examples of confluent stateful operators? First let's categorize the world of stateful operators we'd like to implement in data analytics. As mentioned previosuly, there are two important cateogories: nonblocking and blocking . Blocking operators cannot emit any outputs to their downstream children until all of their inputs have been processed. Examples are any kind of aggregation and sort. For (naive) aggregation, the stateful operator does not know it has the final result for any of its aggregation keys until it has seen all of its inputs. For sorting, the stateful operator cannot guarantee that it would emit results in sorted order until it has received all its inputs. We call any operator that is not blocking non-blocking. Example non-blocking operators are map, filter, projection and join. Blocking operators are pipeline breakers, and negate the benefits of using a streaming framework like Quokka. Confluence is easy to reason about for blocking operators. The blocking operator emit only one output, at the very end. We just have to make sure that this output is the same regardless of the order in which we supply the operator's inputs. Since this operator is typically a function of the final state, we just have to ensure that the final state is the same. If we imagine that each incoming message changes the state of the operator by function f , then it's easy to see that as long as f is commutative this is true. For example, any kind of aggregation is commutative, the merge step in merge-sort is commutative, etc. Confluence is harder to reason about for nonblocking operators. We must guarantee that regardless of the order the input batches are supplied, the set of output batches do not change. Let\u2019s say we only have two incoming messages, m and n, to a node with starting state S. Then the outputs produced by giving m first to S, changing the state S to f(m, S), while producing output o(m, S) and then giving n to S, changing the state to f(n, f(m,S)) while producing output o(n, f(m,S)), which is {o(m,S), o(n,f(m,s))} is the same as if we gave the outputs in the reverse order. Note that this assumes that m and n are all the messages the node will see. Confluence is about eventual consistency. While in general there are many ways to achieve this kind of behavior as long as only monotonic operations are applied to the state at each input batch (Bloom), in Quokka all the stock non-blocking operators take the approach of setting the state as sets of immutable batches of data, that can only be added to. This is clearly monotonic. If you are writing a stateful operator for Quokka, this is the recommended approach. What this means is that it is impossible to perform operations that require a specific batch amongst the set of batches, such as list indexing, since ordering of the batches in a set in the state is undefined. Most meaningful operations take the incoming message and produce an output that depends on the entire set, or not at all. An example of a confluent stateful operator in Quokka is a join. The code can be roughly summarized as follows: state0 = set() state1 = set() for each input: if input from stream0: state0.add(input) emit set(input.join(i) for i in state1) else: state1.add(input) emit set(i.join(input) for i in state0) Note that there is in fact a non-monotonic domain-specific optimization we can make that will preserve confluence in the case of a primary key join. Any input streamed in from stream0 can guarantee that any future records from that table will not have the same key value. Thus all state1 related to the record\u2019s key can be safely deleted. Quokka currently does not implement this optimization.","title":"Stateful Actors"},{"location":"runtime/#datasets-and-streams","text":"Let's talk more about how non-blocking and blocking operators work in Quokka. Blocking operators could be introduced by operations like aggregations and sort, or simply by user command when they wish to materialize data with .materialize() (similar to .cache() semantics in Spark or .compute() semantics in Dask). Such blocking operators will produce a Dataset in Quokka, while non-blocking operators will produce a Stream . Downstream operators could depend on both upstream datasets and streams. The difference is that the upstream dataset need to be completely materialized when an operator starts executing, while a stream is just a promise that batches of data will be produced at some point in the future in any order. In other words, from the perspective of the operator, it can pull data from an upstream dataset and expects data to be pushed to it from the stream. In the very first code listing for TPC-H query 6, agged is a dataset whereas lineitem is a stream. In practice, a Quokka DAG can consist of many blocking operators and non-blocking operators organized in complicated ways. For example, here is the DAG for a PageRank application: As previously described, Quokka decomposes the computation into stages, with each stage ending in the creation of a Dataset. In this case the computation will be broken into two stages, the first of which consists of the nonblocking input sparse matrix read and caching (the upper row). The second will be the bottom row. The second stage depends on the first one, so it will be launched after the first one has completed. This is very similar to how stages in Spark work. (Note that strictly speaking, every stage has to start from a Dataset too. In this case the input nodes depend on Datasets that are pre-created in S3 or Disk, and are abbreviated in this graph.) Similarly to an RDD, Quokka represents a Dataset as a collection of immutable objects, and some associated metadata on those objects, which is itself an immutable object. The objects are all stored on a shared-memory object store with persistence (currently RocksDB). When you use task_graph.add_blocking_node in Quokka, a Dataset object will be returned. You can use this Dataset object in downstream operators. Quokka guarantees that by the time the downstream operators execute, all the Datasets that they depend on would have been materialized in this object store. The stock Dataset class in Quokka exposes some convenience methods such as an iterator to iterate through the objects. The user could also interact directly with the object store after looking up metadata from the Dataset object. There are more specialized Dataset class implementations in Quokka like KVDataset or RangeDataset which corresponds to hash-based partitioning or range-based partitioning of objects that expose more methods. The user could also implement a custom Dataset class that descends from Dataset with even more methods. It is important to ensure that when using a Dataset in a downstream operator that also takes streaming inputs, the confluence property is respected. Unfortunately, Quokka currently does not enforce this and it's possible for you to mess this up when writing your code. Although it's not that easy to mess up, since you cannot change the objects you read from the Dataset. A downstream operator could treat the Dataset as a stream by simply invoking the iterator to iterate through the objects in the Dataset. However, for many downstream operations, it might be desirable to explicitly convert a Dataset into a Stream again (e.g. to use stock operators that only have stream-based implementations). You can do that by using the specialized task node add_input_dataset . Internally, this task node just calls the iterator repeatedly and produce a stream of batches corresponding to the objects in the Dataset.","title":"Datasets and Streams"},{"location":"runtime/#fault-tolerance-future-work","text":"The current theory is a bit complicated. I am still thinking through how this should work exactly, but hopefully the gist gets through. Given our group of confluent stateful operators, how do we achieve fault tolerance? A Quokka application can be thought of as a DAG, where each node corresponds to a channel, from one of the task nodes. Each node is assigned to a physical hardware instance. Quokka is designed to expect many nodes to be assigned to one physical instance. For example, let's imagine the following case, where the nodes circled belongs to machine A and the rest belong to machine B, and nodes 1 and 2 are channels of the input node. 3, 4 and 5 are non-blocking operators, 6 and 7 are blocking operators. Quokka follows a checkpoint-based system where each channel periodically asynchronously checkpoints its local state to persistent storage (AWS S3). Note that this is quite efficient given the types of states we typically have, such as (typically) small intermediate aggregation results and sets of batches that are monotonically added to. (This is definitely an area of future work) The problem is easy to spot: \"yes checkpoints are great, but you must turn off the entire system when a machine fails to sync it back to the latest good state, and then reapply all the inputs.\" Yes that is true for a general-purpose streaming system like Flink or Naiad. Coordinated global rollbacks really suck. But in Quokka where all the stateful operators are confluent, this need not happen. What happens when machine A dies? TLDR: machine B can keep doing work as if nothing is wrong, while machine A's workload eventually gets rescheduled. The gory details: nodes 1, 3, 6 and 7 carry on with life (they won't even know machine A just died). 1 will notice that it can no longer send messages to 4 and 5. That's ok, it will just buffer those messages. 3 and 6 will realize that they have fewer incoming messages now. 7 will notice that they have no more incoming messages. That's ok, they can work on their backlog. The system then goes about recovering 2, 4 and 5. It will request a new machine to schedule 2, 4 and 5, or simply schedule them to machine B. 2 is a channel of an input node, which has no state. In Quokka, all message sent between channels are tagged with a sequence number. The number starts from 0 and monotonically increases. This way, the task node discards messages with a wrong sequence number. The state of a stateful operator is also tagged with a number. The state number starts from 0 and monotonically increases every time. When an operator checkpoints, it writes its state, its state number, and the latest sequence number it expects from its inputs. A consistent checkpoint contains all this information. Quokka will look at the last consistent checkpoint of nodes 4 and 5, and find the minimum of all the latest sequence numbers across both consistent checkpoints. This is the batch that 2 will now have to start to produce. Let's say that node 4 had the smaller latest sequence number. Then node 4 will immediately start catching up. Node 5 will look at the incoming batches, find that their sequence numbers are smaller than expected, and proceed to ignore all of them. Eventually, node 5 will start recovering state as well. After both nodes catch up to the point where they died, node 6 and 7 will start accepting messages from node 4 and node 5 since now their messages have valid sequence numbers. What if in this example, node 2 was not an input node but a task node? Then the dead subgraph has no way of re-reading the input. Long story short, each node needs to buffer outgoing messages, until its children notify it that the state change affected by that outgoing message has been persisted to a checkpoint. This way, messages can be replayed when needed. All this while, machine B has been carrying on with its life. This means that if we started out in a load balanced way, then this fault recovery has introduced stragglers -- node 4 and 5 will now finish after node 3. This is actually okay from a resource-usage point of view. Note that nowhere in this process are we wasting resources across the cluster, as seen in global synchronous rollbacks. Only the lost states need to be recomputed, similar in vein to the fault-tolerance mechanism in Spark. In addition, fault recovery required minimal communication with workers that did not experience a fault, minimizing fault recovery overhead. Stragglers are okay for Quokka, we will mediate them through the dynamic scheduling mechanism described in the next section.","title":"Fault tolerance (future work)"},{"location":"runtime/#scheduling-and-autoscaling-future-work","text":"There are two auto-scaling strategies in Quokka. The first is automatic, while the second might require some user input. Recall that Quokka is designed to expect many channels to be assigned to the same physical hardware. But first, let's talk about how Quokka schedules channels to hardware, assuming that the graph is static, and the number and type of machines are fixed. Firstly, in the current runtime API, when instantiating a task node or input node, the user manually specifies how many channels are there and where those channels go. Dynamic channel scheduling is done when programming in higher-level APIs. We observe that each channel is in effect an independent stateful oeprator that can be scheduled independently. However, different scheduling strategies entail different communication costs. If channel A sends a large volume of messages to channel B, then we should schedule them on the same machine. Note that contrary to intuition, there is no benefit at all in scheduling multiple channels from the same input node or task node on the same machine apart from parallelism, since they never talk to each other. Channel scheduling can be dynamic, in the sense that a channel can be moved from one physical machine to another in a very straight-forward way. The self-contained nature of an actor is an oft-quoted strength of the actor model. All that needs to happen is for Quokka to transfer the state of the actor to another node (which could be done asynchronously after the transfer decision is made), and change the partition function for the channel's parents so that the appropriate physical machine receives the incoming messages. The data transfer cost is the only cost in moving an actor. Different criteria can be used to decide if a channel should be moved to another physical machine. These could include machine specific characteristics, such as limited memory available or high CPU usage on the current machine, or the lack thereof on the other machine. Quokka can also use channel-specific information, for example if the system observes the channel transfering large amounts of data to another channel on another machine and determines that the cost in moving this channel can be overcame by the benefit in data locality achieved after the move. The stragglers introduced by fault recovery can be mediated in this fashion. Node 1 and 3 will finish before node 2 and 4/5, creating less resource usage on machine B. The system will then try to move one of node 4/5 onto machine B.","title":"Scheduling and Autoscaling (future work)"},{"location":"runtime/#manual-autoscaling-using-combiner-functions","text":"To be written.","title":"Manual autoscaling using combiner functions"},{"location":"runtime/#example-applications","text":"","title":"Example Applications"},{"location":"runtime/#tpc-h-query-12","text":"","title":"TPC-H query 12"},{"location":"runtime/#pagerank","text":"Let's talk about how PageRank works in the Quokka programming model.","title":"Pagerank"},{"location":"runtime/#taskgraph-api","text":"","title":"TaskGraph API"},{"location":"runtime/#new_input_csv-bucket-key-names-parallelism-iplocalhostbatch_funcnone-sep-dependents-stride-64-1024-1024","text":"Currently, new_input_csv only supports reading a CSV in batches from an AWS S3 bucket. Required arguments in order: bucket : str. AWS S3 bucket key : str. AWS S3 key names : list of str. Column names. Note that if your rows ends with a delimiter value, such as in TPC-H, you will have to end this list with a placeholder such as \"null\". Look at the TPC-H code examples under apps. parallelism : int. the runtime API expects the programmer to explicitly state the amount of intra-op parallelism to expose. 8 is typically a good number. Keyword arguments: ip : str. the IP address of the physical machine the input node should be placed. Defaults to local execution. batch_func : function. the user can optionally pass in a function to execute on the input CSV chunk before it's passed off to downstream dependents. Currently the input CSV is parsed into a Pandas Dataframe, so batch_func can be any Python function that can take a Pandas Dataframe as input and produces a Pandas Dataframe. This can be done to perform predicate pushdown for SQL for example. sep : str. delimiter dependents : list of int. an input node can depend on other input nodes, i.e. only start once another input node is done. For example to implement as hash join where one input might depend on another, one could do the following: a = new_input_csv(...) b = new_input_csv(...,dependents=[a]) stide : int. how many bytes to read from the input S3 file to read at a time, default to 64 MB. Returns : a node id which is a handle to this input node, that can be used as the sources argument for task nodes or dependents arguments for other input nodes.","title":"new_input_csv (bucket, key, names, parallelism, ip='localhost',batch_func=None, sep = \",\", dependents = [], stride = 64 * 1024 * 1024)"},{"location":"runtime/#new_input_parquetbucket-key-names-parallelism-columns-skip_conditions-iplocalhostbatch_funcnone-sep-dependents-stride-64-1024-1024","text":"Not yet implemented.","title":"new_input_parquet(bucket, key, names, parallelism, columns, skip_conditions, ip='localhost',batch_func=None, sep = \",\", dependents = [], stride = 64 * 1024 * 1024)"},{"location":"runtime/#new_task_nodesources-functionobject-parallelism-partition_key-iplocalhost","text":"Instantiate a new task node with an executor object that defines the handler function which runs on each incoming batch. Required arguments in order: sources : dict of int -> int. the upstream sources that feed batches to this task node. Expects a dictionary, where the keys are integers and values are node ids (also stored as integers). This in effect names the source nodes. i.e. if you specify {0: source_node_id_x, 1:source_node_id_y} , from the perspective of this task node you are calling the batches coming from source_node_id_x source 0 and the batches coming from node_id_y source 1. You will make use of these identifiers writing the executor class's handler function for incoming batches. functionObject : an executor object which defines the input batch handler function. More details on this in the next section. You can write your own or use a pre-supplied one from the sql, ml or graph packages. parallelism : int. the runtime API expects the programmer to explicitly state the amount of intra-op parallelism to expose. Think carefully about this choice. Computationally intensive tasks might benefit from parallelism, while simple tasks such as aggregation might not. partition_key : dict of int -> in. This argument expects a dictionary with a key for each key in the sources dict. It describes how the input batches should be partitioned amongst the channels. If the value is None, then the input batch is copied and broadcast to all channels. Otherwise, currently each channel receives the sub-batch input_batch[input_batch.partition_key % parallelism == channel_id]. If this partition key is not in the input batch's columns from the specified source node, a runtime error would ensue. Keyword arguments: ip : str. the IP address of the physical machine the input node should be placed. Defaults to local execution.","title":"new_task_node(sources, functionObject, parallelism, partition_key, ip='localhost')"},{"location":"runtime/#writing-your-own-stateless-executor-object","text":"The best place to learn how to write your own executor object classes is by looking at the available executor object classes in the SQL library. In short, an executor class is simply a child class of this base class: class StatelessExecutor: def __init__(self) -> None: raise NotImplementedError def early_termination(self): self.early_termination = True def execute(self,batch,stream_id, executor_id): raise NotImplementedError def done(self,executor_id): raise NotImplementedError The Stateless","title":"Writing Your Own (Stateless) Executor Object"},{"location":"simple/","text":"Tutorials This section is for learning how to use Quokka's DataStream API. Quokka's DataStream API is basically a dataframe API. It takes heavy inspiration from SparkSQL and Polars, and adopts a lazy execution model. This means that in contrast to Pandas, your operations are not executed immediately after you define them. Instead, Quokka builds a logical plan under the hood and executes it only when the user wants to \"collect\" the result, just like Spark. For the first part of our tutorial, we are going to go through implementing a few SQL queries in the TPC-H benchmark suite. You can download the data here . It is about 1GB unzipped. Please download the data (should take 2 minutes) and extract it to some directory locally. If you are testing this on a VM where clicking the link can't work, try this command after pip installing gdown: gdown https://drive.google.com/uc?id=14yDfWZUAxifM5i7kf7CFvOQCIrU7sRXP . If it complain gdown not found, maybe you need to do something like this: /home/ubuntu/.local/bin/gdown . The SQL queries themselves can be found on this awesome interface . These tutorials will use your local machine. They shouldn't take too long to run. It would be great if you can follow along, not just for fun -- if you find a bug in this tutorial I will buy you a cup of coffee! You can also refer to the API reference on the index. Lesson -1: Things Please read the Getting Started section. I spent way too much time making the cartoons on that page. Lesson 0: Reading Things For every Quokka program, we need to set up a QuokkaContext object. This is similar to the Spark SQLContext . This can easily be done by running the following two lines of code in your Python terminal. from pyquokka.df import * qc = QuokkaContext() Once we have the QuokkaContext object, we can start reading data to obtain DataStreams. Quokka can read data on disk and on the cloud (currently S3). For the purposes of this tutorial we will be reading data from disk. Quokka currently reads CSV and Parquet, with plans to add JSON soon. Here is how you would read a CSV file if you know the schema : # the last column is called NULL, because the TPC-H data generator likes to put a | at the end of each row, making it appear as if there is a final column # with no values. Don't worry, we can drop this column. lineitem_scheme = [\"l_orderkey\",\"l_partkey\",\"l_suppkey\",\"l_linenumber\",\"l_quantity\",\"l_extendedprice\", \"l_discount\",\"l_tax\",\"l_returnflag\",\"l_linestatus\",\"l_shipdate\",\"l_commitdate\",\"l_receiptdate\",\"l_shipinstruct\",\"l_shipmode\",\"l_comment\", \"null\"] lineitem = qc.read_csv(disk_path + \"lineitem.tbl\", lineitem_scheme, sep=\"|\") And if you don't know the schema but there is a header row where column names are separated with the same separator as the data : lineitem = qc.read_csv(disk_path + \"lineitem.tbl\", sep=\"|\", has_header=True) The test files you just downloaded are of this form. No need to specify the schema for those. In this case Quokka will just use the header row for the schema. You can also read a directory of CSV files: lineitem = qc.read_csv(disk_path + \"lineitem/*\", lineitem_scheme, sep=\"|\", has_header = True) Now let's read all the tables of the TPC-H benchmark suite. Set disk_path to where you unzipped the files. lineitem = qc.read_csv(disk_path + \"lineitem.tbl\", sep=\"|\", has_header=True) orders = qc.read_csv(disk_path + \"orders.tbl\", sep=\"|\", has_header=True) customer = qc.read_csv(disk_path + \"customer.tbl\",sep = \"|\", has_header=True) part = qc.read_csv(disk_path + \"part.tbl\", sep = \"|\", has_header=True) supplier = qc.read_csv(disk_path + \"supplier.tbl\", sep = \"|\", has_header=True) partsupp = qc.read_csv(disk_path + \"partsupp.tbl\", sep = \"|\", has_header=True) nation = qc.read_csv(disk_path + \"nation.tbl\", sep = \"|\", has_header=True) region = qc.read_csv(disk_path + \"region.tbl\", sep = \"|\", has_header=True) If you want to read the Parquet files, you should first run this script to generate the Parquet files: import polars as pl disk_path = \"/home/ubuntu/tpc-h/\" #replace files = [\"lineitem.tbl\",\"orders.tbl\",\"customer.tbl\",\"part.tbl\",\"supplier.tbl\",\"partsupp.tbl\",\"nation.tbl\",\"region.tbl\"] for file in files: df = pl.read_csv(disk_path + file,sep=\"|\",has_header = True, parse_dates = True).drop(\"null\") df.write_parquet(disk_path + file.replace(\"tbl\", \"parquet\"), row_group_size=100000) To read in a Parquet file, you don't have to worry about headers or schema, just do: lineitem = qc.read_parquet(disk_path + \"lineitem.parquet\") Currently, qc.read_csv and qc.read_parquet will either return a DataStream or just a Polars DataFrame directly if the data size is small (set at 10 MB). Lesson 1: Doing Things Now that we have read the data, let's do things with it. First, why don't we count how many rows there are in the lineitem table. lineitem.count() If you don't see the number 6001215 after a while, something is very wrong. Please send me an email, I will help you fix things (and buy you a coffee): zihengw@stanford.edu. Feel free to type other random things and see if it's supported, but for those interested, let's follow a structured curriculum. Let's take a look at TPC-H query 1 . This is how you would write it in Quokka. This is very similar to how you'd write in another DataFrame library like Polars or Dask. def do_1(): d = lineitem.filter_sql(\"l_shipdate <= date '1998-12-01' - interval '90' day\") d = d.with_columns({\"disc_price\": d[\"l_extendedprice\"] * (1 - d[\"l_discount\"]), \"charge\": d[\"l_extendedprice\"] * (1 - d[\"l_discount\"]) * (1 + d[\"l_tax\"])}) f = d.groupby([\"l_returnflag\", \"l_linestatus\"], orderby=[\"l_returnflag\",\"l_linestatus\"]).agg({\"l_quantity\":[\"sum\",\"avg\"], \"l_extendedprice\":[\"sum\",\"avg\"], \"disc_price\":\"sum\", \"charge\":\"sum\", \"l_discount\":\"avg\",\"*\":\"count\"}) return f.collect() Quokka supports filtering DataStreams by DataStream.filter() . Filters can be specified in SQL syntax. The columns in the SQL expression must exist in the schema of the DataStream. A more Pythonic way of doing this like b = b[b.a < 5] isn't supported yet, mainly due to the finickiness surrounding date types etc. The result of a filter() is another DataStream whose Polars DataFrames will only contain rows that respect the predicate. On the plus side, Quokka uses the amazing SQLGlot library to support most ANSI-SQL compliant predicates, including dates, between, IN, even arithmetic in conditions. Try out some different predicates ! Please give SQLGlot a star when you're at it. For example, you can specify this super complicated predicate for TPC-H query 6 : def do_6(): d = lineitem.filter_sql(\"l_shipdate >= date '1994-01-01' and l_shipdate < date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01 and l_quantity < 24\") d = d.with_columns({\"revenue\": lambda x: x[\"l_extendedprice\"] * x[\"l_discount\"]}, required_columns={\"l_extendedprice\", \"l_discount\"}) f = d.aggregate({\"revenue\":[\"sum\"]}) return f.collect() Quokka supports creating new columns in DataStreams with with_columns . Read more about how this works here . This is in principle similar to Spark df.with_column and Pandas UDFs. You can also use with_columns_sql , documented here . Like most Quokka operations, with_columns will produce a new DataStream with an added column and is not inplace. This means that the command is lazy, and won't trigger the runtime to produce the actual data. It simply builds a logical plan of what to do in the background, which can be optimized when the user specifically ask for the result. Finally, we can group the DataStream and aggregate it to get the result. Read more about aggregation syntax here . The aggregation will produce another DataStream, which we call collect() on, to convert it to a Polars DataFrame in your Python terminal. When you call .collect() , the logical plan you have built is actually optimized and executed. This is exactly how Spark works. To view the optimized logical plan and learn more about what Quokka is doing, you can do f.explain() which will produce a graph, or f.explain(mode=\"text\") which will produce a textual explanation. Joins work very intuitively. For example, this is how to do TPC-H query 12 . def do_12(): d = lineitem.join(orders,left_on=\"l_orderkey\", right_on=\"o_orderkey\") d = d.filter_sql(\"l_shipmode IN ('MAIL','SHIP') and l_commitdate < l_receiptdate and l_shipdate < l_commitdate and \\ l_receiptdate >= date '1994-01-01' and l_receiptdate < date '1995-01-01'\") f = d.groupby(\"l_shipmode\").agg_sql(\"\"\" sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, sum(case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end) as low_line_count \"\"\") return f.collect() Note it does not matter if you filter after the join or before the join, Quokka will automatically push them down during the logical plan optimization. The join operator on a DataStream takes in either another DataStream or a Polars DataFrame in your Python session. In the latter case, this Polars DataFrame will be broadcasted to different workers similar to Spark's broadcast join. Here is another example, TPC-H query 3 . def do_3(): d = lineitem.join(orders,left_on=\"l_orderkey\", right_on=\"o_orderkey\") d = customer.join(d,left_on=\"c_custkey\", right_on=\"o_custkey\") d = d.filter_sql(\"c_mktsegment = 'BUILDING' and o_orderdate < date '1995-03-15' and l_shipdate > date '1995-03-15'\") d = d.with_columns({\"revenue\": d[\"l_extendedprice\"] * ( 1 - d[\"l_discount\"])}) f = d.groupby([\"l_orderkey\",\"o_orderdate\",\"o_shippriority\"]).agg({\"revenue\":[\"sum\"]}) return f.collect() Note unlike some SQL engines, Quokka currently will not try to figure out the optimal join ordering between the specified three-way join between lineitem, orders and customer tables. You are responsible for figuring that out at the moment -- try to join smaller tables first and then join them against larger tables, or try to minimize the intermeidate result size from those joins. Feel free to look at some other queries in the Quokka github , or browse the API references. While you are there, please give Quokka a star! Lesson 2: Writing Things So far, we have just learned about how to read things into DataStreams and do things to DataStreams. You can also write out DataStreams to persistent storage like disk or S3 to record all the amazing things we did with them. Quokka currently operates like Spark and by default writes a directory of files, with a default maximum file size for different file formats. This makes it easy to perform parallel writing. To write out a DataStream to CSV or Parquet to a local directory (you must specify a valid absolute path), simply do: d.write_csv(\"/home/ubuntu/test-path/\").collect() d.write_parquet(\"/home/ubuntu/test-path/\").collect() To write out a DataStream to S3, you should specify an S3 bucket and prefix like this: d.write_csv(\"s3://bucket/prefix/\").collect() d.write_parquet(\"s3://bucket/prefix/\").collect() The collected Polars DataFrame at the end is just a column of filenames produced. Lesson 3: Things you can't do. Here is a brief discussion of what Quokka is not great for. Quokka's main advantage stems from the fact it can pipeline the execution of DataStreams. Once a partition (typically a Polars DataFrame) in a DataStream has been generated, it can be immediately consumed by a downstream user. This means downstream processing of this partition and upstream generation of the next partition can be overlapped. Now, if an operator processing a DataStream cannot emit any partitions downstream until it has seen all of the partitions in its input DataStreams, the pipeline breaks. An example of this is an aggregation. You cannot safely emit the result of a sum of a column of a table until you have seen every row! The main examples of this in data processing are groupby-aggregations and distributed sorts. Currently, calling groupby().agg() or just agg() on a DataStream will produce another DataStream. However that DataStream will consist of exactly one batch, which holds the final result, emitted when it's computed. It is recommended to just call collect() or compute() on that result. Quokka currently does not support distributed sort -- indeed a sort heavy workload is really great for Spark. Distributed sorting is not exactly needed for many analytical SQL workloads since you typically do the aggregation before the order by, which greatly reduce the number of rows you have to sort. You can then sort after you have done collect() . However for many other workloads distributed sorting is critical, and Quokka aims to support this as soon as possible. Things that Quokka can do and doesn't do yet: fine grained placement of UDFs or UDAFs on GPUs or CPUs, core-count-control, Docker support, reading JSON, etc. Most of these can be easily implemented (and some already are) in the graph level API, however it takes effort to figure out what's the best abstractions to expose in the DataStream API. If you want to make this list shorter, I welcome contributions: zihengw@stanford.edu.","title":"DataStream API (Use Quokka)"},{"location":"simple/#tutorials","text":"This section is for learning how to use Quokka's DataStream API. Quokka's DataStream API is basically a dataframe API. It takes heavy inspiration from SparkSQL and Polars, and adopts a lazy execution model. This means that in contrast to Pandas, your operations are not executed immediately after you define them. Instead, Quokka builds a logical plan under the hood and executes it only when the user wants to \"collect\" the result, just like Spark. For the first part of our tutorial, we are going to go through implementing a few SQL queries in the TPC-H benchmark suite. You can download the data here . It is about 1GB unzipped. Please download the data (should take 2 minutes) and extract it to some directory locally. If you are testing this on a VM where clicking the link can't work, try this command after pip installing gdown: gdown https://drive.google.com/uc?id=14yDfWZUAxifM5i7kf7CFvOQCIrU7sRXP . If it complain gdown not found, maybe you need to do something like this: /home/ubuntu/.local/bin/gdown . The SQL queries themselves can be found on this awesome interface . These tutorials will use your local machine. They shouldn't take too long to run. It would be great if you can follow along, not just for fun -- if you find a bug in this tutorial I will buy you a cup of coffee! You can also refer to the API reference on the index.","title":"Tutorials"},{"location":"simple/#lesson-1-things","text":"Please read the Getting Started section. I spent way too much time making the cartoons on that page.","title":"Lesson -1: Things"},{"location":"simple/#lesson-0-reading-things","text":"For every Quokka program, we need to set up a QuokkaContext object. This is similar to the Spark SQLContext . This can easily be done by running the following two lines of code in your Python terminal. from pyquokka.df import * qc = QuokkaContext() Once we have the QuokkaContext object, we can start reading data to obtain DataStreams. Quokka can read data on disk and on the cloud (currently S3). For the purposes of this tutorial we will be reading data from disk. Quokka currently reads CSV and Parquet, with plans to add JSON soon. Here is how you would read a CSV file if you know the schema : # the last column is called NULL, because the TPC-H data generator likes to put a | at the end of each row, making it appear as if there is a final column # with no values. Don't worry, we can drop this column. lineitem_scheme = [\"l_orderkey\",\"l_partkey\",\"l_suppkey\",\"l_linenumber\",\"l_quantity\",\"l_extendedprice\", \"l_discount\",\"l_tax\",\"l_returnflag\",\"l_linestatus\",\"l_shipdate\",\"l_commitdate\",\"l_receiptdate\",\"l_shipinstruct\",\"l_shipmode\",\"l_comment\", \"null\"] lineitem = qc.read_csv(disk_path + \"lineitem.tbl\", lineitem_scheme, sep=\"|\") And if you don't know the schema but there is a header row where column names are separated with the same separator as the data : lineitem = qc.read_csv(disk_path + \"lineitem.tbl\", sep=\"|\", has_header=True) The test files you just downloaded are of this form. No need to specify the schema for those. In this case Quokka will just use the header row for the schema. You can also read a directory of CSV files: lineitem = qc.read_csv(disk_path + \"lineitem/*\", lineitem_scheme, sep=\"|\", has_header = True) Now let's read all the tables of the TPC-H benchmark suite. Set disk_path to where you unzipped the files. lineitem = qc.read_csv(disk_path + \"lineitem.tbl\", sep=\"|\", has_header=True) orders = qc.read_csv(disk_path + \"orders.tbl\", sep=\"|\", has_header=True) customer = qc.read_csv(disk_path + \"customer.tbl\",sep = \"|\", has_header=True) part = qc.read_csv(disk_path + \"part.tbl\", sep = \"|\", has_header=True) supplier = qc.read_csv(disk_path + \"supplier.tbl\", sep = \"|\", has_header=True) partsupp = qc.read_csv(disk_path + \"partsupp.tbl\", sep = \"|\", has_header=True) nation = qc.read_csv(disk_path + \"nation.tbl\", sep = \"|\", has_header=True) region = qc.read_csv(disk_path + \"region.tbl\", sep = \"|\", has_header=True) If you want to read the Parquet files, you should first run this script to generate the Parquet files: import polars as pl disk_path = \"/home/ubuntu/tpc-h/\" #replace files = [\"lineitem.tbl\",\"orders.tbl\",\"customer.tbl\",\"part.tbl\",\"supplier.tbl\",\"partsupp.tbl\",\"nation.tbl\",\"region.tbl\"] for file in files: df = pl.read_csv(disk_path + file,sep=\"|\",has_header = True, parse_dates = True).drop(\"null\") df.write_parquet(disk_path + file.replace(\"tbl\", \"parquet\"), row_group_size=100000) To read in a Parquet file, you don't have to worry about headers or schema, just do: lineitem = qc.read_parquet(disk_path + \"lineitem.parquet\") Currently, qc.read_csv and qc.read_parquet will either return a DataStream or just a Polars DataFrame directly if the data size is small (set at 10 MB).","title":"Lesson 0: Reading Things"},{"location":"simple/#lesson-1-doing-things","text":"Now that we have read the data, let's do things with it. First, why don't we count how many rows there are in the lineitem table. lineitem.count() If you don't see the number 6001215 after a while, something is very wrong. Please send me an email, I will help you fix things (and buy you a coffee): zihengw@stanford.edu. Feel free to type other random things and see if it's supported, but for those interested, let's follow a structured curriculum. Let's take a look at TPC-H query 1 . This is how you would write it in Quokka. This is very similar to how you'd write in another DataFrame library like Polars or Dask. def do_1(): d = lineitem.filter_sql(\"l_shipdate <= date '1998-12-01' - interval '90' day\") d = d.with_columns({\"disc_price\": d[\"l_extendedprice\"] * (1 - d[\"l_discount\"]), \"charge\": d[\"l_extendedprice\"] * (1 - d[\"l_discount\"]) * (1 + d[\"l_tax\"])}) f = d.groupby([\"l_returnflag\", \"l_linestatus\"], orderby=[\"l_returnflag\",\"l_linestatus\"]).agg({\"l_quantity\":[\"sum\",\"avg\"], \"l_extendedprice\":[\"sum\",\"avg\"], \"disc_price\":\"sum\", \"charge\":\"sum\", \"l_discount\":\"avg\",\"*\":\"count\"}) return f.collect() Quokka supports filtering DataStreams by DataStream.filter() . Filters can be specified in SQL syntax. The columns in the SQL expression must exist in the schema of the DataStream. A more Pythonic way of doing this like b = b[b.a < 5] isn't supported yet, mainly due to the finickiness surrounding date types etc. The result of a filter() is another DataStream whose Polars DataFrames will only contain rows that respect the predicate. On the plus side, Quokka uses the amazing SQLGlot library to support most ANSI-SQL compliant predicates, including dates, between, IN, even arithmetic in conditions. Try out some different predicates ! Please give SQLGlot a star when you're at it. For example, you can specify this super complicated predicate for TPC-H query 6 : def do_6(): d = lineitem.filter_sql(\"l_shipdate >= date '1994-01-01' and l_shipdate < date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01 and l_quantity < 24\") d = d.with_columns({\"revenue\": lambda x: x[\"l_extendedprice\"] * x[\"l_discount\"]}, required_columns={\"l_extendedprice\", \"l_discount\"}) f = d.aggregate({\"revenue\":[\"sum\"]}) return f.collect() Quokka supports creating new columns in DataStreams with with_columns . Read more about how this works here . This is in principle similar to Spark df.with_column and Pandas UDFs. You can also use with_columns_sql , documented here . Like most Quokka operations, with_columns will produce a new DataStream with an added column and is not inplace. This means that the command is lazy, and won't trigger the runtime to produce the actual data. It simply builds a logical plan of what to do in the background, which can be optimized when the user specifically ask for the result. Finally, we can group the DataStream and aggregate it to get the result. Read more about aggregation syntax here . The aggregation will produce another DataStream, which we call collect() on, to convert it to a Polars DataFrame in your Python terminal. When you call .collect() , the logical plan you have built is actually optimized and executed. This is exactly how Spark works. To view the optimized logical plan and learn more about what Quokka is doing, you can do f.explain() which will produce a graph, or f.explain(mode=\"text\") which will produce a textual explanation. Joins work very intuitively. For example, this is how to do TPC-H query 12 . def do_12(): d = lineitem.join(orders,left_on=\"l_orderkey\", right_on=\"o_orderkey\") d = d.filter_sql(\"l_shipmode IN ('MAIL','SHIP') and l_commitdate < l_receiptdate and l_shipdate < l_commitdate and \\ l_receiptdate >= date '1994-01-01' and l_receiptdate < date '1995-01-01'\") f = d.groupby(\"l_shipmode\").agg_sql(\"\"\" sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, sum(case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end) as low_line_count \"\"\") return f.collect() Note it does not matter if you filter after the join or before the join, Quokka will automatically push them down during the logical plan optimization. The join operator on a DataStream takes in either another DataStream or a Polars DataFrame in your Python session. In the latter case, this Polars DataFrame will be broadcasted to different workers similar to Spark's broadcast join. Here is another example, TPC-H query 3 . def do_3(): d = lineitem.join(orders,left_on=\"l_orderkey\", right_on=\"o_orderkey\") d = customer.join(d,left_on=\"c_custkey\", right_on=\"o_custkey\") d = d.filter_sql(\"c_mktsegment = 'BUILDING' and o_orderdate < date '1995-03-15' and l_shipdate > date '1995-03-15'\") d = d.with_columns({\"revenue\": d[\"l_extendedprice\"] * ( 1 - d[\"l_discount\"])}) f = d.groupby([\"l_orderkey\",\"o_orderdate\",\"o_shippriority\"]).agg({\"revenue\":[\"sum\"]}) return f.collect() Note unlike some SQL engines, Quokka currently will not try to figure out the optimal join ordering between the specified three-way join between lineitem, orders and customer tables. You are responsible for figuring that out at the moment -- try to join smaller tables first and then join them against larger tables, or try to minimize the intermeidate result size from those joins. Feel free to look at some other queries in the Quokka github , or browse the API references. While you are there, please give Quokka a star!","title":"Lesson 1: Doing Things"},{"location":"simple/#lesson-2-writing-things","text":"So far, we have just learned about how to read things into DataStreams and do things to DataStreams. You can also write out DataStreams to persistent storage like disk or S3 to record all the amazing things we did with them. Quokka currently operates like Spark and by default writes a directory of files, with a default maximum file size for different file formats. This makes it easy to perform parallel writing. To write out a DataStream to CSV or Parquet to a local directory (you must specify a valid absolute path), simply do: d.write_csv(\"/home/ubuntu/test-path/\").collect() d.write_parquet(\"/home/ubuntu/test-path/\").collect() To write out a DataStream to S3, you should specify an S3 bucket and prefix like this: d.write_csv(\"s3://bucket/prefix/\").collect() d.write_parquet(\"s3://bucket/prefix/\").collect() The collected Polars DataFrame at the end is just a column of filenames produced.","title":"Lesson 2: Writing Things"},{"location":"simple/#lesson-3-things-you-cant-do","text":"Here is a brief discussion of what Quokka is not great for. Quokka's main advantage stems from the fact it can pipeline the execution of DataStreams. Once a partition (typically a Polars DataFrame) in a DataStream has been generated, it can be immediately consumed by a downstream user. This means downstream processing of this partition and upstream generation of the next partition can be overlapped. Now, if an operator processing a DataStream cannot emit any partitions downstream until it has seen all of the partitions in its input DataStreams, the pipeline breaks. An example of this is an aggregation. You cannot safely emit the result of a sum of a column of a table until you have seen every row! The main examples of this in data processing are groupby-aggregations and distributed sorts. Currently, calling groupby().agg() or just agg() on a DataStream will produce another DataStream. However that DataStream will consist of exactly one batch, which holds the final result, emitted when it's computed. It is recommended to just call collect() or compute() on that result. Quokka currently does not support distributed sort -- indeed a sort heavy workload is really great for Spark. Distributed sorting is not exactly needed for many analytical SQL workloads since you typically do the aggregation before the order by, which greatly reduce the number of rows you have to sort. You can then sort after you have done collect() . However for many other workloads distributed sorting is critical, and Quokka aims to support this as soon as possible. Things that Quokka can do and doesn't do yet: fine grained placement of UDFs or UDAFs on GPUs or CPUs, core-count-control, Docker support, reading JSON, etc. Most of these can be easily implemented (and some already are) in the graph level API, however it takes effort to figure out what's the best abstractions to expose in the DataStream API. If you want to make this list shorter, I welcome contributions: zihengw@stanford.edu.","title":"Lesson 3: Things you can't do."},{"location":"started/","text":"Quokka in Three Cartoons The fundamental concept in Quokka is a stream of Polars DataFrames , which we call a DataStream . A Polars DataFrame is basically a Pandas DataFrame, except that it's backed by Apache Arrow and supports fast compute with Polars . Readers familiar with Spark RDDs can interpret a DataStream as an RDD where data partitions are materialized in sequence. In contrast to Spark, partitions can be consumed as soon as they are generated. This facilitates pipeline parallelism between multiple data processing stages and makes Quokka really fast. The user defines input readers that generate a DataStream from a dataset. For example, Quokka's cloud CSV reader generates a DataStream from an S3 bucket of CSV files. The user can also define stateful operators that operate on one or more DataStreams to produce one more DataStream. Finally a DataStream could be written to an output sink , which could be a distributed in-memory dataset that can be converted to Pandas or stable storage on disk or S3. In this illustration, the bush produces a DataStream of leaves and the forest produces a DataStream of acorns. The brown quokka consumes those two streams and magically turn it into a stream of strawberries. The grey quokka takes in this stream of strawberries, slices them up and puts them in a salad bowl. Unfortunately, people like us can't slice strawberries for a living and have to process tables of numbers. Quokka exposes useful primitives that allow you to filter, aggregate and join DataStreams, similar to what you can do in Polars or Spark. Please look at the tutorials to learn more. It would be a dismal world if there is only one quokka of each kind. Quokka supports parallelism for stateful operators with channels , which are parallel instantiations of a stateful operator to achieve data parallelism. Input sources can also have channels to parallelize the reading of a data source. For example, we can have two bushes and two forests, and four brown quokkas. While the user can manually specify the number of channels they want for operators, in most cases it's automagically decided for you based on what you are doing, similar to Spark. Each channel in an input source or stateful operator can be scheduled independently to a machine in a cluster. Actors on the same machine talk to each other through memory while actors on different machines communicate through the network. An example scheduling of our quokkas: The user shouldn't have to worry about this scheduling in most cases if using the DataStream API. However I couldn't resist making this cartoon, and it might be cool to know how Quokka works under the hood. Image credits: some icons taken from flaticon.com.","title":"Cartoons"},{"location":"started/#quokka-in-three-cartoons","text":"The fundamental concept in Quokka is a stream of Polars DataFrames , which we call a DataStream . A Polars DataFrame is basically a Pandas DataFrame, except that it's backed by Apache Arrow and supports fast compute with Polars . Readers familiar with Spark RDDs can interpret a DataStream as an RDD where data partitions are materialized in sequence. In contrast to Spark, partitions can be consumed as soon as they are generated. This facilitates pipeline parallelism between multiple data processing stages and makes Quokka really fast. The user defines input readers that generate a DataStream from a dataset. For example, Quokka's cloud CSV reader generates a DataStream from an S3 bucket of CSV files. The user can also define stateful operators that operate on one or more DataStreams to produce one more DataStream. Finally a DataStream could be written to an output sink , which could be a distributed in-memory dataset that can be converted to Pandas or stable storage on disk or S3. In this illustration, the bush produces a DataStream of leaves and the forest produces a DataStream of acorns. The brown quokka consumes those two streams and magically turn it into a stream of strawberries. The grey quokka takes in this stream of strawberries, slices them up and puts them in a salad bowl. Unfortunately, people like us can't slice strawberries for a living and have to process tables of numbers. Quokka exposes useful primitives that allow you to filter, aggregate and join DataStreams, similar to what you can do in Polars or Spark. Please look at the tutorials to learn more. It would be a dismal world if there is only one quokka of each kind. Quokka supports parallelism for stateful operators with channels , which are parallel instantiations of a stateful operator to achieve data parallelism. Input sources can also have channels to parallelize the reading of a data source. For example, we can have two bushes and two forests, and four brown quokkas. While the user can manually specify the number of channels they want for operators, in most cases it's automagically decided for you based on what you are doing, similar to Spark. Each channel in an input source or stateful operator can be scheduled independently to a machine in a cluster. Actors on the same machine talk to each other through memory while actors on different machines communicate through the network. An example scheduling of our quokkas: The user shouldn't have to worry about this scheduling in most cases if using the DataStream API. However I couldn't resist making this cartoon, and it might be cool to know how Quokka works under the hood. Image credits: some icons taken from flaticon.com.","title":"Quokka in Three Cartoons"},{"location":"tutorial/","text":"Advanced Tutorials This section is for learning how to use Quokka's graph level API. This is expected for use cases where the DataStream API cannot satisfy your needs. Most users are not expected to program at this level. You should contact me: zihengw@stanford.edu or hit me up on Discord if you want to do this. You should probably stop reading now, unless you are a Stanford undergrad or masters student (or somebody else) who somehow decided to work with me on Quokka. You should read this tutorial if you want to add a new executor or input reader into Quokka. You should probably read up on the Cartoons section first, and then come back here. You probably should also first run the DataStream Tutorial to make sure Quokka actually works on your machine. The code for the tutorials can be found under apps/tutorials . They might perform meaningless tasks or perform tasks which you shoudn't necessarily use Quokka for, but they will showcase how Quokka works. I wrote Quokka. As a result I might take some things for granted that you might not. If you spot a typo or find some sections too difficult to understand, I would appreciate your feedback! Better yet, the docs are also open source under quokka/docs, so you can also make a PR. Lesson 0: Addition Let's walk through our first Quokka program. This first example defines an input reader which produces a stream of numbers, and a stateful operator which adds them up. Let's first look at the import section. In general when you are writing a Quokka program, you will need to import TaskGraph , LocalCluster and Executor . TaskGraph is the main object that you will use to define your program. LocalCluster is the execution context for the program, which you can replace with an EC2Cluster made from QuokkaClusterManager . Executor is an abstract class which you should extend to implement your own executors. Quokka also provides canned executors which you call import from pyquokka.executors such as join . from pyquokka.quokka_runtime import TaskGraph from pyquokka.utils import LocalCluster from pyquokka.executors import Executor from pyquokka.target_info import TargetInfo, PassThroughPartitioner from pyquokka.placement_strategy import SingleChannelStrategy import sqlglot import time import polars Quokka provides many optimized input readers for different input data formats. However, in this tutorial we are going to define a custom input reader class to showcase how the input reader works. The mindset here is that there will be many channels of this input reader spread across your cluster, and each channel will have its own copy of an object of this class. They will all be initialized in the same way, but each channel can produce its own data. An input reader object can be initialized with arbitrary arguments. The initialization will be performed locally before shipping the object over the network to the executors (in your code), so you need to make sure that the object can be pickled. This means that you can't initialize the object with a socket or a file descriptor. class SimpleDataset: # we define limit here, which is the max number we will generate def __init__(self, limit) -> None: self.limit = limit self.num_channels = None The key to implementing an input reader object lies in the get_own_state method implementation. This method will be called by the runtime once it decided how many channels this input reader will have. Armed with this information as well as the arguments passed to the object's constructor, the method should return a dictionary which maps channel ids to what the channel is assigned to produce . In this example, we will have N channels, and each channel will produce numbers k, k + N, k + 2N, all the way up to the limit. The get_own_state method will be called locally by the runtime after the object is initialized (not in your code though). If the method adds some state variables to the object, then they should be picklable too. Now what does \" what the channel is assigned to produce \" even mean? It is supposed to be a list of arbitrary picklable objects, though I'd recommend to keep it simple and stick to primitive types and at most tuples. What should these objects be? They are going to be fed as input arguments to the execute method, which will be executed by the channels across the cluster on their own copy of the input reader object. In this dead simple example, the list just contains the numbers a channel is supposed to produce, and the execute method simply wraps the number in a Polars DataFrame and spits it out. In more complicated examples, such as a CSV reader, the list typically contains the file names and offsets of the CSV files the channel is supposed to read. The execute method then reads the correct portion of the correct CSV file and returns the data. The mental model should be that the get_own_state method, executed once locally on the client, determines what each channel is supposed to produce, and the execute method, executed on each channel across the cluster, actually produces the data. On channel x , the execute method is called len(channel_info[x]) times, once for each of the values in the list, in order. The return value of the execute method is always None followed by the data item, which can be a Pyarrow table of Polars Dataframe . If you really want to know why the None is required, email me and I'll explain over coffee. def get_own_state(self, num_channels): channel_info = {} for channel in range(num_channels): channel_info[channel] = [i for i in range(channel, self.limit, num_channels)] return channel_info def execute(self, channel, state = None): curr_number = state return None, polars.DataFrame({\"number\": [curr_number]}) Oh and by the way, remember we said you can't have unpicklable class attributes like sockets and file descriptors in the constructor and get_own_state ? The right way to initialize these things is to set the attribute to None in the constructor and assign an actual value in the execute method. But since that method will be called more than once, you should put a guard to see if it has already been initialized, like this: def __init__(): ... self.s3 = None def execute(): if self.s3 is None: self.s3 = s3.client(...) ... Now that we defined the input reader, we are going to define the executor. Similar to the input reader, we define a Python class. There will be multiple channels of this executor, each holding its own copy of this object. The executor exposes two important methods, execute and done , which might produce outputs for more downstream executors. execute is called whenever upstream input reader channels have produced some input batches for the channel to process. done is called when the channel knows it will no longer receive any more inputs and has already processed all the inputs it has. Our executor here adds up all the elements in an input stream and returns the sum. The execute method takes three arguments. Before we talk about what they are let's (re)visit this excellent cartoon. In Quokka we have executors and input readers, which we refer to as actors . Each actor can have multiple channels, e.g. we have bushes and trees as own input readers, each with two channels. The execute method on an executor is called when an upstream actor has batches for the executor to process. The first argument batches , is a list of Apache Arrow Tables from the upstream actor. The items in the batch could have come from one channel, several, or all of them of this actor! However it could not contain mixed batches from multiple actors. If we take the perspective of one of the quokka channels, this list could contain either acorns or leaves, but not both. The second argument stream_id is used to identify which source input reader/executor the batches came from. In this example we only have one input source so we can ignore this argument. Each upstream actor source is identified by an integer, which you can specify when hooking up the TaskGraph. The third argument channel denotes the channel id of the channel executing the object. Similar to the argument for the input reader. Here we also don't use this argument. This could be useful, e.g. when each channel writes output data to a shared file system and you want to ensure the written files have unique names. The code for our executor is pretty simple. We initialize a sum variable to None in the constructor. In the execute method, we add up the batches and store the result in the sum variable. In the done method, we print the sum and return it. Note that both execute and done methods can optionally return data to downstream actors. In this example it doesn't make sense for the execute to return anything as you don't know the sum until the last batch has been processed. However, the done method can return the sum. The return type for the execute and done methods must be Pyarrow Table of Polars Dataframe. class AddExecutor(Executor): def __init__(self) -> None: self.sum = None def execute(self,batches,stream_id, channel): for batch in batches: self.sum = polars.from_arrow(batch) if self.sum is None else self.sum + polars.from_arrow(batch) def done(self,channel): print(\"I am executor \", channel, \" my sum is \", self.sum) return self.sum Now that we have defined our input reader and stateful operator, we can hook them up together in a TaskGraph. Defining the TaskGraph requires a cluster object, which is LocalCluster here but can be an EC2Cluster for cloud deployments. Now let's hook up the TaskGraph. This programming model should remind you strongly of Tensorflow, if you had the good fortune of trying to use Tensorflow 1.0. The TaskGraph exposes new_input_reader_node , new_non_blocking_node and new_blocking_node APIs. The first one is used to define an input reader, the second one is used to define a \"normal\" executor, and the third one is used to define the last executor in the TaskGraph (i.e. the grey/gold quokka in the cartoon). Whereas the return values of non_blocking_node will be pushed to downstream executors, the return values of blocking_node will be collected in a Dataset object. In our example, the sum executor is the last executor, so we just use new_blocking_node , like this: cluster = LocalCluster() task_graph = TaskGraph(cluster) reader = SimpleDataset(80) numbers = task_graph.new_input_reader_node(reader) executor = AddExecutor() sum = task_graph.new_blocking_node({0:numbers},executor, placement_strategy = SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(sum.to_df()) Let's talk about the arguments to new_blocking_node one by one. The first argument to new_blocking_node is a dictionary. The values are the upstream actors you'd like the executor to receive data from. The keys are arbitrary integers, which you can use to identify the source actor in the execute method. Those integers will be passed as the stream_id argument to the execute method. In our example, we only have one upstream actor, so we just use 0 as the key. The second argument is the executor object. The third argument is the placement strategy of the executor, which determines how many channels the executor will have. In our example, we use SingleChannelStrategy , which means the executor will have one channel. If we use CustomChannelsStrategy(n) , the executor will have n channels on each TaskManager. What is a TaskManager? It can be interpretted as a thread pool. Each TaskManager holds channels from different actors and decide how to schedule their execution. There are separate TaskManagers for input readers and executors. The number of TaskManagers is determined by the io_per_node and exec_per_node keyword arugments to the TaskGraph() constructor. What is the source_target_info ? It's time for another schematic. The execute method for both input readers and executors could produce a PyArrow Table of Polars Dataframe to be pushed to downstream actors. How does Quokka generate separate messages for the different channels in a downstream actor? Please look at the schematic ^. First, filters are applied, specified by a SQLGlot predicate. In our example, we use sqlglot.exp.TRUE , which means no filter is applied. Then a partitioner is applied to split the filtered data into data for each channel. In our example, we use PassThroughPartitioner , which means a channel in the target actor gets all the data from a range of channels in the source actor, assuming the source actor has more channels. Other partitioners include BroadcastPartitioner and HashPartitioner. After the partitioner is applied, a series of functions ( batch_funcs ) are applied to each message destined for a downstream channel. In our example, we use [] , which means no functions are applied. You can supply any arbitrary list of Python functions with Polars Dataframe input and Polars Dataframe output, though you have to make sure that the columns you need in a later function must be in the executor output or generated by a previous function. Finally, a projection is applied to the data. In our example, we use None , which means no projection is applied. You can supply a list of column names. new_blocking_node returns sum , a Quokka Dataset object. It has a to_df method which returns a Polars Dataframe, once the TaskGraph has been run. To run the TaskGraph, we call task_graph.create() to initialize it, and then task_graph.run() . Lesson 1: Joins If you think the first lesson was too complicated, it proably was. This is because we had to define custom input readers and stateful operators. Hopefully in the process you learned a few things about how Quokka works. In most scenarios, it is my hope that you don't have to define custom objects, and use canned implementations which you can just import. This is similar to how Tensorflow or Pytorch works. If you know how to import torch.nn.Conv2d , you get the idea. Here, we are going to take two CSVs on disk, join them, and count the number of records in the result: select count(*) from a and b where a.key = b.key . You can use the a.csv and b.csv provided in the apps/tutorials folder, or you can supply your own and change the CSV input reader arguments appropriately. Without further ado, here's the code: import time from pyquokka.quokka_runtime import TaskGraph from pyquokka.executors import JoinExecutor, CountExecutor from pyquokka.target_info import TargetInfo, PassThroughPartitioner, HashPartitioner from pyquokka.placement_strategy import SingleChannelStrategy from pyquokka.dataset import InputDiskCSVDataset import sqlglot import pandas as pd from pyquokka.utils import LocalCluster, QuokkaClusterManager manager = QuokkaClusterManager() cluster = LocalCluster() task_graph = TaskGraph(cluster) a_reader = InputDiskCSVDataset(\"a.csv\", header = True, stride = 1024) b_reader = InputDiskCSVDataset(\"b.csv\", header = True , stride = 1024) a = task_graph.new_input_reader_node(a_reader) b = task_graph.new_input_reader_node(b_reader) join_executor = JoinExecutor(left_on=\"key_a\", right_on = \"key_b\") joined = task_graph.new_non_blocking_node({0:a,1:b},join_executor, source_target_info={0:TargetInfo(partitioner = HashPartitioner(\"key_a\"), predicate = sqlglot.exp.TRUE, projection = [\"key_a\"], batch_funcs = []), 1:TargetInfo(partitioner = HashPartitioner(\"key_b\"), predicate = sqlglot.exp.TRUE, projection = [\"key_b\"], batch_funcs = [])}) count_executor = CountExecutor() count = task_graph.new_blocking_node({0:joined},count_executor, placement_strategy= SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(count.to_df()) a = pd.read_csv(\"a.csv\",names=[\"key\",\"val1\",\"val2\"]) b = pd.read_csv(\"b.csv\",names=[\"key\",\"val1\",\"val2\"]) print(len(a.merge(b,on=\"key\",how=\"inner\"))) Note here we defined a new_nonblocking_node for the join operator and a new_blocking_node for the count operator. We see that new_nonblocking_node takes in pretty much the same arguments as new_blocking_node , with the difference being it returns an executor that can be used as sources for downstream actors. Quokka by default will execute the join in a pipelined parallel fashion with the count. As a result, the input reader, join and count actors are all executing concurrently in the system. The count operator will return the count as a single number which will be stored in a Dataset object. You can try to play around with this example to test out what the predicate , projection and batch_funcs do. For example, try to set a predicate which filters out rows with val1 > 0.5 : sqlglot.parse_one(\"val1 > 0.5\") . You can also try to set a projection on one of the TargetInfos which only returns the key_a column: [\"key_a\"] . Lesson 2: More inputs readers, more joins Lesson 2.1: JSON Joins We are again going to be dealing with joins. This time, though, we are going to join data from two JSONs on disk. As in lesson 1, we are then going to count the records in the result. You can use the a.json and b.json provided in the apps/tutorials folder, or, as in lesson 1, you can supply your own and change the JSON reader accordingly. Here's the code: import time from pyquokka.quokka_runtime import TaskGraph from pyquokka.executors import JoinExecutor, CountExecutor from pyquokka.target_info import TargetInfo, PassThroughPartitioner, HashPartitioner from pyquokka.placement_strategy import SingleChannelStrategy from pyquokka.dataset import InputDiskJSONDataset import sqlglot import pandas as pd import pyarrow as pa from pyquokka.utils import LocalCluster, QuokkaClusterManager manager = QuokkaClusterManager() cluster = LocalCluster() task_graph = TaskGraph(cluster) schema_a = pa.schema([ pa.field('key_a', pa.int64()), pa.field('val1_a', pa.float64()), pa.field('val2_a', pa.float64()) ]) schema_b = pa.schema([ pa.field('key_b', pa.int64()), pa.field('val1_b', pa.float64()), pa.field('val2_b', pa.float64()) ]) a_reader = InputDiskJSONDataset(\"a.json\", schema=schema_a) b_reader = InputDiskJSONDataset(\"b.json\", schema=schema_b) a = task_graph.new_input_reader_node(a_reader) b = task_graph.new_input_reader_node(b_reader) join_executor = JoinExecutor(left_on=\"key_a\", right_on = \"key_b\") joined = task_graph.new_non_blocking_node({0:a,1:b},join_executor, source_target_info={0:TargetInfo(partitioner = HashPartitioner(\"key_a\"), predicate = sqlglot.exp.TRUE, projection = [\"key_a\"], batch_funcs = []), 1:TargetInfo(partitioner = HashPartitioner(\"key_b\"), predicate = sqlglot.exp.TRUE, projection = [\"key_b\"], batch_funcs = [])}) count_executor = CountExecutor() count = task_graph.new_blocking_node({0:joined},count_executor, placement_strategy= SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(count.to_df()) a = json.read_json(\"a.json\") b = json.read_json(\"b.json\") a = a.to_pandas() b = b.to_pandas() print(len(a.merge(b,left_on=\"key_a\", right_on = \"key_b\", how=\"inner\"))) Notice how when creating a_reader and b_reader , we specify a pyarrow schema. This can be important if you want the values JSON file to be parsed as specific types. When using the InputJSONDiskReader with an explicit schema, make sure to specify a valid pyarrow schema, so the parser does not throw an error. We do not have to use an explicit schema, however. See the next lesson for this. Lesson 2.2: JSON and CSV Joins We will now be joining a JSON file and a CSV on disk. The code is the following: import time from pyquokka.quokka_runtime import TaskGraph from pyquokka.executors import JoinExecutor, CountExecutor from pyquokka.target_info import TargetInfo, PassThroughPartitioner, HashPartitioner from pyquokka.placement_strategy import SingleChannelStrategy from pyquokka.dataset import InputDiskCSVDataset, InputDiskJSONDataset import sqlglot import pandas as pd from pyquokka.utils import LocalCluster, QuokkaClusterManager manager = QuokkaClusterManager() cluster = LocalCluster() task_graph = TaskGraph(cluster) a_reader = InputDiskJSONDataset(\"a.json\") b_reader = InputDiskCSVDataset(\"b.csv\", header = True , stride = 1024) a = task_graph.new_input_reader_node(a_reader) b = task_graph.new_input_reader_node(b_reader) join_executor = JoinExecutor(left_on=\"key_a\", right_on = \"key_b\") joined = task_graph.new_non_blocking_node({0:a,1:b},join_executor, source_target_info={0:TargetInfo(partitioner = HashPartitioner(\"key_a\"), predicate = sqlglot.exp.TRUE, projection = [\"key_a\"], batch_funcs = []), 1:TargetInfo(partitioner = HashPartitioner(\"key_b\"), predicate = sqlglot.exp.TRUE, projection = [\"key_b\"], batch_funcs = [])}) count_executor = CountExecutor() count = task_graph.new_blocking_node({0:joined},count_executor, placement_strategy= SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(count.to_df()) a = json.read_json(\"a.json\") a = a.to_pandas() a[\"key_a\"] = a[\"key_a\"].astype(str) b = pd.read_csv(\"b.csv\",names=[\"key_b\",\"val1\",\"val2\"]) print(len(a.merge(b,left_on=\"key_a\", right_on = \"key_b\", how=\"inner\"))) Here, we are using our InputDiskJSONReader as the a_reader and our InputDiskCSVReader as the b_reader . Notice how we are not using an explicit pyarrow schema for the JSON input reader here. We simply let the input reader infer the data type of the values present in our JSON file. This is it for now. I unfortunately can't find too much time to write tutorials, but I hope this is enough to get you started. If you have any questions, feel free to reach out to me on the Quokka Discord server. You can always check out how the Quokka canned executors and input readers work, in pyquokka/executors.py and pyquokka/dataset.py . If you are feeling particularly audacious, you can try implementing a new input reader!","title":"TaskGraph API (Contribute to Quokka)"},{"location":"tutorial/#advanced-tutorials","text":"This section is for learning how to use Quokka's graph level API. This is expected for use cases where the DataStream API cannot satisfy your needs. Most users are not expected to program at this level. You should contact me: zihengw@stanford.edu or hit me up on Discord if you want to do this. You should probably stop reading now, unless you are a Stanford undergrad or masters student (or somebody else) who somehow decided to work with me on Quokka. You should read this tutorial if you want to add a new executor or input reader into Quokka. You should probably read up on the Cartoons section first, and then come back here. You probably should also first run the DataStream Tutorial to make sure Quokka actually works on your machine. The code for the tutorials can be found under apps/tutorials . They might perform meaningless tasks or perform tasks which you shoudn't necessarily use Quokka for, but they will showcase how Quokka works. I wrote Quokka. As a result I might take some things for granted that you might not. If you spot a typo or find some sections too difficult to understand, I would appreciate your feedback! Better yet, the docs are also open source under quokka/docs, so you can also make a PR.","title":"Advanced Tutorials"},{"location":"tutorial/#lesson-0-addition","text":"Let's walk through our first Quokka program. This first example defines an input reader which produces a stream of numbers, and a stateful operator which adds them up. Let's first look at the import section. In general when you are writing a Quokka program, you will need to import TaskGraph , LocalCluster and Executor . TaskGraph is the main object that you will use to define your program. LocalCluster is the execution context for the program, which you can replace with an EC2Cluster made from QuokkaClusterManager . Executor is an abstract class which you should extend to implement your own executors. Quokka also provides canned executors which you call import from pyquokka.executors such as join . from pyquokka.quokka_runtime import TaskGraph from pyquokka.utils import LocalCluster from pyquokka.executors import Executor from pyquokka.target_info import TargetInfo, PassThroughPartitioner from pyquokka.placement_strategy import SingleChannelStrategy import sqlglot import time import polars Quokka provides many optimized input readers for different input data formats. However, in this tutorial we are going to define a custom input reader class to showcase how the input reader works. The mindset here is that there will be many channels of this input reader spread across your cluster, and each channel will have its own copy of an object of this class. They will all be initialized in the same way, but each channel can produce its own data. An input reader object can be initialized with arbitrary arguments. The initialization will be performed locally before shipping the object over the network to the executors (in your code), so you need to make sure that the object can be pickled. This means that you can't initialize the object with a socket or a file descriptor. class SimpleDataset: # we define limit here, which is the max number we will generate def __init__(self, limit) -> None: self.limit = limit self.num_channels = None The key to implementing an input reader object lies in the get_own_state method implementation. This method will be called by the runtime once it decided how many channels this input reader will have. Armed with this information as well as the arguments passed to the object's constructor, the method should return a dictionary which maps channel ids to what the channel is assigned to produce . In this example, we will have N channels, and each channel will produce numbers k, k + N, k + 2N, all the way up to the limit. The get_own_state method will be called locally by the runtime after the object is initialized (not in your code though). If the method adds some state variables to the object, then they should be picklable too. Now what does \" what the channel is assigned to produce \" even mean? It is supposed to be a list of arbitrary picklable objects, though I'd recommend to keep it simple and stick to primitive types and at most tuples. What should these objects be? They are going to be fed as input arguments to the execute method, which will be executed by the channels across the cluster on their own copy of the input reader object. In this dead simple example, the list just contains the numbers a channel is supposed to produce, and the execute method simply wraps the number in a Polars DataFrame and spits it out. In more complicated examples, such as a CSV reader, the list typically contains the file names and offsets of the CSV files the channel is supposed to read. The execute method then reads the correct portion of the correct CSV file and returns the data. The mental model should be that the get_own_state method, executed once locally on the client, determines what each channel is supposed to produce, and the execute method, executed on each channel across the cluster, actually produces the data. On channel x , the execute method is called len(channel_info[x]) times, once for each of the values in the list, in order. The return value of the execute method is always None followed by the data item, which can be a Pyarrow table of Polars Dataframe . If you really want to know why the None is required, email me and I'll explain over coffee. def get_own_state(self, num_channels): channel_info = {} for channel in range(num_channels): channel_info[channel] = [i for i in range(channel, self.limit, num_channels)] return channel_info def execute(self, channel, state = None): curr_number = state return None, polars.DataFrame({\"number\": [curr_number]}) Oh and by the way, remember we said you can't have unpicklable class attributes like sockets and file descriptors in the constructor and get_own_state ? The right way to initialize these things is to set the attribute to None in the constructor and assign an actual value in the execute method. But since that method will be called more than once, you should put a guard to see if it has already been initialized, like this: def __init__(): ... self.s3 = None def execute(): if self.s3 is None: self.s3 = s3.client(...) ... Now that we defined the input reader, we are going to define the executor. Similar to the input reader, we define a Python class. There will be multiple channels of this executor, each holding its own copy of this object. The executor exposes two important methods, execute and done , which might produce outputs for more downstream executors. execute is called whenever upstream input reader channels have produced some input batches for the channel to process. done is called when the channel knows it will no longer receive any more inputs and has already processed all the inputs it has. Our executor here adds up all the elements in an input stream and returns the sum. The execute method takes three arguments. Before we talk about what they are let's (re)visit this excellent cartoon. In Quokka we have executors and input readers, which we refer to as actors . Each actor can have multiple channels, e.g. we have bushes and trees as own input readers, each with two channels. The execute method on an executor is called when an upstream actor has batches for the executor to process. The first argument batches , is a list of Apache Arrow Tables from the upstream actor. The items in the batch could have come from one channel, several, or all of them of this actor! However it could not contain mixed batches from multiple actors. If we take the perspective of one of the quokka channels, this list could contain either acorns or leaves, but not both. The second argument stream_id is used to identify which source input reader/executor the batches came from. In this example we only have one input source so we can ignore this argument. Each upstream actor source is identified by an integer, which you can specify when hooking up the TaskGraph. The third argument channel denotes the channel id of the channel executing the object. Similar to the argument for the input reader. Here we also don't use this argument. This could be useful, e.g. when each channel writes output data to a shared file system and you want to ensure the written files have unique names. The code for our executor is pretty simple. We initialize a sum variable to None in the constructor. In the execute method, we add up the batches and store the result in the sum variable. In the done method, we print the sum and return it. Note that both execute and done methods can optionally return data to downstream actors. In this example it doesn't make sense for the execute to return anything as you don't know the sum until the last batch has been processed. However, the done method can return the sum. The return type for the execute and done methods must be Pyarrow Table of Polars Dataframe. class AddExecutor(Executor): def __init__(self) -> None: self.sum = None def execute(self,batches,stream_id, channel): for batch in batches: self.sum = polars.from_arrow(batch) if self.sum is None else self.sum + polars.from_arrow(batch) def done(self,channel): print(\"I am executor \", channel, \" my sum is \", self.sum) return self.sum Now that we have defined our input reader and stateful operator, we can hook them up together in a TaskGraph. Defining the TaskGraph requires a cluster object, which is LocalCluster here but can be an EC2Cluster for cloud deployments. Now let's hook up the TaskGraph. This programming model should remind you strongly of Tensorflow, if you had the good fortune of trying to use Tensorflow 1.0. The TaskGraph exposes new_input_reader_node , new_non_blocking_node and new_blocking_node APIs. The first one is used to define an input reader, the second one is used to define a \"normal\" executor, and the third one is used to define the last executor in the TaskGraph (i.e. the grey/gold quokka in the cartoon). Whereas the return values of non_blocking_node will be pushed to downstream executors, the return values of blocking_node will be collected in a Dataset object. In our example, the sum executor is the last executor, so we just use new_blocking_node , like this: cluster = LocalCluster() task_graph = TaskGraph(cluster) reader = SimpleDataset(80) numbers = task_graph.new_input_reader_node(reader) executor = AddExecutor() sum = task_graph.new_blocking_node({0:numbers},executor, placement_strategy = SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(sum.to_df()) Let's talk about the arguments to new_blocking_node one by one. The first argument to new_blocking_node is a dictionary. The values are the upstream actors you'd like the executor to receive data from. The keys are arbitrary integers, which you can use to identify the source actor in the execute method. Those integers will be passed as the stream_id argument to the execute method. In our example, we only have one upstream actor, so we just use 0 as the key. The second argument is the executor object. The third argument is the placement strategy of the executor, which determines how many channels the executor will have. In our example, we use SingleChannelStrategy , which means the executor will have one channel. If we use CustomChannelsStrategy(n) , the executor will have n channels on each TaskManager. What is a TaskManager? It can be interpretted as a thread pool. Each TaskManager holds channels from different actors and decide how to schedule their execution. There are separate TaskManagers for input readers and executors. The number of TaskManagers is determined by the io_per_node and exec_per_node keyword arugments to the TaskGraph() constructor. What is the source_target_info ? It's time for another schematic. The execute method for both input readers and executors could produce a PyArrow Table of Polars Dataframe to be pushed to downstream actors. How does Quokka generate separate messages for the different channels in a downstream actor? Please look at the schematic ^. First, filters are applied, specified by a SQLGlot predicate. In our example, we use sqlglot.exp.TRUE , which means no filter is applied. Then a partitioner is applied to split the filtered data into data for each channel. In our example, we use PassThroughPartitioner , which means a channel in the target actor gets all the data from a range of channels in the source actor, assuming the source actor has more channels. Other partitioners include BroadcastPartitioner and HashPartitioner. After the partitioner is applied, a series of functions ( batch_funcs ) are applied to each message destined for a downstream channel. In our example, we use [] , which means no functions are applied. You can supply any arbitrary list of Python functions with Polars Dataframe input and Polars Dataframe output, though you have to make sure that the columns you need in a later function must be in the executor output or generated by a previous function. Finally, a projection is applied to the data. In our example, we use None , which means no projection is applied. You can supply a list of column names. new_blocking_node returns sum , a Quokka Dataset object. It has a to_df method which returns a Polars Dataframe, once the TaskGraph has been run. To run the TaskGraph, we call task_graph.create() to initialize it, and then task_graph.run() .","title":"Lesson 0: Addition"},{"location":"tutorial/#lesson-1-joins","text":"If you think the first lesson was too complicated, it proably was. This is because we had to define custom input readers and stateful operators. Hopefully in the process you learned a few things about how Quokka works. In most scenarios, it is my hope that you don't have to define custom objects, and use canned implementations which you can just import. This is similar to how Tensorflow or Pytorch works. If you know how to import torch.nn.Conv2d , you get the idea. Here, we are going to take two CSVs on disk, join them, and count the number of records in the result: select count(*) from a and b where a.key = b.key . You can use the a.csv and b.csv provided in the apps/tutorials folder, or you can supply your own and change the CSV input reader arguments appropriately. Without further ado, here's the code: import time from pyquokka.quokka_runtime import TaskGraph from pyquokka.executors import JoinExecutor, CountExecutor from pyquokka.target_info import TargetInfo, PassThroughPartitioner, HashPartitioner from pyquokka.placement_strategy import SingleChannelStrategy from pyquokka.dataset import InputDiskCSVDataset import sqlglot import pandas as pd from pyquokka.utils import LocalCluster, QuokkaClusterManager manager = QuokkaClusterManager() cluster = LocalCluster() task_graph = TaskGraph(cluster) a_reader = InputDiskCSVDataset(\"a.csv\", header = True, stride = 1024) b_reader = InputDiskCSVDataset(\"b.csv\", header = True , stride = 1024) a = task_graph.new_input_reader_node(a_reader) b = task_graph.new_input_reader_node(b_reader) join_executor = JoinExecutor(left_on=\"key_a\", right_on = \"key_b\") joined = task_graph.new_non_blocking_node({0:a,1:b},join_executor, source_target_info={0:TargetInfo(partitioner = HashPartitioner(\"key_a\"), predicate = sqlglot.exp.TRUE, projection = [\"key_a\"], batch_funcs = []), 1:TargetInfo(partitioner = HashPartitioner(\"key_b\"), predicate = sqlglot.exp.TRUE, projection = [\"key_b\"], batch_funcs = [])}) count_executor = CountExecutor() count = task_graph.new_blocking_node({0:joined},count_executor, placement_strategy= SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(count.to_df()) a = pd.read_csv(\"a.csv\",names=[\"key\",\"val1\",\"val2\"]) b = pd.read_csv(\"b.csv\",names=[\"key\",\"val1\",\"val2\"]) print(len(a.merge(b,on=\"key\",how=\"inner\"))) Note here we defined a new_nonblocking_node for the join operator and a new_blocking_node for the count operator. We see that new_nonblocking_node takes in pretty much the same arguments as new_blocking_node , with the difference being it returns an executor that can be used as sources for downstream actors. Quokka by default will execute the join in a pipelined parallel fashion with the count. As a result, the input reader, join and count actors are all executing concurrently in the system. The count operator will return the count as a single number which will be stored in a Dataset object. You can try to play around with this example to test out what the predicate , projection and batch_funcs do. For example, try to set a predicate which filters out rows with val1 > 0.5 : sqlglot.parse_one(\"val1 > 0.5\") . You can also try to set a projection on one of the TargetInfos which only returns the key_a column: [\"key_a\"] .","title":"Lesson 1: Joins"},{"location":"tutorial/#lesson-2-more-inputs-readers-more-joins","text":"","title":"Lesson 2: More inputs readers, more joins"},{"location":"tutorial/#lesson-21-json-joins","text":"We are again going to be dealing with joins. This time, though, we are going to join data from two JSONs on disk. As in lesson 1, we are then going to count the records in the result. You can use the a.json and b.json provided in the apps/tutorials folder, or, as in lesson 1, you can supply your own and change the JSON reader accordingly. Here's the code: import time from pyquokka.quokka_runtime import TaskGraph from pyquokka.executors import JoinExecutor, CountExecutor from pyquokka.target_info import TargetInfo, PassThroughPartitioner, HashPartitioner from pyquokka.placement_strategy import SingleChannelStrategy from pyquokka.dataset import InputDiskJSONDataset import sqlglot import pandas as pd import pyarrow as pa from pyquokka.utils import LocalCluster, QuokkaClusterManager manager = QuokkaClusterManager() cluster = LocalCluster() task_graph = TaskGraph(cluster) schema_a = pa.schema([ pa.field('key_a', pa.int64()), pa.field('val1_a', pa.float64()), pa.field('val2_a', pa.float64()) ]) schema_b = pa.schema([ pa.field('key_b', pa.int64()), pa.field('val1_b', pa.float64()), pa.field('val2_b', pa.float64()) ]) a_reader = InputDiskJSONDataset(\"a.json\", schema=schema_a) b_reader = InputDiskJSONDataset(\"b.json\", schema=schema_b) a = task_graph.new_input_reader_node(a_reader) b = task_graph.new_input_reader_node(b_reader) join_executor = JoinExecutor(left_on=\"key_a\", right_on = \"key_b\") joined = task_graph.new_non_blocking_node({0:a,1:b},join_executor, source_target_info={0:TargetInfo(partitioner = HashPartitioner(\"key_a\"), predicate = sqlglot.exp.TRUE, projection = [\"key_a\"], batch_funcs = []), 1:TargetInfo(partitioner = HashPartitioner(\"key_b\"), predicate = sqlglot.exp.TRUE, projection = [\"key_b\"], batch_funcs = [])}) count_executor = CountExecutor() count = task_graph.new_blocking_node({0:joined},count_executor, placement_strategy= SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(count.to_df()) a = json.read_json(\"a.json\") b = json.read_json(\"b.json\") a = a.to_pandas() b = b.to_pandas() print(len(a.merge(b,left_on=\"key_a\", right_on = \"key_b\", how=\"inner\"))) Notice how when creating a_reader and b_reader , we specify a pyarrow schema. This can be important if you want the values JSON file to be parsed as specific types. When using the InputJSONDiskReader with an explicit schema, make sure to specify a valid pyarrow schema, so the parser does not throw an error. We do not have to use an explicit schema, however. See the next lesson for this.","title":"Lesson 2.1: JSON Joins"},{"location":"tutorial/#lesson-22-json-and-csv-joins","text":"We will now be joining a JSON file and a CSV on disk. The code is the following: import time from pyquokka.quokka_runtime import TaskGraph from pyquokka.executors import JoinExecutor, CountExecutor from pyquokka.target_info import TargetInfo, PassThroughPartitioner, HashPartitioner from pyquokka.placement_strategy import SingleChannelStrategy from pyquokka.dataset import InputDiskCSVDataset, InputDiskJSONDataset import sqlglot import pandas as pd from pyquokka.utils import LocalCluster, QuokkaClusterManager manager = QuokkaClusterManager() cluster = LocalCluster() task_graph = TaskGraph(cluster) a_reader = InputDiskJSONDataset(\"a.json\") b_reader = InputDiskCSVDataset(\"b.csv\", header = True , stride = 1024) a = task_graph.new_input_reader_node(a_reader) b = task_graph.new_input_reader_node(b_reader) join_executor = JoinExecutor(left_on=\"key_a\", right_on = \"key_b\") joined = task_graph.new_non_blocking_node({0:a,1:b},join_executor, source_target_info={0:TargetInfo(partitioner = HashPartitioner(\"key_a\"), predicate = sqlglot.exp.TRUE, projection = [\"key_a\"], batch_funcs = []), 1:TargetInfo(partitioner = HashPartitioner(\"key_b\"), predicate = sqlglot.exp.TRUE, projection = [\"key_b\"], batch_funcs = [])}) count_executor = CountExecutor() count = task_graph.new_blocking_node({0:joined},count_executor, placement_strategy= SingleChannelStrategy(), source_target_info={0:TargetInfo(partitioner = PassThroughPartitioner(), predicate = sqlglot.exp.TRUE, projection = None, batch_funcs = [])}) task_graph.create() start = time.time() task_graph.run() print(\"total time \", time.time() - start) print(count.to_df()) a = json.read_json(\"a.json\") a = a.to_pandas() a[\"key_a\"] = a[\"key_a\"].astype(str) b = pd.read_csv(\"b.csv\",names=[\"key_b\",\"val1\",\"val2\"]) print(len(a.merge(b,left_on=\"key_a\", right_on = \"key_b\", how=\"inner\"))) Here, we are using our InputDiskJSONReader as the a_reader and our InputDiskCSVReader as the b_reader . Notice how we are not using an explicit pyarrow schema for the JSON input reader here. We simply let the input reader infer the data type of the values present in our JSON file. This is it for now. I unfortunately can't find too much time to write tutorials, but I hope this is enough to get you started. If you have any questions, feel free to reach out to me on the Quokka Discord server. You can always check out how the Quokka canned executors and input readers work, in pyquokka/executors.py and pyquokka/dataset.py . If you are feeling particularly audacious, you can try implementing a new input reader!","title":"Lesson 2.2: JSON and CSV Joins"},{"location":"dataset/to_arrow_refs/","text":"DataSet.to_arrow_refs This will return a list of Ray ObjectRefs to Arrow Tables. This is a blocking call. It will NOT move data to your local machine. Return List of Ray ObjectRefs to Arrow Tables Source code in pyquokka/quokka_dataset.py 40 41 42 43 44 45 46 47 48 def to_arrow_refs ( self ): \"\"\" This will return a list of Ray ObjectRefs to Arrow Tables. This is a blocking call. It will NOT move data to your local machine. Return: List of Ray ObjectRefs to Arrow Tables \"\"\" return ray . get ( self . wrapped_dataset . to_arrow_refs . remote ( self . dataset_id ))","title":"DataSet.to_arrow_refs"},{"location":"dataset/to_arrow_refs/#datasetto_arrow_refs","text":"This will return a list of Ray ObjectRefs to Arrow Tables. This is a blocking call. It will NOT move data to your local machine. Return List of Ray ObjectRefs to Arrow Tables Source code in pyquokka/quokka_dataset.py 40 41 42 43 44 45 46 47 48 def to_arrow_refs ( self ): \"\"\" This will return a list of Ray ObjectRefs to Arrow Tables. This is a blocking call. It will NOT move data to your local machine. Return: List of Ray ObjectRefs to Arrow Tables \"\"\" return ray . get ( self . wrapped_dataset . to_arrow_refs . remote ( self . dataset_id ))","title":"DataSet.to_arrow_refs"},{"location":"dataset/to_df/","text":"DataSet.to_df This is a blocking call. It will collect all the data from the cluster and return a Polars DataFrame to the calling Python session (could be your local machine, be careful of OOM!). Return Polars DataFrame Source code in pyquokka/quokka_dataset.py 26 27 28 29 30 31 32 33 34 35 def to_df ( self ): \"\"\" This is a blocking call. It will collect all the data from the cluster and return a Polars DataFrame to the calling Python session (could be your local machine, be careful of OOM!). Return: Polars DataFrame \"\"\" return ray . get ( self . wrapped_dataset . to_df . remote ( self . dataset_id ))","title":"DataSet.to_df"},{"location":"dataset/to_df/#datasetto_df","text":"This is a blocking call. It will collect all the data from the cluster and return a Polars DataFrame to the calling Python session (could be your local machine, be careful of OOM!). Return Polars DataFrame Source code in pyquokka/quokka_dataset.py 26 27 28 29 30 31 32 33 34 35 def to_df ( self ): \"\"\" This is a blocking call. It will collect all the data from the cluster and return a Polars DataFrame to the calling Python session (could be your local machine, be careful of OOM!). Return: Polars DataFrame \"\"\" return ray . get ( self . wrapped_dataset . to_df . remote ( self . dataset_id ))","title":"DataSet.to_df"},{"location":"dataset/to_ray_dataset/","text":"DataSet.to_ray_dataset This will convert the Quokka Dataset to a Ray Dataset. This is a blocking call. It will NOT move data to your local machine. Return Ray Dataset Source code in pyquokka/quokka_dataset.py 50 51 52 53 54 55 56 57 58 def to_ray_dataset ( self ): \"\"\" This will convert the Quokka Dataset to a Ray Dataset. This is a blocking call. It will NOT move data to your local machine. Return: Ray Dataset \"\"\" return ray . data . from_arrow_refs ( self . to_arrow_refs ())","title":"DataSet.to_ray_dataset"},{"location":"dataset/to_ray_dataset/#datasetto_ray_dataset","text":"This will convert the Quokka Dataset to a Ray Dataset. This is a blocking call. It will NOT move data to your local machine. Return Ray Dataset Source code in pyquokka/quokka_dataset.py 50 51 52 53 54 55 56 57 58 def to_ray_dataset ( self ): \"\"\" This will convert the Quokka Dataset to a Ray Dataset. This is a blocking call. It will NOT move data to your local machine. Return: Ray Dataset \"\"\" return ray . data . from_arrow_refs ( self . to_arrow_refs ())","title":"DataSet.to_ray_dataset"},{"location":"datastream/agg/","text":"DataStream.agg Aggregate this DataStream according to the defined aggregations without any pre-grouping. This is similar to Pandas df.agg() . The result will be one row. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call .compute() or .collect() on this DataStream instead of doing additional operations on it like .filter() since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do .explain() on it. Parameters: Name Type Description Default aggregations dict similar to a dictionary argument to Pandas df.agg() . The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) >>> d = lineitem . filter ( \"l_shipdate <= date '1998-12-01' - interval '90' day\" ) >>> d = d . with_column ( \"disc_price\" , lambda x : x [ \"l_extendedprice\" ] * ( 1 - x [ \"l_discount\" ]), required_columns = { \"l_extendedprice\" , \"l_discount\" }) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well. >>> f = d . agg ({ \"l_quantity\" :[ \"sum\" , \"avg\" ], \"l_extendedprice\" :[ \"sum\" , \"avg\" ], \"disc_price\" : \"sum\" , \"l_discount\" : \"min\" , \"*\" : \"count\" }) Source code in pyquokka/datastream.py 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 def agg ( self , aggregations ): \"\"\" Aggregate this DataStream according to the defined aggregations without any pre-grouping. This is similar to Pandas `df.agg()`. The result will be one row. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call `.compute()` or `.collect()` on this DataStream instead of doing additional operations on it like `.filter()` since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do `.explain()` on it. Args: aggregations (dict): similar to a dictionary argument to Pandas `df.agg()`. The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") >>> d = lineitem.filter(\"l_shipdate <= date '1998-12-01' - interval '90' day\") >>> d = d.with_column(\"disc_price\", lambda x:x[\"l_extendedprice\"] * (1 - x[\"l_discount\"]), required_columns ={\"l_extendedprice\", \"l_discount\"}) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well. >>> f = d.agg({\"l_quantity\":[\"sum\",\"avg\"], \"l_extendedprice\":[\"sum\",\"avg\"], \"disc_price\":\"sum\", \"l_discount\":\"min\",\"*\":\"count\"}) \"\"\" return self . _grouped_aggregate ([], aggregations , None )","title":"DataStream.aggregate"},{"location":"datastream/agg/#datastreamagg","text":"Aggregate this DataStream according to the defined aggregations without any pre-grouping. This is similar to Pandas df.agg() . The result will be one row. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call .compute() or .collect() on this DataStream instead of doing additional operations on it like .filter() since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do .explain() on it. Parameters: Name Type Description Default aggregations dict similar to a dictionary argument to Pandas df.agg() . The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) >>> d = lineitem . filter ( \"l_shipdate <= date '1998-12-01' - interval '90' day\" ) >>> d = d . with_column ( \"disc_price\" , lambda x : x [ \"l_extendedprice\" ] * ( 1 - x [ \"l_discount\" ]), required_columns = { \"l_extendedprice\" , \"l_discount\" }) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well. >>> f = d . agg ({ \"l_quantity\" :[ \"sum\" , \"avg\" ], \"l_extendedprice\" :[ \"sum\" , \"avg\" ], \"disc_price\" : \"sum\" , \"l_discount\" : \"min\" , \"*\" : \"count\" }) Source code in pyquokka/datastream.py 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 def agg ( self , aggregations ): \"\"\" Aggregate this DataStream according to the defined aggregations without any pre-grouping. This is similar to Pandas `df.agg()`. The result will be one row. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call `.compute()` or `.collect()` on this DataStream instead of doing additional operations on it like `.filter()` since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do `.explain()` on it. Args: aggregations (dict): similar to a dictionary argument to Pandas `df.agg()`. The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") >>> d = lineitem.filter(\"l_shipdate <= date '1998-12-01' - interval '90' day\") >>> d = d.with_column(\"disc_price\", lambda x:x[\"l_extendedprice\"] * (1 - x[\"l_discount\"]), required_columns ={\"l_extendedprice\", \"l_discount\"}) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well. >>> f = d.agg({\"l_quantity\":[\"sum\",\"avg\"], \"l_extendedprice\":[\"sum\",\"avg\"], \"disc_price\":\"sum\", \"l_discount\":\"min\",\"*\":\"count\"}) \"\"\" return self . _grouped_aggregate ([], aggregations , None )","title":"DataStream.agg"},{"location":"datastream/agg_sql/","text":"DataStream.agg_sql This is the SQL version of agg . It takes a SQL statement as input instead of a dictionary. The SQL statement must be a valid SQL statement. The requirements are similar to what you need for transform_sql . Please look at the examples. Exotic SQL statements may not work, such as count_distinct , percentile etc. Please limit your aggregations to mean/max/min/sum/avg/count for now. Parameters: Name Type Description Default aggregations str a valid SQL statement. The requirements are similar to what you need for transform_sql . required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d . agg_sql ( \"sum(l_extendedprice * (1 - l_discount)) as revenue\" ) >>> f = d . agg_sql ( \"count(*) as count_order\" ) >>> f = d . agg_sql ( \" >>> sum ( case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end ) as high_line_count , >>> sum ( case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end ) as low_line_count >>> \") Source code in pyquokka/datastream.py 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 def agg_sql ( self , aggregations : str ): \"\"\" This is the SQL version of `agg`. It takes a SQL statement as input instead of a dictionary. The SQL statement must be a valid SQL statement. The requirements are similar to what you need for `transform_sql`. Please look at the examples. Exotic SQL statements may not work, such as `count_distinct`, `percentile` etc. Please limit your aggregations to mean/max/min/sum/avg/count for now. Args: aggregations (str): a valid SQL statement. The requirements are similar to what you need for `transform_sql`. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d.agg_sql(\"sum(l_extendedprice * (1 - l_discount)) as revenue\") >>> f = d.agg_sql(\"count(*) as count_order\") >>> f = d.agg_sql(\" >>> sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, >>> sum(case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end) as low_line_count >>> \") \"\"\" return self . _grouped_aggregate_sql ([], aggregations , None )","title":"DataStream.agg_sql"},{"location":"datastream/agg_sql/#datastreamagg_sql","text":"This is the SQL version of agg . It takes a SQL statement as input instead of a dictionary. The SQL statement must be a valid SQL statement. The requirements are similar to what you need for transform_sql . Please look at the examples. Exotic SQL statements may not work, such as count_distinct , percentile etc. Please limit your aggregations to mean/max/min/sum/avg/count for now. Parameters: Name Type Description Default aggregations str a valid SQL statement. The requirements are similar to what you need for transform_sql . required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d . agg_sql ( \"sum(l_extendedprice * (1 - l_discount)) as revenue\" ) >>> f = d . agg_sql ( \"count(*) as count_order\" ) >>> f = d . agg_sql ( \" >>> sum ( case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end ) as high_line_count , >>> sum ( case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end ) as low_line_count >>> \") Source code in pyquokka/datastream.py 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 def agg_sql ( self , aggregations : str ): \"\"\" This is the SQL version of `agg`. It takes a SQL statement as input instead of a dictionary. The SQL statement must be a valid SQL statement. The requirements are similar to what you need for `transform_sql`. Please look at the examples. Exotic SQL statements may not work, such as `count_distinct`, `percentile` etc. Please limit your aggregations to mean/max/min/sum/avg/count for now. Args: aggregations (str): a valid SQL statement. The requirements are similar to what you need for `transform_sql`. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d.agg_sql(\"sum(l_extendedprice * (1 - l_discount)) as revenue\") >>> f = d.agg_sql(\"count(*) as count_order\") >>> f = d.agg_sql(\" >>> sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, >>> sum(case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end) as low_line_count >>> \") \"\"\" return self . _grouped_aggregate_sql ([], aggregations , None )","title":"DataStream.agg_sql"},{"location":"datastream/collect/","text":"DataStream.collect This will trigger the execution of computational graph, similar to Spark collect(). The result will be a Polars DataFrame returned to the client. Like Spark, this will be slow or cause OOM if the result is very large! If you want to compute a temporary result that will be used in a future computation, try to use the compute() method instead. Return Polars DataFrame. Examples: Result will be a Polars DataFrame, as if you did polars.read_csv(\"my_csv.csv\") >>> f = qc . read_csv ( \"my_csv.csv\" ) >>> result = f . collect () Source code in pyquokka/datastream.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def collect ( self ): \"\"\" This will trigger the execution of computational graph, similar to Spark collect(). The result will be a Polars DataFrame returned to the client. Like Spark, this will be slow or cause OOM if the result is very large! If you want to compute a temporary result that will be used in a future computation, try to use the `compute()` method instead. Return: Polars DataFrame. Examples: Result will be a Polars DataFrame, as if you did polars.read_csv(\"my_csv.csv\") >>> f = qc.read_csv(\"my_csv.csv\") >>> result = f.collect() \"\"\" if self . materialized : return self . _get_materialized_df () dataset = self . quokka_context . new_dataset ( self , self . schema ) result = self . quokka_context . execute_node ( dataset . source_node_id ) return result","title":"DataStream.collect"},{"location":"datastream/collect/#datastreamcollect","text":"This will trigger the execution of computational graph, similar to Spark collect(). The result will be a Polars DataFrame returned to the client. Like Spark, this will be slow or cause OOM if the result is very large! If you want to compute a temporary result that will be used in a future computation, try to use the compute() method instead. Return Polars DataFrame. Examples: Result will be a Polars DataFrame, as if you did polars.read_csv(\"my_csv.csv\") >>> f = qc . read_csv ( \"my_csv.csv\" ) >>> result = f . collect () Source code in pyquokka/datastream.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def collect ( self ): \"\"\" This will trigger the execution of computational graph, similar to Spark collect(). The result will be a Polars DataFrame returned to the client. Like Spark, this will be slow or cause OOM if the result is very large! If you want to compute a temporary result that will be used in a future computation, try to use the `compute()` method instead. Return: Polars DataFrame. Examples: Result will be a Polars DataFrame, as if you did polars.read_csv(\"my_csv.csv\") >>> f = qc.read_csv(\"my_csv.csv\") >>> result = f.collect() \"\"\" if self . materialized : return self . _get_materialized_df () dataset = self . quokka_context . new_dataset ( self , self . schema ) result = self . quokka_context . execute_node ( dataset . source_node_id ) return result","title":"DataStream.collect"},{"location":"datastream/compute/","text":"DataStream.compute This will trigger the execution of computational graph, but store the result cached across the cluster. The result will be a Quokka DataSet. You can read a DataSet x back into a DataStream via qc.read_dataset(x) . This is similar to Spark's persist() method. Return Quokka DataSet. This can be thought of as a list of objects cached in memory/disk across the cluster. Examples: >>> f = qc . read_csv ( \"my_csv.csv\" ) >>> result = f . collect () >>> d = qc . read_dataset ( result ) Source code in pyquokka/datastream.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def compute ( self ): \"\"\" This will trigger the execution of computational graph, but store the result cached across the cluster. The result will be a Quokka DataSet. You can read a DataSet `x` back into a DataStream via `qc.read_dataset(x)`. This is similar to Spark's `persist()` method. Return: Quokka DataSet. This can be thought of as a list of objects cached in memory/disk across the cluster. Examples: >>> f = qc.read_csv(\"my_csv.csv\") >>> result = f.collect() >>> d = qc.read_dataset(result) \"\"\" dataset = self . quokka_context . new_dataset ( self , self . schema ) return self . quokka_context . execute_node ( dataset . source_node_id , collect = False )","title":"DataStream.compute"},{"location":"datastream/compute/#datastreamcompute","text":"This will trigger the execution of computational graph, but store the result cached across the cluster. The result will be a Quokka DataSet. You can read a DataSet x back into a DataStream via qc.read_dataset(x) . This is similar to Spark's persist() method. Return Quokka DataSet. This can be thought of as a list of objects cached in memory/disk across the cluster. Examples: >>> f = qc . read_csv ( \"my_csv.csv\" ) >>> result = f . collect () >>> d = qc . read_dataset ( result ) Source code in pyquokka/datastream.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def compute ( self ): \"\"\" This will trigger the execution of computational graph, but store the result cached across the cluster. The result will be a Quokka DataSet. You can read a DataSet `x` back into a DataStream via `qc.read_dataset(x)`. This is similar to Spark's `persist()` method. Return: Quokka DataSet. This can be thought of as a list of objects cached in memory/disk across the cluster. Examples: >>> f = qc.read_csv(\"my_csv.csv\") >>> result = f.collect() >>> d = qc.read_dataset(result) \"\"\" dataset = self . quokka_context . new_dataset ( self , self . schema ) return self . quokka_context . execute_node ( dataset . source_node_id , collect = False )","title":"DataStream.compute"},{"location":"datastream/count/","text":"DataStream.count Return total row count. Parameters: Name Type Description Default collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 def count ( self , collect = True ): \"\"\" Return total row count. Args: collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" if collect : return self . agg ({ \"*\" : \"count\" }) . collect () else : return self . agg ({ \"*\" : \"count\" })","title":"DataStream.count"},{"location":"datastream/count/#datastreamcount","text":"Return total row count. Parameters: Name Type Description Default collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 def count ( self , collect = True ): \"\"\" Return total row count. Args: collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" if collect : return self . agg ({ \"*\" : \"count\" }) . collect () else : return self . agg ({ \"*\" : \"count\" })","title":"DataStream.count"},{"location":"datastream/count_distinct/","text":"DataStream.count_distinct Count the number of distinct values of a column. This may result in out of memory. This is not approximate. Parameters: Name Type Description Default col str the column to count distinct values of required Source code in pyquokka/datastream.py 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 def count_distinct ( self , col ): \"\"\" Count the number of distinct values of a column. This may result in out of memory. This is not approximate. Args: col (str): the column to count distinct values of \"\"\" return self . _grouped_count_distinct ([], col )","title":"DataStream.count_distinct"},{"location":"datastream/count_distinct/#datastreamcount_distinct","text":"Count the number of distinct values of a column. This may result in out of memory. This is not approximate. Parameters: Name Type Description Default col str the column to count distinct values of required Source code in pyquokka/datastream.py 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 def count_distinct ( self , col ): \"\"\" Count the number of distinct values of a column. This may result in out of memory. This is not approximate. Args: col (str): the column to count distinct values of \"\"\" return self . _grouped_count_distinct ([], col )","title":"DataStream.count_distinct"},{"location":"datastream/distinct/","text":"DataStream.distinct Return a new DataStream with specified columns and unique rows. This is like SELECT DISTINCT(KEYS) FROM ... in SQL. Note all the other columns will be dropped, since their behavior is unspecified. If you want to do deduplication, you can use this operator with keys set to all the columns. This could be accomplished by using groupby().agg() but using distinct is generally faster because it is nonblocking, compared to a groupby. Quokka really likes nonblocking operations because it can then pipeline it with other operators. Parameters: Name Type Description Default keys list a list of columns to select distinct on. required Return A transformed DataStream whose columns are in keys and whose rows are unique. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Select only the l_orderdate and l_orderkey columns, return only unique rows. >>> f = f . distinct ([ \"l_orderdate\" , \"l_orderkey\" ]) This will now fail, since l_comment is no longer in f's schema. >>> f = f . select ([ \"l_comment\" ]) Source code in pyquokka/datastream.py 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 def distinct ( self , keys : list ): \"\"\" Return a new DataStream with specified columns and unique rows. This is like `SELECT DISTINCT(KEYS) FROM ...` in SQL. Note all the other columns will be dropped, since their behavior is unspecified. If you want to do deduplication, you can use this operator with keys set to all the columns. This could be accomplished by using `groupby().agg()` but using `distinct` is generally faster because it is nonblocking, compared to a groupby. Quokka really likes nonblocking operations because it can then pipeline it with other operators. Args: keys (list): a list of columns to select distinct on. Return: A transformed DataStream whose columns are in keys and whose rows are unique. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Select only the l_orderdate and l_orderkey columns, return only unique rows. >>> f = f.distinct([\"l_orderdate\", \"l_orderkey\"]) This will now fail, since l_comment is no longer in f's schema. >>> f = f.select([\"l_comment\"]) \"\"\" if type ( keys ) == str : keys = [ keys ] assert type ( keys ) == list , \"keys must be a list of column names\" assert all ([ key in self . schema for key in keys ]), \"keys must be a subset of the columns in the DataStream\" select_stream = self . select ( keys ) return self . quokka_context . new_stream ( sources = { 0 : select_stream }, partitioners = { 0 : HashPartitioner ( keys [ 0 ])}, node = StatefulNode ( schema = keys , # this is a stateful node, but predicates and projections can be pushed down. schema_mapping = { col : { 0 : col } for col in keys }, required_columns = { 0 : set ( keys )}, operator = DistinctExecutor ( keys ) ), schema = keys , )","title":"DataStream.distinct"},{"location":"datastream/distinct/#datastreamdistinct","text":"Return a new DataStream with specified columns and unique rows. This is like SELECT DISTINCT(KEYS) FROM ... in SQL. Note all the other columns will be dropped, since their behavior is unspecified. If you want to do deduplication, you can use this operator with keys set to all the columns. This could be accomplished by using groupby().agg() but using distinct is generally faster because it is nonblocking, compared to a groupby. Quokka really likes nonblocking operations because it can then pipeline it with other operators. Parameters: Name Type Description Default keys list a list of columns to select distinct on. required Return A transformed DataStream whose columns are in keys and whose rows are unique. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Select only the l_orderdate and l_orderkey columns, return only unique rows. >>> f = f . distinct ([ \"l_orderdate\" , \"l_orderkey\" ]) This will now fail, since l_comment is no longer in f's schema. >>> f = f . select ([ \"l_comment\" ]) Source code in pyquokka/datastream.py 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 def distinct ( self , keys : list ): \"\"\" Return a new DataStream with specified columns and unique rows. This is like `SELECT DISTINCT(KEYS) FROM ...` in SQL. Note all the other columns will be dropped, since their behavior is unspecified. If you want to do deduplication, you can use this operator with keys set to all the columns. This could be accomplished by using `groupby().agg()` but using `distinct` is generally faster because it is nonblocking, compared to a groupby. Quokka really likes nonblocking operations because it can then pipeline it with other operators. Args: keys (list): a list of columns to select distinct on. Return: A transformed DataStream whose columns are in keys and whose rows are unique. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Select only the l_orderdate and l_orderkey columns, return only unique rows. >>> f = f.distinct([\"l_orderdate\", \"l_orderkey\"]) This will now fail, since l_comment is no longer in f's schema. >>> f = f.select([\"l_comment\"]) \"\"\" if type ( keys ) == str : keys = [ keys ] assert type ( keys ) == list , \"keys must be a list of column names\" assert all ([ key in self . schema for key in keys ]), \"keys must be a subset of the columns in the DataStream\" select_stream = self . select ( keys ) return self . quokka_context . new_stream ( sources = { 0 : select_stream }, partitioners = { 0 : HashPartitioner ( keys [ 0 ])}, node = StatefulNode ( schema = keys , # this is a stateful node, but predicates and projections can be pushed down. schema_mapping = { col : { 0 : col } for col in keys }, required_columns = { 0 : set ( keys )}, operator = DistinctExecutor ( keys ) ), schema = keys , )","title":"DataStream.distinct"},{"location":"datastream/drop/","text":"DataStream.drop Think of this as the anti-opereator to select. Instead of selecting columns, this will drop columns. This is implemented in Quokka as selecting the columns in the DataStream's schema that are not dropped. Parameters: Name Type Description Default cols_to_drop list a list of columns to drop from the source DataStream required Return A DataStream consisting of all columns in the source DataStream that are not in cols_to_drop . Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Drop the l_orderdate and l_orderkey columns >>> f = f . drop ([ \"l_orderdate\" , \"l_orderkey\" ]) This will now fail, since you dropped l_orderdate >>> f = f . select ([ \"l_orderdate\" ]) Source code in pyquokka/datastream.py 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def drop ( self , cols_to_drop : list ): \"\"\" Think of this as the anti-opereator to select. Instead of selecting columns, this will drop columns. This is implemented in Quokka as selecting the columns in the DataStream's schema that are not dropped. Args: cols_to_drop (list): a list of columns to drop from the source DataStream Return: A DataStream consisting of all columns in the source DataStream that are not in `cols_to_drop`. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Drop the l_orderdate and l_orderkey columns >>> f = f.drop([\"l_orderdate\", \"l_orderkey\"]) This will now fail, since you dropped l_orderdate >>> f = f.select([\"l_orderdate\"]) \"\"\" assert type ( cols_to_drop ) == list actual_cols_to_drop = [] for col in cols_to_drop : if col in self . schema : actual_cols_to_drop . append ( col ) if self . sorted is not None : assert col not in self . sorted , \"cannot drop a sort key!\" if len ( actual_cols_to_drop ) == 0 : return self else : if self . materialized : df = self . _get_materialized_df () . drop ( actual_cols_to_drop ) return self . quokka_context . from_polars ( df ) else : return self . select ([ col for col in self . schema if col not in cols_to_drop ])","title":"DataStream.drop"},{"location":"datastream/drop/#datastreamdrop","text":"Think of this as the anti-opereator to select. Instead of selecting columns, this will drop columns. This is implemented in Quokka as selecting the columns in the DataStream's schema that are not dropped. Parameters: Name Type Description Default cols_to_drop list a list of columns to drop from the source DataStream required Return A DataStream consisting of all columns in the source DataStream that are not in cols_to_drop . Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Drop the l_orderdate and l_orderkey columns >>> f = f . drop ([ \"l_orderdate\" , \"l_orderkey\" ]) This will now fail, since you dropped l_orderdate >>> f = f . select ([ \"l_orderdate\" ]) Source code in pyquokka/datastream.py 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def drop ( self , cols_to_drop : list ): \"\"\" Think of this as the anti-opereator to select. Instead of selecting columns, this will drop columns. This is implemented in Quokka as selecting the columns in the DataStream's schema that are not dropped. Args: cols_to_drop (list): a list of columns to drop from the source DataStream Return: A DataStream consisting of all columns in the source DataStream that are not in `cols_to_drop`. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Drop the l_orderdate and l_orderkey columns >>> f = f.drop([\"l_orderdate\", \"l_orderkey\"]) This will now fail, since you dropped l_orderdate >>> f = f.select([\"l_orderdate\"]) \"\"\" assert type ( cols_to_drop ) == list actual_cols_to_drop = [] for col in cols_to_drop : if col in self . schema : actual_cols_to_drop . append ( col ) if self . sorted is not None : assert col not in self . sorted , \"cannot drop a sort key!\" if len ( actual_cols_to_drop ) == 0 : return self else : if self . materialized : df = self . _get_materialized_df () . drop ( actual_cols_to_drop ) return self . quokka_context . from_polars ( df ) else : return self . select ([ col for col in self . schema if col not in cols_to_drop ])","title":"DataStream.drop"},{"location":"datastream/explain/","text":"DataStream.explain This will not trigger the execution of your computation graph but will produce a graph of the execution plan. Parameters: Name Type Description Default mode str 'graph' will show a graph, 'text' will print a textual description. 'graph' Return None. Source code in pyquokka/datastream.py 118 119 120 121 122 123 124 125 126 127 def explain ( self , mode = \"graph\" ): ''' This will not trigger the execution of your computation graph but will produce a graph of the execution plan. Args: mode (str): 'graph' will show a graph, 'text' will print a textual description. Return: None. ''' dataset = self . quokka_context . new_dataset ( self , self . schema ) return self . quokka_context . execute_node ( dataset . source_node_id , explain = True , mode = mode )","title":"DataStream.explain"},{"location":"datastream/explain/#datastreamexplain","text":"This will not trigger the execution of your computation graph but will produce a graph of the execution plan. Parameters: Name Type Description Default mode str 'graph' will show a graph, 'text' will print a textual description. 'graph' Return None. Source code in pyquokka/datastream.py 118 119 120 121 122 123 124 125 126 127 def explain ( self , mode = \"graph\" ): ''' This will not trigger the execution of your computation graph but will produce a graph of the execution plan. Args: mode (str): 'graph' will show a graph, 'text' will print a textual description. Return: None. ''' dataset = self . quokka_context . new_dataset ( self , self . schema ) return self . quokka_context . execute_node ( dataset . source_node_id , explain = True , mode = mode )","title":"DataStream.explain"},{"location":"datastream/filter/","text":"DataStream.filter This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by explain() . Parameters: Name Type Description Default predicate Expression an Expression. required Return A DataStream consisting of rows from the source DataStream that match the predicate. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5 >>> f = f . filter (( f [ \"l_orderkey\" ] < 10 ) & ( f [ \"l_partkey\" ] > 5 \")) Nested conditions are supported. >>> f = f . filter ( f [ \"l_orderkey\" ] < 10 & ( f [ \"l_partkey\" ] > 5 or f [ \"l_partkey\" ] < 1 )) You can do some really complicated stuff! For details on the .str and .dt namespaces see the API reference. Quokka strives to support all the functionality of Polars, so if you see something you need that is not supported, please file an issue on Github. >>> f = f . filter (( f [ \"l_shipdate\" ] . str . strptime () . dt . offset_by ( 1 , \"M\" ) . dt . week () == 3 ) & ( f [ \"l_orderkey\" ] < 1000 )) This will fail! Assuming c_custkey is not in f.schema >>> f = f . filter ( f [ \"c_custkey\" ] > 10 ) Source code in pyquokka/datastream.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def filter ( self , predicate : Expression ): \"\"\" This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by `explain()`. Args: predicate (Expression): an Expression. Return: A DataStream consisting of rows from the source DataStream that match the predicate. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5 >>> f = f.filter((f[\"l_orderkey\"] < 10) & (f[\"l_partkey\"] > 5\")) Nested conditions are supported. >>> f = f.filter(f[\"l_orderkey\"] < 10 & (f[\"l_partkey\"] > 5 or f[\"l_partkey\"] < 1)) You can do some really complicated stuff! For details on the .str and .dt namespaces see the API reference. Quokka strives to support all the functionality of Polars, so if you see something you need that is not supported, please file an issue on Github. >>> f = f.filter((f[\"l_shipdate\"].str.strptime().dt.offset_by(1, \"M\").dt.week() == 3) & (f[\"l_orderkey\"] < 1000)) This will fail! Assuming c_custkey is not in f.schema >>> f = f.filter(f[\"c_custkey\"] > 10) \"\"\" assert type ( predicate ) == Expression , \"Must supply an Expression.\" return self . filter_sql ( predicate . sql ())","title":"DataStream.filter"},{"location":"datastream/filter/#datastreamfilter","text":"This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by explain() . Parameters: Name Type Description Default predicate Expression an Expression. required Return A DataStream consisting of rows from the source DataStream that match the predicate. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5 >>> f = f . filter (( f [ \"l_orderkey\" ] < 10 ) & ( f [ \"l_partkey\" ] > 5 \")) Nested conditions are supported. >>> f = f . filter ( f [ \"l_orderkey\" ] < 10 & ( f [ \"l_partkey\" ] > 5 or f [ \"l_partkey\" ] < 1 )) You can do some really complicated stuff! For details on the .str and .dt namespaces see the API reference. Quokka strives to support all the functionality of Polars, so if you see something you need that is not supported, please file an issue on Github. >>> f = f . filter (( f [ \"l_shipdate\" ] . str . strptime () . dt . offset_by ( 1 , \"M\" ) . dt . week () == 3 ) & ( f [ \"l_orderkey\" ] < 1000 )) This will fail! Assuming c_custkey is not in f.schema >>> f = f . filter ( f [ \"c_custkey\" ] > 10 ) Source code in pyquokka/datastream.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def filter ( self , predicate : Expression ): \"\"\" This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by `explain()`. Args: predicate (Expression): an Expression. Return: A DataStream consisting of rows from the source DataStream that match the predicate. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5 >>> f = f.filter((f[\"l_orderkey\"] < 10) & (f[\"l_partkey\"] > 5\")) Nested conditions are supported. >>> f = f.filter(f[\"l_orderkey\"] < 10 & (f[\"l_partkey\"] > 5 or f[\"l_partkey\"] < 1)) You can do some really complicated stuff! For details on the .str and .dt namespaces see the API reference. Quokka strives to support all the functionality of Polars, so if you see something you need that is not supported, please file an issue on Github. >>> f = f.filter((f[\"l_shipdate\"].str.strptime().dt.offset_by(1, \"M\").dt.week() == 3) & (f[\"l_orderkey\"] < 1000)) This will fail! Assuming c_custkey is not in f.schema >>> f = f.filter(f[\"c_custkey\"] > 10) \"\"\" assert type ( predicate ) == Expression , \"Must supply an Expression.\" return self . filter_sql ( predicate . sql ())","title":"DataStream.filter"},{"location":"datastream/filter_sql/","text":"DataStream.filter_sql This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by explain() . Parameters: Name Type Description Default predicate str a SQL WHERE clause, look at the examples. required Return A DataStream consisting of rows from the source DataStream that match the predicate. Examples: Read in a CSV file into a DataStream f. >>> f = qc . read_csv ( \"lineitem.csv\" ) Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5. >>> f = f . filter_sql ( \"l_orderkey < 10 and l_partkey > 5\" ) Nested conditions are supported. >>> f = f . filter_sql ( \"l_orderkey < 10 and (l_partkey > 5 or l_partkey < 1)\" ) Most SQL features such as IN and date are supported. Anything DuckDB supports should work. >>> f = f . filter_sql ( \"l_shipmode IN ('MAIL','SHIP') and l_receiptdate < date '1995-01-01'\" ) You can do arithmetic in the predicate just like in SQL. >>> f = f . filter_sql ( \"l_shipdate < date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01\" ) This will fail! Assuming c_custkey is not in f.schema >>> f = f . filter_sql ( \"c_custkey > 10\" ) Source code in pyquokka/datastream.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def filter_sql ( self , predicate : str ): \"\"\" This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by `explain()`. Args: predicate (str): a SQL WHERE clause, look at the examples. Return: A DataStream consisting of rows from the source DataStream that match the predicate. Examples: Read in a CSV file into a DataStream f. >>> f = qc.read_csv(\"lineitem.csv\") Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5. >>> f = f.filter_sql(\"l_orderkey < 10 and l_partkey > 5\") Nested conditions are supported. >>> f = f.filter_sql(\"l_orderkey < 10 and (l_partkey > 5 or l_partkey < 1)\") Most SQL features such as IN and date are supported. Anything DuckDB supports should work. >>> f = f.filter_sql(\"l_shipmode IN ('MAIL','SHIP') and l_receiptdate < date '1995-01-01'\") You can do arithmetic in the predicate just like in SQL. >>> f = f.filter_sql(\"l_shipdate < date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01\") This will fail! Assuming c_custkey is not in f.schema >>> f = f.filter_sql(\"c_custkey > 10\") \"\"\" assert type ( predicate ) == str predicate = sqlglot . parse_one ( predicate ) # convert to CNF predicate = optimizer . normalize . normalize ( predicate , dnf = False ) columns = set ( i . name for i in predicate . find_all ( sqlglot . expressions . Column )) for column in columns : assert column in self . schema , \"Tried to filter on a column not in the schema {} \" . format ( column ) if self . materialized : batch_arrow = self . _get_materialized_df () . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) df = polars . from_arrow ( con . execute ( \"select * from batch_arrow where \" + predicate . sql ( dialect = \"duckdb\" )) . arrow ()) return self . quokka_context . from_polars ( df ) if not optimizer . normalize . normalized ( predicate ): def f ( df ): batch_arrow = df . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) return polars . from_arrow ( con . execute ( \"select * from batch_arrow where \" + predicate . sql ( dialect = \"duckdb\" )) . arrow ()) transformed = self . transform ( f , new_schema = self . schema , required_columns = self . schema ) return transformed else : return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = FilterNode ( self . schema , predicate ), schema = self . schema , sorted = self . sorted )","title":"DataStream.filter_sql"},{"location":"datastream/filter_sql/#datastreamfilter_sql","text":"This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by explain() . Parameters: Name Type Description Default predicate str a SQL WHERE clause, look at the examples. required Return A DataStream consisting of rows from the source DataStream that match the predicate. Examples: Read in a CSV file into a DataStream f. >>> f = qc . read_csv ( \"lineitem.csv\" ) Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5. >>> f = f . filter_sql ( \"l_orderkey < 10 and l_partkey > 5\" ) Nested conditions are supported. >>> f = f . filter_sql ( \"l_orderkey < 10 and (l_partkey > 5 or l_partkey < 1)\" ) Most SQL features such as IN and date are supported. Anything DuckDB supports should work. >>> f = f . filter_sql ( \"l_shipmode IN ('MAIL','SHIP') and l_receiptdate < date '1995-01-01'\" ) You can do arithmetic in the predicate just like in SQL. >>> f = f . filter_sql ( \"l_shipdate < date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01\" ) This will fail! Assuming c_custkey is not in f.schema >>> f = f . filter_sql ( \"c_custkey > 10\" ) Source code in pyquokka/datastream.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def filter_sql ( self , predicate : str ): \"\"\" This will filter the DataStream to contain only rows that match a certain predicate specified in SQL syntax. You can write any SQL clause you would generally put in a WHERE statement containing arbitrary conjunctions and disjunctions. The columns in your statement must be in the schema of this DataStream! Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from a filter being applied to a batch in the source DataStream. While this certainly may be the case, filters are aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a filter node in a Quokka execution plan shown by `explain()`. Args: predicate (str): a SQL WHERE clause, look at the examples. Return: A DataStream consisting of rows from the source DataStream that match the predicate. Examples: Read in a CSV file into a DataStream f. >>> f = qc.read_csv(\"lineitem.csv\") Filter for all the rows where l_orderkey smaller than 10 and l_partkey greater than 5. >>> f = f.filter_sql(\"l_orderkey < 10 and l_partkey > 5\") Nested conditions are supported. >>> f = f.filter_sql(\"l_orderkey < 10 and (l_partkey > 5 or l_partkey < 1)\") Most SQL features such as IN and date are supported. Anything DuckDB supports should work. >>> f = f.filter_sql(\"l_shipmode IN ('MAIL','SHIP') and l_receiptdate < date '1995-01-01'\") You can do arithmetic in the predicate just like in SQL. >>> f = f.filter_sql(\"l_shipdate < date '1994-01-01' + interval '1' year and l_discount between 0.06 - 0.01 and 0.06 + 0.01\") This will fail! Assuming c_custkey is not in f.schema >>> f = f.filter_sql(\"c_custkey > 10\") \"\"\" assert type ( predicate ) == str predicate = sqlglot . parse_one ( predicate ) # convert to CNF predicate = optimizer . normalize . normalize ( predicate , dnf = False ) columns = set ( i . name for i in predicate . find_all ( sqlglot . expressions . Column )) for column in columns : assert column in self . schema , \"Tried to filter on a column not in the schema {} \" . format ( column ) if self . materialized : batch_arrow = self . _get_materialized_df () . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) df = polars . from_arrow ( con . execute ( \"select * from batch_arrow where \" + predicate . sql ( dialect = \"duckdb\" )) . arrow ()) return self . quokka_context . from_polars ( df ) if not optimizer . normalize . normalized ( predicate ): def f ( df ): batch_arrow = df . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) return polars . from_arrow ( con . execute ( \"select * from batch_arrow where \" + predicate . sql ( dialect = \"duckdb\" )) . arrow ()) transformed = self . transform ( f , new_schema = self . schema , required_columns = self . schema ) return transformed else : return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = FilterNode ( self . schema , predicate ), schema = self . schema , sorted = self . sorted )","title":"DataStream.filter_sql"},{"location":"datastream/gramian/","text":"DataStream.gramian This will compute DataStream[columns]^T * DataStream[columns]. The result will be len(columns) * len(columns), with schema same as columns. Parameters: Name Type Description Default columns list List of columns. required Return A new DataStream of shape len(columns) * len(columns) which is DataStream[columns]^T * DataStream[columns]. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) Now create two columns high and low using SQL. >>> d = d . gramian ([ \"l_quantity\" , \"l_extendedprice\" ]) Result will be a 2x2 matrix. Source code in pyquokka/datastream.py 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 def gramian ( self , columns , demean = None ): \"\"\" This will compute DataStream[columns]^T * DataStream[columns]. The result will be len(columns) * len(columns), with schema same as columns. Args: columns (list): List of columns. Return: A new DataStream of shape len(columns) * len(columns) which is DataStream[columns]^T * DataStream[columns]. Examples: >>> d = qc.read_csv(\"lineitem.csv\") Now create two columns high and low using SQL. >>> d = d.gramian([\"l_quantity\", \"l_extendedprice\"]) Result will be a 2x2 matrix. \"\"\" def udf2 ( x ): x = x . select ( columns ) . to_numpy () - demean with threadpool_limits ( limits = 8 , user_api = 'blas' ): product = np . dot ( x . transpose (), x ) return polars . from_numpy ( product , schema = columns ) for col in columns : assert col in self . schema if demean is not None : assert type ( demean ) == np . ndarray , \"demean must be a numpy array\" assert len ( demean ) == len ( columns ), \"demean must be the same length as columns\" else : demean = np . zeros ( len ( columns )) if self . materialized : df = self . _get_materialized_df () stuff = df . select ( columns ) . to_numpy () - demean product = np . dot ( stuff . transpose (), stuff ) return self . quokka_context . from_polars ( polars . from_numpy ( product , schema = columns )) class AggExecutor ( Executor ): def __init__ ( self ) -> None : self . state = None def execute ( self , batches , stream_id , executor_id ): for batch in batches : #print(batch) if self . state is None : self . state = polars . from_arrow ( batch ) else : self . state += polars . from_arrow ( batch ) def done ( self , executor_id ): return self . state local_agg_executor = AggExecutor () agg_executor = AggExecutor () stream = self . select ( columns ) stream = stream . transform ( udf2 , new_schema = columns , required_columns = set ( columns ), foldable = True ) stream = stream . stateful_transform ( local_agg_executor , columns , required_columns = set ( columns ), partitioner = PassThroughPartitioner (), placement_strategy = CustomChannelsStrategy ( 1 )) return stream . stateful_transform ( agg_executor , columns , required_columns = set ( columns ), partitioner = BroadcastPartitioner (), placement_strategy = SingleChannelStrategy ())","title":"DataStream.gramian"},{"location":"datastream/gramian/#datastreamgramian","text":"This will compute DataStream[columns]^T * DataStream[columns]. The result will be len(columns) * len(columns), with schema same as columns. Parameters: Name Type Description Default columns list List of columns. required Return A new DataStream of shape len(columns) * len(columns) which is DataStream[columns]^T * DataStream[columns]. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) Now create two columns high and low using SQL. >>> d = d . gramian ([ \"l_quantity\" , \"l_extendedprice\" ]) Result will be a 2x2 matrix. Source code in pyquokka/datastream.py 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 def gramian ( self , columns , demean = None ): \"\"\" This will compute DataStream[columns]^T * DataStream[columns]. The result will be len(columns) * len(columns), with schema same as columns. Args: columns (list): List of columns. Return: A new DataStream of shape len(columns) * len(columns) which is DataStream[columns]^T * DataStream[columns]. Examples: >>> d = qc.read_csv(\"lineitem.csv\") Now create two columns high and low using SQL. >>> d = d.gramian([\"l_quantity\", \"l_extendedprice\"]) Result will be a 2x2 matrix. \"\"\" def udf2 ( x ): x = x . select ( columns ) . to_numpy () - demean with threadpool_limits ( limits = 8 , user_api = 'blas' ): product = np . dot ( x . transpose (), x ) return polars . from_numpy ( product , schema = columns ) for col in columns : assert col in self . schema if demean is not None : assert type ( demean ) == np . ndarray , \"demean must be a numpy array\" assert len ( demean ) == len ( columns ), \"demean must be the same length as columns\" else : demean = np . zeros ( len ( columns )) if self . materialized : df = self . _get_materialized_df () stuff = df . select ( columns ) . to_numpy () - demean product = np . dot ( stuff . transpose (), stuff ) return self . quokka_context . from_polars ( polars . from_numpy ( product , schema = columns )) class AggExecutor ( Executor ): def __init__ ( self ) -> None : self . state = None def execute ( self , batches , stream_id , executor_id ): for batch in batches : #print(batch) if self . state is None : self . state = polars . from_arrow ( batch ) else : self . state += polars . from_arrow ( batch ) def done ( self , executor_id ): return self . state local_agg_executor = AggExecutor () agg_executor = AggExecutor () stream = self . select ( columns ) stream = stream . transform ( udf2 , new_schema = columns , required_columns = set ( columns ), foldable = True ) stream = stream . stateful_transform ( local_agg_executor , columns , required_columns = set ( columns ), partitioner = PassThroughPartitioner (), placement_strategy = CustomChannelsStrategy ( 1 )) return stream . stateful_transform ( agg_executor , columns , required_columns = set ( columns ), partitioner = BroadcastPartitioner (), placement_strategy = SingleChannelStrategy ())","title":"DataStream.gramian"},{"location":"datastream/groupby/","text":"DataStream.groupby Group a DataStream on a list of columns, optionally specifying an ordering requirement. This returns a GroupedDataStream object, which currently only expose the aggregate method. This is similar to Pandas df.groupby().agg() syntax. Eventually the GroupedDataStream object will also support different kinds of window functions. Parameters: Name Type Description Default groupby list or str a column or a list of columns to group on. required orderby list a list of ordering requirements of the groupby columns, specified in a list like this: [(col1, \"asc\"), (col2, \"desc\")]. None Return A GroupedDataStream object with the specified grouping and the current DataStream. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) result will be a GroupedDataStream. >>> result = lineitem . groupby ([ \"l_orderkey\" , \"l_orderdate\" ], orderby = [( \"l_orderkey\" , \"asc\" ), ( \"l_orderdate\" , \"desc\" )]) Source code in pyquokka/datastream.py 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 def groupby ( self , groupby : list , orderby = None ): \"\"\" Group a DataStream on a list of columns, optionally specifying an ordering requirement. This returns a GroupedDataStream object, which currently only expose the `aggregate` method. This is similar to Pandas `df.groupby().agg()` syntax. Eventually the GroupedDataStream object will also support different kinds of window functions. Args: groupby (list or str): a column or a list of columns to group on. orderby (list): a list of ordering requirements of the groupby columns, specified in a list like this: [(col1, \"asc\"), (col2, \"desc\")]. Return: A GroupedDataStream object with the specified grouping and the current DataStream. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") `result` will be a GroupedDataStream. >>> result = lineitem.groupby([\"l_orderkey\",\"l_orderdate\"], orderby = [(\"l_orderkey\", \"asc\"), (\"l_orderdate\", \"desc\")]) \"\"\" if type ( groupby ) == str : groupby = [ groupby ] assert type ( groupby ) == list and len ( groupby ) > 0 , \"must specify at least one group key as a list of group keys, i.e. [key1,key2]\" if orderby is not None : assert type ( orderby ) == list for i in range ( len ( orderby )): if type ( orderby [ i ]) == tuple : assert orderby [ i ][ 0 ] in groupby assert orderby [ i ][ 1 ] == \"asc\" or orderby [ i ][ 1 ] == \"desc\" elif type ( orderby [ i ]) == str : assert orderby [ i ] in groupby orderby [ i ] = ( orderby [ i ], \"asc\" ) else : raise Exception ( \"don't understand orderby format\" ) return GroupedDataStream ( self , groupby = groupby , orderby = orderby )","title":"DataStream.groupby"},{"location":"datastream/groupby/#datastreamgroupby","text":"Group a DataStream on a list of columns, optionally specifying an ordering requirement. This returns a GroupedDataStream object, which currently only expose the aggregate method. This is similar to Pandas df.groupby().agg() syntax. Eventually the GroupedDataStream object will also support different kinds of window functions. Parameters: Name Type Description Default groupby list or str a column or a list of columns to group on. required orderby list a list of ordering requirements of the groupby columns, specified in a list like this: [(col1, \"asc\"), (col2, \"desc\")]. None Return A GroupedDataStream object with the specified grouping and the current DataStream. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) result will be a GroupedDataStream. >>> result = lineitem . groupby ([ \"l_orderkey\" , \"l_orderdate\" ], orderby = [( \"l_orderkey\" , \"asc\" ), ( \"l_orderdate\" , \"desc\" )]) Source code in pyquokka/datastream.py 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 def groupby ( self , groupby : list , orderby = None ): \"\"\" Group a DataStream on a list of columns, optionally specifying an ordering requirement. This returns a GroupedDataStream object, which currently only expose the `aggregate` method. This is similar to Pandas `df.groupby().agg()` syntax. Eventually the GroupedDataStream object will also support different kinds of window functions. Args: groupby (list or str): a column or a list of columns to group on. orderby (list): a list of ordering requirements of the groupby columns, specified in a list like this: [(col1, \"asc\"), (col2, \"desc\")]. Return: A GroupedDataStream object with the specified grouping and the current DataStream. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") `result` will be a GroupedDataStream. >>> result = lineitem.groupby([\"l_orderkey\",\"l_orderdate\"], orderby = [(\"l_orderkey\", \"asc\"), (\"l_orderdate\", \"desc\")]) \"\"\" if type ( groupby ) == str : groupby = [ groupby ] assert type ( groupby ) == list and len ( groupby ) > 0 , \"must specify at least one group key as a list of group keys, i.e. [key1,key2]\" if orderby is not None : assert type ( orderby ) == list for i in range ( len ( orderby )): if type ( orderby [ i ]) == tuple : assert orderby [ i ][ 0 ] in groupby assert orderby [ i ][ 1 ] == \"asc\" or orderby [ i ][ 1 ] == \"desc\" elif type ( orderby [ i ]) == str : assert orderby [ i ] in groupby orderby [ i ] = ( orderby [ i ], \"asc\" ) else : raise Exception ( \"don't understand orderby format\" ) return GroupedDataStream ( self , groupby = groupby , orderby = orderby )","title":"DataStream.groupby"},{"location":"datastream/grouped_agg/","text":"GroupedDataStream.agg Aggregate this GroupedDataStream according to the defined aggregations. This is similar to Pandas df.groupby().agg() . The result's length will be however number of rows as there are unique group keys combinations. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call .compute() or .collect() on this DataStream instead of doing additional operations on it like .filter() since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do .explain() on it. Parameters: Name Type Description Default aggregations dict similar to a dictionary argument to Pandas df.agg() . The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. You should call .collect() or .compute() on it as it is impossible to pipeline past an aggregation, so might as well as materialize it right now. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) >>> d = lineitem . filter ( \"l_shipdate <= date '1998-12-01' - interval '90' day\" ) >>> d = d . with_column ( \"disc_price\" , lambda x : x [ \"l_extendedprice\" ] * ( 1 - x [ \"l_discount\" ]), required_columns = { \"l_extendedprice\" , \"l_discount\" }) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well, of each unique combination of l_returnflag and l_linestatus >>> f = d . groupby ([ \"l_returnflag\" , \"l_linestatus\" ]) . agg ({ \"l_quantity\" :[ \"sum\" , \"avg\" ], \"l_extendedprice\" :[ \"sum\" , \"avg\" ], \"disc_price\" : \"sum\" , \"l_discount\" : \"min\" , \"*\" : \"count\" }) Source code in pyquokka/datastream.py 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 def agg ( self , aggregations : dict ): \"\"\" Aggregate this GroupedDataStream according to the defined aggregations. This is similar to Pandas `df.groupby().agg()`. The result's length will be however number of rows as there are unique group keys combinations. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call `.compute()` or `.collect()` on this DataStream instead of doing additional operations on it like `.filter()` since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do `.explain()` on it. Args: aggregations (dict): similar to a dictionary argument to Pandas `df.agg()`. The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. You should call `.collect()` or `.compute()` on it as it is impossible to pipeline past an aggregation, so might as well as materialize it right now. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") >>> d = lineitem.filter(\"l_shipdate <= date '1998-12-01' - interval '90' day\") >>> d = d.with_column(\"disc_price\", lambda x:x[\"l_extendedprice\"] * (1 - x[\"l_discount\"]), required_columns ={\"l_extendedprice\", \"l_discount\"}) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well, of each unique combination of l_returnflag and l_linestatus >>> f = d.groupby([\"l_returnflag\", \"l_linestatus\"]).agg({\"l_quantity\":[\"sum\",\"avg\"], \"l_extendedprice\":[\"sum\",\"avg\"], \"disc_price\":\"sum\", \"l_discount\":\"min\",\"*\":\"count\"}) \"\"\" return self . source_data_stream . _grouped_aggregate ( self . groupby , aggregations , self . orderby )","title":"GroupedDataStream.aggregate"},{"location":"datastream/grouped_agg/#groupeddatastreamagg","text":"Aggregate this GroupedDataStream according to the defined aggregations. This is similar to Pandas df.groupby().agg() . The result's length will be however number of rows as there are unique group keys combinations. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call .compute() or .collect() on this DataStream instead of doing additional operations on it like .filter() since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do .explain() on it. Parameters: Name Type Description Default aggregations dict similar to a dictionary argument to Pandas df.agg() . The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. You should call .collect() or .compute() on it as it is impossible to pipeline past an aggregation, so might as well as materialize it right now. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) >>> d = lineitem . filter ( \"l_shipdate <= date '1998-12-01' - interval '90' day\" ) >>> d = d . with_column ( \"disc_price\" , lambda x : x [ \"l_extendedprice\" ] * ( 1 - x [ \"l_discount\" ]), required_columns = { \"l_extendedprice\" , \"l_discount\" }) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well, of each unique combination of l_returnflag and l_linestatus >>> f = d . groupby ([ \"l_returnflag\" , \"l_linestatus\" ]) . agg ({ \"l_quantity\" :[ \"sum\" , \"avg\" ], \"l_extendedprice\" :[ \"sum\" , \"avg\" ], \"disc_price\" : \"sum\" , \"l_discount\" : \"min\" , \"*\" : \"count\" }) Source code in pyquokka/datastream.py 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 def agg ( self , aggregations : dict ): \"\"\" Aggregate this GroupedDataStream according to the defined aggregations. This is similar to Pandas `df.groupby().agg()`. The result's length will be however number of rows as there are unique group keys combinations. The result is a DataStream that will return a batch when the entire aggregation is done, since it's impossible to return any aggregation results without seeing the entire dataset. As a result, you should call `.compute()` or `.collect()` on this DataStream instead of doing additional operations on it like `.filter()` since those won't be pipelined anyways. The only reason Quokka by default returns a DataStream instead of just returning a Polars DataFrame or a Quokka DataSet is so you can do `.explain()` on it. Args: aggregations (dict): similar to a dictionary argument to Pandas `df.agg()`. The key is the column name, where the value is a str that is \"min\", \"max\", \"mean\", \"sum\", \"avg\" or a list of such strings. If you desire to have the count column in your result, add a key \"*\" with value \"count\". Look at the examples. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. You should call `.collect()` or `.compute()` on it as it is impossible to pipeline past an aggregation, so might as well as materialize it right now. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") >>> d = lineitem.filter(\"l_shipdate <= date '1998-12-01' - interval '90' day\") >>> d = d.with_column(\"disc_price\", lambda x:x[\"l_extendedprice\"] * (1 - x[\"l_discount\"]), required_columns ={\"l_extendedprice\", \"l_discount\"}) I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount column, and oh give me the total row count as well, of each unique combination of l_returnflag and l_linestatus >>> f = d.groupby([\"l_returnflag\", \"l_linestatus\"]).agg({\"l_quantity\":[\"sum\",\"avg\"], \"l_extendedprice\":[\"sum\",\"avg\"], \"disc_price\":\"sum\", \"l_discount\":\"min\",\"*\":\"count\"}) \"\"\" return self . source_data_stream . _grouped_aggregate ( self . groupby , aggregations , self . orderby )","title":"GroupedDataStream.agg"},{"location":"datastream/grouped_agg_sql/","text":"GroupedDataStream.agg_sql The SQL version of agg . Look at the examples. Parameters: Name Type Description Default aggregations str a string that is a valid SQL aggregation expression. Look at the examples. required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d . groupby ([ \"l_orderkey\" , \"o_orderdate\" , \"o_shippriority\" ]) . agg_sql ( \"sum(l_extendedprice * (1 - l_discount)) as revenue\" ) >>> f = d . groupby ( \"o_orderpriority\" ) . agg_sql ( \"count(*) as count_order\" ) >>> f = d . groupby ( \"l_shipmode\" ) . agg_sql ( \" >>> sum ( case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end ) as high_line_count , >>> sum ( case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end ) as low_line_count >>> \") Source code in pyquokka/datastream.py 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 def agg_sql ( self , aggregations : str ): \"\"\" The SQL version of `agg`. Look at the examples. Args: aggregations (str): a string that is a valid SQL aggregation expression. Look at the examples. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d.groupby([\"l_orderkey\",\"o_orderdate\",\"o_shippriority\"]).agg_sql(\"sum(l_extendedprice * (1 - l_discount)) as revenue\") >>> f = d.groupby(\"o_orderpriority\").agg_sql(\"count(*) as count_order\") >>> f = d.groupby(\"l_shipmode\").agg_sql(\" >>> sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, >>> sum(case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end) as low_line_count >>> \") \"\"\" return self . source_data_stream . _grouped_aggregate_sql ( self . groupby , aggregations , self . orderby )","title":"GroupedDataStream.agg_sql"},{"location":"datastream/grouped_agg_sql/#groupeddatastreamagg_sql","text":"The SQL version of agg . Look at the examples. Parameters: Name Type Description Default aggregations str a string that is a valid SQL aggregation expression. Look at the examples. required Return A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d . groupby ([ \"l_orderkey\" , \"o_orderdate\" , \"o_shippriority\" ]) . agg_sql ( \"sum(l_extendedprice * (1 - l_discount)) as revenue\" ) >>> f = d . groupby ( \"o_orderpriority\" ) . agg_sql ( \"count(*) as count_order\" ) >>> f = d . groupby ( \"l_shipmode\" ) . agg_sql ( \" >>> sum ( case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end ) as high_line_count , >>> sum ( case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end ) as low_line_count >>> \") Source code in pyquokka/datastream.py 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 def agg_sql ( self , aggregations : str ): \"\"\" The SQL version of `agg`. Look at the examples. Args: aggregations (str): a string that is a valid SQL aggregation expression. Look at the examples. Return: A DataStream object that holds the aggregation result. It will only emit one batch, which is the result when it's done. Examples: >>> d = d.groupby([\"l_orderkey\",\"o_orderdate\",\"o_shippriority\"]).agg_sql(\"sum(l_extendedprice * (1 - l_discount)) as revenue\") >>> f = d.groupby(\"o_orderpriority\").agg_sql(\"count(*) as count_order\") >>> f = d.groupby(\"l_shipmode\").agg_sql(\" >>> sum(case when o_orderpriority = '1-URGENT' or o_orderpriority = '2-HIGH' then 1 else 0 end) as high_line_count, >>> sum(case when o_orderpriority <> '1-URGENT' and o_orderpriority <> '2-HIGH' then 1 else 0 end) as low_line_count >>> \") \"\"\" return self . source_data_stream . _grouped_aggregate_sql ( self . groupby , aggregations , self . orderby )","title":"GroupedDataStream.agg_sql"},{"location":"datastream/grouped_count_distinct/","text":"GroupedDataStream.count_distinct Count the number of distinct values of a column for each group. This may result in out of memory. This is not approximate. Parameters: Name Type Description Default col str the column to count distinct values of required Source code in pyquokka/datastream.py 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 def count_distinct ( self , col : str ): \"\"\" Count the number of distinct values of a column for each group. This may result in out of memory. This is not approximate. Args: col (str): the column to count distinct values of \"\"\" return self . source_data_stream . _grouped_count_distinct ( self . groupby , col , self . orderby )","title":"GroupedDataStream.count_distinct"},{"location":"datastream/grouped_count_distinct/#groupeddatastreamcount_distinct","text":"Count the number of distinct values of a column for each group. This may result in out of memory. This is not approximate. Parameters: Name Type Description Default col str the column to count distinct values of required Source code in pyquokka/datastream.py 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 def count_distinct ( self , col : str ): \"\"\" Count the number of distinct values of a column for each group. This may result in out of memory. This is not approximate. Args: col (str): the column to count distinct values of \"\"\" return self . source_data_stream . _grouped_count_distinct ( self . groupby , col , self . orderby )","title":"GroupedDataStream.count_distinct"},{"location":"datastream/join/","text":"DataStream.join Join a DataStream with another DataStream. This may result in a distributed hash join or a broadcast join depending on cardinality estimates. Parameters: Name Type Description Default right DataStream the DataStream to join to. required on str You could either specify this, if the join column has the same name in this DataStream and right , or left_on and right_on if the join columns don't have the same name. None left_on str the name of the join column in this DataStream. None right_on str the name of the join column in right . None suffix str if right has columns with the same names as columns in this DataStream, their names will be appended with the suffix in the result. '_2' how str supports \"inner\", \"left\", \"semi\" or \"anti\" 'inner' Return A new DataStream that's the joined result of this DataStream and \"right\". By default, columns from both side will be retained, except for right_on from the right side. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) >>> orders = qc . read_csv ( \"orders.csv\" ) >>> result = lineitem . join ( orders , left_on = \"l_orderkey\" , right_on = \"o_orderkey\" ) >>> result = result . select ([ \"o_orderkey\" ]) Source code in pyquokka/datastream.py 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 def join ( self , right , on = None , left_on = None , right_on = None , suffix = \"_2\" , how = \"inner\" , maintain_sort_order = None ): \"\"\" Join a DataStream with another DataStream. This may result in a distributed hash join or a broadcast join depending on cardinality estimates. Args: right (DataStream): the DataStream to join to. on (str): You could either specify this, if the join column has the same name in this DataStream and `right`, or `left_on` and `right_on` if the join columns don't have the same name. left_on (str): the name of the join column in this DataStream. right_on (str): the name of the join column in `right`. suffix (str): if `right` has columns with the same names as columns in this DataStream, their names will be appended with the suffix in the result. how (str): supports \"inner\", \"left\", \"semi\" or \"anti\" Return: A new DataStream that's the joined result of this DataStream and \"right\". By default, columns from both side will be retained, except for `right_on` from the right side. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") >>> orders = qc.read_csv(\"orders.csv\") >>> result = lineitem.join(orders, left_on = \"l_orderkey\", right_on = \"o_orderkey\") >>> result = result.select([\"o_orderkey\"]) \"\"\" assert how in { \"inner\" , \"left\" , \"semi\" , \"anti\" } assert issubclass ( type ( right ), DataStream ), \"must join against a Quokka DataStream\" if maintain_sort_order is not None : assert how in { \"inner\" , \"left\" } # our broadcast join strategy should automatically satisfy this, no need to do anything special if type ( right ) == polars . DataFrame : assert maintain_sort_order == \"left\" assert self . sorted is not None else : assert maintain_sort_order in { \"left\" , \"right\" } if maintain_sort_order == \"left\" : assert self . sorted is not None else : assert right . sorted is not None if how == \"left\" : assert maintain_sort_order == \"right\" , \"in a left join, can only maintain order of the right table\" #if type(right) == polars.DataFrame and right.to_arrow().nbytes > 10485760: # raise Exception(\"You cannot join a DataStream against a Polars DataFrame more than 10MB in size. Sorry.\") if on is None : assert left_on is not None and right_on is not None assert left_on in self . schema , \"join key not found in left table\" assert right_on in right . schema , \"join key not found in right table\" else : assert on in self . schema , \"join key not found in left table\" assert on in right . schema , \"join key not found in right table\" left_on = on right_on = on on = None # we can't do this check since schema is now a list of names with no type info. This should change in the future. #assert node1.schema[left_on] == node2.schema[right_on], \"join column has different schema in tables\" new_schema = self . schema . copy () if self . materialized : schema_mapping = { col : { - 1 : col } for col in self . schema } else : schema_mapping = { col : { 0 : col } for col in self . schema } # if the right table is already materialized, the schema mapping should forget about it since we can't push anything down anyways. # an optimization could be to push down the predicate directly to the materialized Polars DataFrame in the BroadcastJoinExecutor # leave this as a TODO. this could be greatly benenficial if it significantly reduces the size of the small table. if right . materialized : right_table_id = - 1 else : right_table_id = 1 rename_dict = {} # import pdb;pdb.set_trace() right_cols = right . schema if how not in { \"semi\" , \"anti\" } else [ right_on ] for col in right_cols : if col == right_on : continue if col in new_schema : assert col + \\ suffix not in new_schema , ( \"the suffix was not enough to guarantee unique col names\" , col + suffix , new_schema ) new_schema . append ( col + suffix ) schema_mapping [ col + suffix ] = { right_table_id : col + suffix } rename_dict [ col ] = col + suffix else : new_schema . append ( col ) schema_mapping [ col ] = { right_table_id : col } # you only need the key column on the RHS! select overloads in DataStream or Polars DataFrame runtime polymorphic if how == \"semi\" or how == \"anti\" : right = right . select ([ right_on ]) if len ( rename_dict ) > 0 : right = right . rename ( rename_dict ) if ( not self . materialized and not right . materialized ) or ( self . materialized and not right . materialized and how != \"inner\" ): # if self.materialized, rewrite the schema_mapping for col in schema_mapping : if list ( schema_mapping [ col ] . keys ())[ 0 ] == - 1 : schema_mapping [ col ] = { 0 : col } if maintain_sort_order is None : assume_sorted = {} elif maintain_sort_order == \"left\" : assume_sorted = { 0 : True } else : assume_sorted = { 1 : True } return self . quokka_context . new_stream ( sources = { 0 : self , 1 : right }, partitioners = { 0 : HashPartitioner ( left_on ), 1 : HashPartitioner ( right_on )}, node = JoinNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 0 : { left_on }, 1 : { right_on }}, join_spec = ( how , { 0 : left_on , 1 : right_on }), assume_sorted = assume_sorted ), schema = new_schema , ) elif self . materialized and not right . materialized : assert how in { \"inner\" } new_schema . remove ( left_on ) new_schema = [ right_on ] + new_schema del schema_mapping [ left_on ] schema_mapping [ right_on ] = { 1 : right_on } new_stream = self . quokka_context . new_stream ( sources = { 1 : right }, partitioners = { 1 : PassThroughPartitioner ()}, node = BroadcastJoinNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 1 : { right_on }}, operator = BroadcastJoinExecutor ( self . _get_materialized_df (), small_on = left_on , big_on = right_on , suffix = suffix , how = how ) ), schema = new_schema , ) if right_on == left_on : return new_stream else : return new_stream . rename ({ right_on : left_on }) elif not self . materialized and right . materialized : return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = BroadcastJoinNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 0 : { left_on }}, operator = BroadcastJoinExecutor ( right . _get_materialized_df (), small_on = right_on , big_on = left_on , suffix = suffix , how = how ) ), schema = new_schema , ) else : right_df = right . _get_materialized_df () left_df = self . _get_materialized_df () result = left_df . join ( right_df , how = how , left_on = left_on , right_on = right_on , suffix = suffix ) return self . quokka_context . from_polars ( result )","title":"DataStream.join"},{"location":"datastream/join/#datastreamjoin","text":"Join a DataStream with another DataStream. This may result in a distributed hash join or a broadcast join depending on cardinality estimates. Parameters: Name Type Description Default right DataStream the DataStream to join to. required on str You could either specify this, if the join column has the same name in this DataStream and right , or left_on and right_on if the join columns don't have the same name. None left_on str the name of the join column in this DataStream. None right_on str the name of the join column in right . None suffix str if right has columns with the same names as columns in this DataStream, their names will be appended with the suffix in the result. '_2' how str supports \"inner\", \"left\", \"semi\" or \"anti\" 'inner' Return A new DataStream that's the joined result of this DataStream and \"right\". By default, columns from both side will be retained, except for right_on from the right side. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) >>> orders = qc . read_csv ( \"orders.csv\" ) >>> result = lineitem . join ( orders , left_on = \"l_orderkey\" , right_on = \"o_orderkey\" ) >>> result = result . select ([ \"o_orderkey\" ]) Source code in pyquokka/datastream.py 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 def join ( self , right , on = None , left_on = None , right_on = None , suffix = \"_2\" , how = \"inner\" , maintain_sort_order = None ): \"\"\" Join a DataStream with another DataStream. This may result in a distributed hash join or a broadcast join depending on cardinality estimates. Args: right (DataStream): the DataStream to join to. on (str): You could either specify this, if the join column has the same name in this DataStream and `right`, or `left_on` and `right_on` if the join columns don't have the same name. left_on (str): the name of the join column in this DataStream. right_on (str): the name of the join column in `right`. suffix (str): if `right` has columns with the same names as columns in this DataStream, their names will be appended with the suffix in the result. how (str): supports \"inner\", \"left\", \"semi\" or \"anti\" Return: A new DataStream that's the joined result of this DataStream and \"right\". By default, columns from both side will be retained, except for `right_on` from the right side. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") >>> orders = qc.read_csv(\"orders.csv\") >>> result = lineitem.join(orders, left_on = \"l_orderkey\", right_on = \"o_orderkey\") >>> result = result.select([\"o_orderkey\"]) \"\"\" assert how in { \"inner\" , \"left\" , \"semi\" , \"anti\" } assert issubclass ( type ( right ), DataStream ), \"must join against a Quokka DataStream\" if maintain_sort_order is not None : assert how in { \"inner\" , \"left\" } # our broadcast join strategy should automatically satisfy this, no need to do anything special if type ( right ) == polars . DataFrame : assert maintain_sort_order == \"left\" assert self . sorted is not None else : assert maintain_sort_order in { \"left\" , \"right\" } if maintain_sort_order == \"left\" : assert self . sorted is not None else : assert right . sorted is not None if how == \"left\" : assert maintain_sort_order == \"right\" , \"in a left join, can only maintain order of the right table\" #if type(right) == polars.DataFrame and right.to_arrow().nbytes > 10485760: # raise Exception(\"You cannot join a DataStream against a Polars DataFrame more than 10MB in size. Sorry.\") if on is None : assert left_on is not None and right_on is not None assert left_on in self . schema , \"join key not found in left table\" assert right_on in right . schema , \"join key not found in right table\" else : assert on in self . schema , \"join key not found in left table\" assert on in right . schema , \"join key not found in right table\" left_on = on right_on = on on = None # we can't do this check since schema is now a list of names with no type info. This should change in the future. #assert node1.schema[left_on] == node2.schema[right_on], \"join column has different schema in tables\" new_schema = self . schema . copy () if self . materialized : schema_mapping = { col : { - 1 : col } for col in self . schema } else : schema_mapping = { col : { 0 : col } for col in self . schema } # if the right table is already materialized, the schema mapping should forget about it since we can't push anything down anyways. # an optimization could be to push down the predicate directly to the materialized Polars DataFrame in the BroadcastJoinExecutor # leave this as a TODO. this could be greatly benenficial if it significantly reduces the size of the small table. if right . materialized : right_table_id = - 1 else : right_table_id = 1 rename_dict = {} # import pdb;pdb.set_trace() right_cols = right . schema if how not in { \"semi\" , \"anti\" } else [ right_on ] for col in right_cols : if col == right_on : continue if col in new_schema : assert col + \\ suffix not in new_schema , ( \"the suffix was not enough to guarantee unique col names\" , col + suffix , new_schema ) new_schema . append ( col + suffix ) schema_mapping [ col + suffix ] = { right_table_id : col + suffix } rename_dict [ col ] = col + suffix else : new_schema . append ( col ) schema_mapping [ col ] = { right_table_id : col } # you only need the key column on the RHS! select overloads in DataStream or Polars DataFrame runtime polymorphic if how == \"semi\" or how == \"anti\" : right = right . select ([ right_on ]) if len ( rename_dict ) > 0 : right = right . rename ( rename_dict ) if ( not self . materialized and not right . materialized ) or ( self . materialized and not right . materialized and how != \"inner\" ): # if self.materialized, rewrite the schema_mapping for col in schema_mapping : if list ( schema_mapping [ col ] . keys ())[ 0 ] == - 1 : schema_mapping [ col ] = { 0 : col } if maintain_sort_order is None : assume_sorted = {} elif maintain_sort_order == \"left\" : assume_sorted = { 0 : True } else : assume_sorted = { 1 : True } return self . quokka_context . new_stream ( sources = { 0 : self , 1 : right }, partitioners = { 0 : HashPartitioner ( left_on ), 1 : HashPartitioner ( right_on )}, node = JoinNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 0 : { left_on }, 1 : { right_on }}, join_spec = ( how , { 0 : left_on , 1 : right_on }), assume_sorted = assume_sorted ), schema = new_schema , ) elif self . materialized and not right . materialized : assert how in { \"inner\" } new_schema . remove ( left_on ) new_schema = [ right_on ] + new_schema del schema_mapping [ left_on ] schema_mapping [ right_on ] = { 1 : right_on } new_stream = self . quokka_context . new_stream ( sources = { 1 : right }, partitioners = { 1 : PassThroughPartitioner ()}, node = BroadcastJoinNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 1 : { right_on }}, operator = BroadcastJoinExecutor ( self . _get_materialized_df (), small_on = left_on , big_on = right_on , suffix = suffix , how = how ) ), schema = new_schema , ) if right_on == left_on : return new_stream else : return new_stream . rename ({ right_on : left_on }) elif not self . materialized and right . materialized : return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = BroadcastJoinNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 0 : { left_on }}, operator = BroadcastJoinExecutor ( right . _get_materialized_df (), small_on = right_on , big_on = left_on , suffix = suffix , how = how ) ), schema = new_schema , ) else : right_df = right . _get_materialized_df () left_df = self . _get_materialized_df () result = left_df . join ( right_df , how = how , left_on = left_on , right_on = right_on , suffix = suffix ) return self . quokka_context . from_polars ( result )","title":"DataStream.join"},{"location":"datastream/max/","text":"DataStream.max Return the maximum values of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 def max ( self , columns , collect = True ): \"\"\" Return the maximum values of the specified columns. Args: columns (str or list): the column name or a list of column names. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"max\" for col in columns }) . collect () else : return self . agg ({ col : \"max\" for col in columns })","title":"DataStream.max"},{"location":"datastream/max/#datastreammax","text":"Return the maximum values of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 def max ( self , columns , collect = True ): \"\"\" Return the maximum values of the specified columns. Args: columns (str or list): the column name or a list of column names. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"max\" for col in columns }) . collect () else : return self . agg ({ col : \"max\" for col in columns })","title":"DataStream.max"},{"location":"datastream/mean/","text":"DataStream.mean Return the mean values of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 def mean ( self , columns , collect = True ): \"\"\" Return the mean values of the specified columns. Args: columns (str or list): the column name or a list of column names. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"mean\" for col in columns }) . collect () else : return self . agg ({ col : \"mean\" for col in columns })","title":"DataStream.mean"},{"location":"datastream/mean/#datastreammean","text":"Return the mean values of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 def mean ( self , columns , collect = True ): \"\"\" Return the mean values of the specified columns. Args: columns (str or list): the column name or a list of column names. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"mean\" for col in columns }) . collect () else : return self . agg ({ col : \"mean\" for col in columns })","title":"DataStream.mean"},{"location":"datastream/min/","text":"DataStream.min Return the minimum values of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 def min ( self , columns , collect = True ): \"\"\" Return the minimum values of the specified columns. Args: columns (str or list): the column name or a list of column names. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"min\" for col in columns }) . collect () else : return self . agg ({ col : \"min\" for col in columns })","title":"DataStream.min"},{"location":"datastream/min/#datastreammin","text":"Return the minimum values of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 def min ( self , columns , collect = True ): \"\"\" Return the minimum values of the specified columns. Args: columns (str or list): the column name or a list of column names. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"min\" for col in columns }) . collect () else : return self . agg ({ col : \"min\" for col in columns })","title":"DataStream.min"},{"location":"datastream/rename/","text":"DataStream.rename Renames columns in the DataStream according to rename_dict. This is similar to polars.rename . The keys you supply in rename_dict must be present in the schema, and the rename operation must not lead to duplicate column names. Note this will lead to a physical operation at runtime. This might also complicate join reodering, so should be avoided if possible. Parameters: Name Type Description Default rename_dict dict key is old column name, value is new column name. required Return A DataStream with new schema according to rename. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Rename the l_orderdate and l_orderkey columns >>> f = f . rename ({ \"l_orderdate\" : \"orderdate\" , \"l_orderkey\" : \"orderkey\" }) This will now fail, since you renamed l_orderdate >>> f = f . select ([ \"l_orderdate\" ]) Source code in pyquokka/datastream.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 def rename ( self , rename_dict ): \"\"\" Renames columns in the DataStream according to rename_dict. This is similar to [`polars.rename`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.rename.html). The keys you supply in rename_dict must be present in the schema, and the rename operation must not lead to duplicate column names. Note this will lead to a physical operation at runtime. This might also complicate join reodering, so should be avoided if possible. Args: rename_dict (dict): key is old column name, value is new column name. Return: A DataStream with new schema according to rename. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Rename the l_orderdate and l_orderkey columns >>> f = f.rename({\"l_orderdate\": \"orderdate\", \"l_orderkey\": \"orderkey\"}) This will now fail, since you renamed l_orderdate >>> f = f.select([\"l_orderdate\"]) \"\"\" new_sorted = {} assert type ( rename_dict ) == dict , \"must specify a dictionary like Polars\" for key in rename_dict : assert key in self . schema , \"key in rename dict must be in schema\" assert rename_dict [ key ] not in self . schema , \"new name must not be in current schema\" if self . sorted is not None and key in self . sorted : new_sorted [ rename_dict [ key ]] = self . sorted [ key ] if self . materialized : df = self . _get_materialized_df () . rename ( rename_dict ) return self . quokka_context . from_polars ( df ) # the fact you can write this in one line is why I love Python new_schema = [ col if col not in rename_dict else rename_dict [ col ] for col in self . schema ] schema_mapping = {} for key in rename_dict : schema_mapping [ rename_dict [ key ]] = { 0 : key } for key in self . schema : if key not in rename_dict : schema_mapping [ key ] = { 0 : key } def f ( x ): return x . rename ( rename_dict ) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 0 : set ( rename_dict . keys ())}, function = f , foldable = True ), schema = new_schema , sorted = new_sorted if len ( new_sorted ) > 0 else None )","title":"DataStream.rename"},{"location":"datastream/rename/#datastreamrename","text":"Renames columns in the DataStream according to rename_dict. This is similar to polars.rename . The keys you supply in rename_dict must be present in the schema, and the rename operation must not lead to duplicate column names. Note this will lead to a physical operation at runtime. This might also complicate join reodering, so should be avoided if possible. Parameters: Name Type Description Default rename_dict dict key is old column name, value is new column name. required Return A DataStream with new schema according to rename. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Rename the l_orderdate and l_orderkey columns >>> f = f . rename ({ \"l_orderdate\" : \"orderdate\" , \"l_orderkey\" : \"orderkey\" }) This will now fail, since you renamed l_orderdate >>> f = f . select ([ \"l_orderdate\" ]) Source code in pyquokka/datastream.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 def rename ( self , rename_dict ): \"\"\" Renames columns in the DataStream according to rename_dict. This is similar to [`polars.rename`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.rename.html). The keys you supply in rename_dict must be present in the schema, and the rename operation must not lead to duplicate column names. Note this will lead to a physical operation at runtime. This might also complicate join reodering, so should be avoided if possible. Args: rename_dict (dict): key is old column name, value is new column name. Return: A DataStream with new schema according to rename. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Rename the l_orderdate and l_orderkey columns >>> f = f.rename({\"l_orderdate\": \"orderdate\", \"l_orderkey\": \"orderkey\"}) This will now fail, since you renamed l_orderdate >>> f = f.select([\"l_orderdate\"]) \"\"\" new_sorted = {} assert type ( rename_dict ) == dict , \"must specify a dictionary like Polars\" for key in rename_dict : assert key in self . schema , \"key in rename dict must be in schema\" assert rename_dict [ key ] not in self . schema , \"new name must not be in current schema\" if self . sorted is not None and key in self . sorted : new_sorted [ rename_dict [ key ]] = self . sorted [ key ] if self . materialized : df = self . _get_materialized_df () . rename ( rename_dict ) return self . quokka_context . from_polars ( df ) # the fact you can write this in one line is why I love Python new_schema = [ col if col not in rename_dict else rename_dict [ col ] for col in self . schema ] schema_mapping = {} for key in rename_dict : schema_mapping [ rename_dict [ key ]] = { 0 : key } for key in self . schema : if key not in rename_dict : schema_mapping [ key ] = { 0 : key } def f ( x ): return x . rename ( rename_dict ) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = new_schema , schema_mapping = schema_mapping , required_columns = { 0 : set ( rename_dict . keys ())}, function = f , foldable = True ), schema = new_schema , sorted = new_sorted if len ( new_sorted ) > 0 else None )","title":"DataStream.rename"},{"location":"datastream/select/","text":"DataStream.select This will create a new DataStream that contains only selected columns from the source DataStream. Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from selecting columns from a batch in the source DataStream. While this certainly may be the case, select() is aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a select node in a Quokka execution plan shown by explain() . It is much better to think of a DataStream simply as a stream of rows that meet certain criteria, and who may be non-deterministically batched together by the Quokka runtime. Indeed, Quokka makes no guarantees on the sizes of these batches, which is determined at runtime. This flexibility is an important reason for Quokka's superior performance. Parameters: Name Type Description Default columns list a list of columns to select from the source DataStream required Return A DataStream consisting of only the columns selected. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Select only the l_orderdate and l_orderkey columns >>> f = f . select ([ \"l_orderdate\" , \"l_orderkey\" ]) This will now fail, since f's schema now consists of only two columns. >>> f = f . select ([ \"l_linenumber\" ]) Source code in pyquokka/datastream.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 def select ( self , columns : list ): \"\"\" This will create a new DataStream that contains only selected columns from the source DataStream. Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from selecting columns from a batch in the source DataStream. While this certainly may be the case, `select()` is aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a select node in a Quokka execution plan shown by `explain()`. It is much better to think of a DataStream simply as a stream of rows that meet certain criteria, and who may be non-deterministically batched together by the Quokka runtime. Indeed, Quokka makes no guarantees on the sizes of these batches, which is determined at runtime. This flexibility is an important reason for Quokka's superior performance. Args: columns (list): a list of columns to select from the source DataStream Return: A DataStream consisting of only the columns selected. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Select only the l_orderdate and l_orderkey columns >>> f = f.select([\"l_orderdate\", \"l_orderkey\"]) This will now fail, since f's schema now consists of only two columns. >>> f = f.select([\"l_linenumber\"]) \"\"\" assert type ( columns ) == set or type ( columns ) == list for column in columns : assert column in self . schema , \"Projection column not in schema\" if self . materialized : df = self . _get_materialized_df () . select ( columns ) return self . quokka_context . from_polars ( df ) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = ProjectionNode ( set ( columns )), schema = columns , sorted = self . sorted )","title":"DataStream.select"},{"location":"datastream/select/#datastreamselect","text":"This will create a new DataStream that contains only selected columns from the source DataStream. Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from selecting columns from a batch in the source DataStream. While this certainly may be the case, select() is aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a select node in a Quokka execution plan shown by explain() . It is much better to think of a DataStream simply as a stream of rows that meet certain criteria, and who may be non-deterministically batched together by the Quokka runtime. Indeed, Quokka makes no guarantees on the sizes of these batches, which is determined at runtime. This flexibility is an important reason for Quokka's superior performance. Parameters: Name Type Description Default columns list a list of columns to select from the source DataStream required Return A DataStream consisting of only the columns selected. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) Select only the l_orderdate and l_orderkey columns >>> f = f . select ([ \"l_orderdate\" , \"l_orderkey\" ]) This will now fail, since f's schema now consists of only two columns. >>> f = f . select ([ \"l_linenumber\" ]) Source code in pyquokka/datastream.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 def select ( self , columns : list ): \"\"\" This will create a new DataStream that contains only selected columns from the source DataStream. Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each batch directly results from selecting columns from a batch in the source DataStream. While this certainly may be the case, `select()` is aggressively optimized by Quokka and is most likely pushed all the way down to the input readers. As a result, you typically should not see a select node in a Quokka execution plan shown by `explain()`. It is much better to think of a DataStream simply as a stream of rows that meet certain criteria, and who may be non-deterministically batched together by the Quokka runtime. Indeed, Quokka makes no guarantees on the sizes of these batches, which is determined at runtime. This flexibility is an important reason for Quokka's superior performance. Args: columns (list): a list of columns to select from the source DataStream Return: A DataStream consisting of only the columns selected. Examples: >>> f = qc.read_csv(\"lineitem.csv\") Select only the l_orderdate and l_orderkey columns >>> f = f.select([\"l_orderdate\", \"l_orderkey\"]) This will now fail, since f's schema now consists of only two columns. >>> f = f.select([\"l_linenumber\"]) \"\"\" assert type ( columns ) == set or type ( columns ) == list for column in columns : assert column in self . schema , \"Projection column not in schema\" if self . materialized : df = self . _get_materialized_df () . select ( columns ) return self . quokka_context . from_polars ( df ) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = ProjectionNode ( set ( columns )), schema = columns , sorted = self . sorted )","title":"DataStream.select"},{"location":"datastream/stateful_transform/","text":"DataStream.stateful_transform EXPERIMENTAL API This is like transform , except you can use a stateful object as your transformation function. This is useful for example, if you want to run a heavy Pytorch model on each batch coming in, and you don't want to reload this model for each function call. Remember the transform API only supports stateless transformations. You could also implement much more complicated stateful transformations, like implementing your own aggregation function if you are not satisfied with Quokka's default operator's performance. This API is still being finalized. A version of it that takes multiple input streams is also going to be added. This is the part of the DataStream level api that is closest to the underlying execution engine. Quokka's underlying execution engine basically executes a series of stateful transformations on batches of data. The difficulty here is how much of that underlying API to expose here so it's still useful without the user having to understand how the Quokka runtime works. To that end, we have to come up with suitable partitioner and placement strategy abstraction classes and interfaces. If you are interested in helping us hammer out this API, please talke to me: zihengw@stanford.edu. Parameters: Name Type Description Default executor pyquokka . executors . Executor The stateful executor. It must be a subclass of pyquokka.executors.Executor , and expose the execute and done functions. More details forthcoming. required new_schema list The names of the columns of the Polars DataFrame that the transformation function produces. required required_columns list or set The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. required Return A transformed DataStream. Examples: Check the code for the gramian function. Source code in pyquokka/datastream.py 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 def stateful_transform ( self , executor : Executor , new_schema : list , required_columns : set , partitioner = PassThroughPartitioner (), placement_strategy = CustomChannelsStrategy ( 1 )): \"\"\" **EXPERIMENTAL API** This is like `transform`, except you can use a stateful object as your transformation function. This is useful for example, if you want to run a heavy Pytorch model on each batch coming in, and you don't want to reload this model for each function call. Remember the `transform` API only supports stateless transformations. You could also implement much more complicated stateful transformations, like implementing your own aggregation function if you are not satisfied with Quokka's default operator's performance. This API is still being finalized. A version of it that takes multiple input streams is also going to be added. This is the part of the DataStream level api that is closest to the underlying execution engine. Quokka's underlying execution engine basically executes a series of stateful transformations on batches of data. The difficulty here is how much of that underlying API to expose here so it's still useful without the user having to understand how the Quokka runtime works. To that end, we have to come up with suitable partitioner and placement strategy abstraction classes and interfaces. If you are interested in helping us hammer out this API, please talke to me: zihengw@stanford.edu. Args: executor (pyquokka.executors.Executor): The stateful executor. It must be a subclass of `pyquokka.executors.Executor`, and expose the `execute` and `done` functions. More details forthcoming. new_schema (list): The names of the columns of the Polars DataFrame that the transformation function produces. required_columns (list or set): The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. Return: A transformed DataStream. Examples: Check the code for the `gramian` function. \"\"\" assert type ( required_columns ) == set assert issubclass ( type ( executor ), Executor ), \"user defined executor must be an instance of a \\ child class of the Executor class defined in pyquokka.executors. You must override the execute and done methods.\" select_stream = self . select ( required_columns ) custom_node = StatefulNode ( schema = new_schema , # cannot push through any predicates or projections! schema_mapping = { col : { - 1 : col } for col in new_schema }, required_columns = { 0 : required_columns }, operator = executor ) custom_node . set_placement_strategy ( placement_strategy ) return self . quokka_context . new_stream ( sources = { 0 : select_stream }, partitioners = { 0 : partitioner }, node = custom_node , schema = new_schema , )","title":"DataStream.stateful_transform"},{"location":"datastream/stateful_transform/#datastreamstateful_transform","text":"EXPERIMENTAL API This is like transform , except you can use a stateful object as your transformation function. This is useful for example, if you want to run a heavy Pytorch model on each batch coming in, and you don't want to reload this model for each function call. Remember the transform API only supports stateless transformations. You could also implement much more complicated stateful transformations, like implementing your own aggregation function if you are not satisfied with Quokka's default operator's performance. This API is still being finalized. A version of it that takes multiple input streams is also going to be added. This is the part of the DataStream level api that is closest to the underlying execution engine. Quokka's underlying execution engine basically executes a series of stateful transformations on batches of data. The difficulty here is how much of that underlying API to expose here so it's still useful without the user having to understand how the Quokka runtime works. To that end, we have to come up with suitable partitioner and placement strategy abstraction classes and interfaces. If you are interested in helping us hammer out this API, please talke to me: zihengw@stanford.edu. Parameters: Name Type Description Default executor pyquokka . executors . Executor The stateful executor. It must be a subclass of pyquokka.executors.Executor , and expose the execute and done functions. More details forthcoming. required new_schema list The names of the columns of the Polars DataFrame that the transformation function produces. required required_columns list or set The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. required Return A transformed DataStream. Examples: Check the code for the gramian function. Source code in pyquokka/datastream.py 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 def stateful_transform ( self , executor : Executor , new_schema : list , required_columns : set , partitioner = PassThroughPartitioner (), placement_strategy = CustomChannelsStrategy ( 1 )): \"\"\" **EXPERIMENTAL API** This is like `transform`, except you can use a stateful object as your transformation function. This is useful for example, if you want to run a heavy Pytorch model on each batch coming in, and you don't want to reload this model for each function call. Remember the `transform` API only supports stateless transformations. You could also implement much more complicated stateful transformations, like implementing your own aggregation function if you are not satisfied with Quokka's default operator's performance. This API is still being finalized. A version of it that takes multiple input streams is also going to be added. This is the part of the DataStream level api that is closest to the underlying execution engine. Quokka's underlying execution engine basically executes a series of stateful transformations on batches of data. The difficulty here is how much of that underlying API to expose here so it's still useful without the user having to understand how the Quokka runtime works. To that end, we have to come up with suitable partitioner and placement strategy abstraction classes and interfaces. If you are interested in helping us hammer out this API, please talke to me: zihengw@stanford.edu. Args: executor (pyquokka.executors.Executor): The stateful executor. It must be a subclass of `pyquokka.executors.Executor`, and expose the `execute` and `done` functions. More details forthcoming. new_schema (list): The names of the columns of the Polars DataFrame that the transformation function produces. required_columns (list or set): The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. Return: A transformed DataStream. Examples: Check the code for the `gramian` function. \"\"\" assert type ( required_columns ) == set assert issubclass ( type ( executor ), Executor ), \"user defined executor must be an instance of a \\ child class of the Executor class defined in pyquokka.executors. You must override the execute and done methods.\" select_stream = self . select ( required_columns ) custom_node = StatefulNode ( schema = new_schema , # cannot push through any predicates or projections! schema_mapping = { col : { - 1 : col } for col in new_schema }, required_columns = { 0 : required_columns }, operator = executor ) custom_node . set_placement_strategy ( placement_strategy ) return self . quokka_context . new_stream ( sources = { 0 : select_stream }, partitioners = { 0 : partitioner }, node = custom_node , schema = new_schema , )","title":"DataStream.stateful_transform"},{"location":"datastream/sum/","text":"DataStream.sum Return the sums of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names to sum. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 def sum ( self , columns , collect = True ): \"\"\" Return the sums of the specified columns. Args: columns (str or list): the column name or a list of column names to sum. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"sum\" for col in columns }) . collect () else : return self . agg ({ col : \"sum\" for col in columns })","title":"DataStream.sum"},{"location":"datastream/sum/#datastreamsum","text":"Return the sums of the specified columns. Parameters: Name Type Description Default columns str or list the column name or a list of column names to sum. required collect bool if True, return a Polars DataFrame. If False, return a Quokka DataStream. True Source code in pyquokka/datastream.py 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 def sum ( self , columns , collect = True ): \"\"\" Return the sums of the specified columns. Args: columns (str or list): the column name or a list of column names to sum. collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream. \"\"\" assert type ( columns ) == str or type ( columns ) == list if type ( columns ) == str : columns = [ columns ] for col in columns : assert col in self . schema if collect : return self . agg ({ col : \"sum\" for col in columns }) . collect () else : return self . agg ({ col : \"sum\" for col in columns })","title":"DataStream.sum"},{"location":"datastream/top_k/","text":"DataStream.top_k This is a topk function that effectively performs select * from stream order by columns limit k. The strategy is to take k rows from each batch coming in and do a final sort and limit k in a stateful executor. Parameters: Name Type Description Default columns str or list a column or a list of columns to sort on. required k int the number of rows to return. required descending bool or list a boolean or a list of booleans indicating whether to sort in descending order. If a list, the length must be the same as the length of columns . None Return A DataStream object with the specified top k rows. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) result will be a DataStream. >>> result = lineitem . top_k ( \"l_orderkey\" , 10 ) >>> result = lineitem . top_k ([ \"l_orderkey\" , \"l_orderdate\" ], 10 , descending = [ True , False ]) Source code in pyquokka/datastream.py 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 def top_k ( self , columns , k , descending = None ): \"\"\" This is a topk function that effectively performs select * from stream order by columns limit k. The strategy is to take k rows from each batch coming in and do a final sort and limit k in a stateful executor. Args: columns (str or list): a column or a list of columns to sort on. k (int): the number of rows to return. descending (bool or list): a boolean or a list of booleans indicating whether to sort in descending order. If a list, the length must be the same as the length of `columns`. Return: A DataStream object with the specified top k rows. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") `result` will be a DataStream. >>> result = lineitem.top_k(\"l_orderkey\", 10) >>> result = lineitem.top_k([\"l_orderkey\", \"l_orderdate\"], 10, descending = [True, False]) \"\"\" if type ( columns ) == str : columns = [ columns ] assert type ( columns ) == list and len ( columns ) > 0 if descending is not None : if type ( descending ) == bool : descending = [ descending ] assert type ( descending ) == list and len ( descending ) == len ( columns ) assert all ([ type ( i ) == bool for i in descending ]) else : descending = [ False ] * len ( columns ) assert type ( k ) == int assert k > 0 new_columns = [] for i in range ( len ( columns )): if descending [ i ]: new_columns . append ( columns [ i ] + \" desc\" ) else : new_columns . append ( columns [ i ] + \" asc\" ) sql_statement = \"select * from batch_arrow order by \" + \",\" . join ( new_columns ) + \" limit \" + str ( k ) def f ( df ): batch_arrow = df . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) return polars . from_arrow ( con . execute ( sql_statement ) . arrow ()) transformed = self . transform ( f , new_schema = self . schema , required_columns = set ( self . schema )) topk_node = StatefulNode ( schema = self . schema , schema_mapping = { col : { 0 : col } for col in self . schema }, required_columns = { 0 : set ( columns )}, operator = ConcatThenSQLExecutor ( sql_statement ) ) topk_node . set_placement_strategy ( SingleChannelStrategy ()) return self . quokka_context . new_stream ( sources = { 0 : transformed }, partitioners = { 0 : BroadcastPartitioner ()}, node = topk_node , schema = self . schema , )","title":"DataStream.top_k"},{"location":"datastream/top_k/#datastreamtop_k","text":"This is a topk function that effectively performs select * from stream order by columns limit k. The strategy is to take k rows from each batch coming in and do a final sort and limit k in a stateful executor. Parameters: Name Type Description Default columns str or list a column or a list of columns to sort on. required k int the number of rows to return. required descending bool or list a boolean or a list of booleans indicating whether to sort in descending order. If a list, the length must be the same as the length of columns . None Return A DataStream object with the specified top k rows. Examples: >>> lineitem = qc . read_csv ( \"lineitem.csv\" ) result will be a DataStream. >>> result = lineitem . top_k ( \"l_orderkey\" , 10 ) >>> result = lineitem . top_k ([ \"l_orderkey\" , \"l_orderdate\" ], 10 , descending = [ True , False ]) Source code in pyquokka/datastream.py 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 def top_k ( self , columns , k , descending = None ): \"\"\" This is a topk function that effectively performs select * from stream order by columns limit k. The strategy is to take k rows from each batch coming in and do a final sort and limit k in a stateful executor. Args: columns (str or list): a column or a list of columns to sort on. k (int): the number of rows to return. descending (bool or list): a boolean or a list of booleans indicating whether to sort in descending order. If a list, the length must be the same as the length of `columns`. Return: A DataStream object with the specified top k rows. Examples: >>> lineitem = qc.read_csv(\"lineitem.csv\") `result` will be a DataStream. >>> result = lineitem.top_k(\"l_orderkey\", 10) >>> result = lineitem.top_k([\"l_orderkey\", \"l_orderdate\"], 10, descending = [True, False]) \"\"\" if type ( columns ) == str : columns = [ columns ] assert type ( columns ) == list and len ( columns ) > 0 if descending is not None : if type ( descending ) == bool : descending = [ descending ] assert type ( descending ) == list and len ( descending ) == len ( columns ) assert all ([ type ( i ) == bool for i in descending ]) else : descending = [ False ] * len ( columns ) assert type ( k ) == int assert k > 0 new_columns = [] for i in range ( len ( columns )): if descending [ i ]: new_columns . append ( columns [ i ] + \" desc\" ) else : new_columns . append ( columns [ i ] + \" asc\" ) sql_statement = \"select * from batch_arrow order by \" + \",\" . join ( new_columns ) + \" limit \" + str ( k ) def f ( df ): batch_arrow = df . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) return polars . from_arrow ( con . execute ( sql_statement ) . arrow ()) transformed = self . transform ( f , new_schema = self . schema , required_columns = set ( self . schema )) topk_node = StatefulNode ( schema = self . schema , schema_mapping = { col : { 0 : col } for col in self . schema }, required_columns = { 0 : set ( columns )}, operator = ConcatThenSQLExecutor ( sql_statement ) ) topk_node . set_placement_strategy ( SingleChannelStrategy ()) return self . quokka_context . new_stream ( sources = { 0 : transformed }, partitioners = { 0 : BroadcastPartitioner ()}, node = topk_node , schema = self . schema , )","title":"DataStream.top_k"},{"location":"datastream/transform/","text":"DataStream.transform This is a rather Quokka-specific API that allows arbitrary transformations on a DataStream, similar to Spark RDD.map. Each batch in the DataStream is going to be transformed according to a user defined function, which can produce a new batch. The new batch can have completely different schema or even length as the original batch, and the original data is considered lost, or consumed by this transformation function. This could be used to implement user-defined-aggregation-functions (UDAFs). Note in cases where you are simply generating a new column from other columns for each row, i.e. UDF, you probably want to use the with_columns method instead. A DataStream is implemented as a stream of batches. In the runtime, your transformation function will be applied to each of those batches. However, there are no guarantees whatsoever on the sizes of these batches! You should probably make sure your logic is correct regardless of the sizes of the batches. For example, if your DataStream consists of a column of numbers, and you wish to compute the sum of those numbers, you could first transform the DataStream to return just the sum of each batch, and then hook this DataStream up to a stateful operator that adds up all the sums. You can use whatever libraries you have installed in your Python environment in this transformation function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the utils package in pyquokka to help you do this, in particular, check out manager.install_python_package . Note a transformation in the logical plan basically precludes any predicate pushdown or early projection past it, since the original columns are assumed to be lost, and we cannot directly establish correspendences between the input columns to a transformation and its output columns for the purposes of predicate pushdown or early projection. The user is required to supply a set or list of required columns, and we will select for those columns (which can be pushed down) before we apply the transformation. Parameters: Name Type Description Default f function The transformation function. This transformation function must take as input a Polars DataFrame and output a Polars DataFrame. The transformation function must not have expectations on the length of its input. Similarly, the transformation function does not have to emit outputs of a specific size. The transformation function must produce the same output columns for every possible input. required new_schema list The names of the columns of the Polars DataFrame that the transformation function produces. required required_columns set The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. required foldable bool Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight functions generally benefit from being folded. Heavyweight functions or those whose efficiency improve with large input sizes might benefit from not being folded. True Return A new transformed DataStream with the supplied schema. Examples: Let's define a user defined function that takes in a Polars DataFrame with a single column \"text\", converts it to a Pyarrow table, and uses nice Pyarrow compute functions to perform the word count on this Polars DataFrame. Note 1) we have to convert it back to a Polars DataFrame afterwards, 2) the function works regardless of input length and 3) the output columns are the same regardless of the input. >>> def udf2 ( x ): >>> x = x . to_arrow () >>> da = compute . list_flatten ( compute . ascii_split_whitespace ( x [ \"text\" ])) >>> c = da . value_counts () . flatten () >>> return polars . from_arrow ( pa . Table . from_arrays ([ c [ 0 ], c [ 1 ]], names = [ \"word\" , \"count\" ])) This is a trick to read in text files, just use read_csv with a separator you know won't appear -- the result will just be DataStream with one column. >>> words = qc . read_csv ( \"random_words.txt\" , [ \"text\" ], sep = \"|\" ) Now transform words to counts. The result will be a DataStream with two columns, \"word\" and \"count\". >>> counted = words . transform ( udf2 , new_schema = [ \"word\" , \"count\" ], required_columns = { \"text\" }, foldable = True ) Source code in pyquokka/datastream.py 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 def transform ( self , f , new_schema : list , required_columns : set , foldable = True ): \"\"\" This is a rather Quokka-specific API that allows arbitrary transformations on a DataStream, similar to Spark RDD.map. Each batch in the DataStream is going to be transformed according to a user defined function, which can produce a new batch. The new batch can have completely different schema or even length as the original batch, and the original data is considered lost, or consumed by this transformation function. This could be used to implement user-defined-aggregation-functions (UDAFs). Note in cases where you are simply generating a new column from other columns for each row, i.e. UDF, you probably want to use the `with_columns` method instead. A DataStream is implemented as a stream of batches. In the runtime, your transformation function will be applied to each of those batches. However, there are no guarantees whatsoever on the sizes of these batches! You should probably make sure your logic is correct regardless of the sizes of the batches. For example, if your DataStream consists of a column of numbers, and you wish to compute the sum of those numbers, you could first transform the DataStream to return just the sum of each batch, and then hook this DataStream up to a stateful operator that adds up all the sums. You can use whatever libraries you have installed in your Python environment in this transformation function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the `utils` package in pyquokka to help you do this, in particular, check out `manager.install_python_package`. Note a transformation in the logical plan basically precludes any predicate pushdown or early projection past it, since the original columns are assumed to be lost, and we cannot directly establish correspendences between the input columns to a transformation and its output columns for the purposes of predicate pushdown or early projection. The user is required to supply a set or list of required columns, and we will select for those columns (which can be pushed down) before we apply the transformation. Args: f (function): The transformation function. This transformation function must take as input a Polars DataFrame and output a Polars DataFrame. The transformation function must not have expectations on the length of its input. Similarly, the transformation function does not have to emit outputs of a specific size. The transformation function must produce the same output columns for every possible input. new_schema (list): The names of the columns of the Polars DataFrame that the transformation function produces. required_columns (set): The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. foldable (bool): Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight functions generally benefit from being folded. Heavyweight functions or those whose efficiency improve with large input sizes might benefit from not being folded. Return: A new transformed DataStream with the supplied schema. Examples: Let's define a user defined function that takes in a Polars DataFrame with a single column \"text\", converts it to a Pyarrow table, and uses nice Pyarrow compute functions to perform the word count on this Polars DataFrame. Note 1) we have to convert it back to a Polars DataFrame afterwards, 2) the function works regardless of input length and 3) the output columns are the same regardless of the input. >>> def udf2(x): >>> x = x.to_arrow() >>> da = compute.list_flatten(compute.ascii_split_whitespace(x[\"text\"])) >>> c = da.value_counts().flatten() >>> return polars.from_arrow(pa.Table.from_arrays([c[0], c[1]], names=[\"word\",\"count\"])) This is a trick to read in text files, just use read_csv with a separator you know won't appear -- the result will just be DataStream with one column. >>> words = qc.read_csv(\"random_words.txt\", [\"text\"], sep = \"|\") Now transform words to counts. The result will be a DataStream with two columns, \"word\" and \"count\". >>> counted = words.transform( udf2, new_schema = [\"word\", \"count\"], required_columns = {\"text\"}, foldable=True) \"\"\" if type ( required_columns ) == list : required_columns = set ( required_columns ) assert type ( required_columns ) == set if self . materialized : df = self . _get_materialized_df () df = f ( df ) return self . quokka_context . from_polars ( df ) select_stream = self . select ( required_columns ) return self . quokka_context . new_stream ( sources = { 0 : select_stream }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = new_schema , schema_mapping = { col : { - 1 : col } for col in new_schema }, required_columns = { 0 : required_columns }, function = f , foldable = foldable ), schema = new_schema , )","title":"DataStream.transform"},{"location":"datastream/transform/#datastreamtransform","text":"This is a rather Quokka-specific API that allows arbitrary transformations on a DataStream, similar to Spark RDD.map. Each batch in the DataStream is going to be transformed according to a user defined function, which can produce a new batch. The new batch can have completely different schema or even length as the original batch, and the original data is considered lost, or consumed by this transformation function. This could be used to implement user-defined-aggregation-functions (UDAFs). Note in cases where you are simply generating a new column from other columns for each row, i.e. UDF, you probably want to use the with_columns method instead. A DataStream is implemented as a stream of batches. In the runtime, your transformation function will be applied to each of those batches. However, there are no guarantees whatsoever on the sizes of these batches! You should probably make sure your logic is correct regardless of the sizes of the batches. For example, if your DataStream consists of a column of numbers, and you wish to compute the sum of those numbers, you could first transform the DataStream to return just the sum of each batch, and then hook this DataStream up to a stateful operator that adds up all the sums. You can use whatever libraries you have installed in your Python environment in this transformation function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the utils package in pyquokka to help you do this, in particular, check out manager.install_python_package . Note a transformation in the logical plan basically precludes any predicate pushdown or early projection past it, since the original columns are assumed to be lost, and we cannot directly establish correspendences between the input columns to a transformation and its output columns for the purposes of predicate pushdown or early projection. The user is required to supply a set or list of required columns, and we will select for those columns (which can be pushed down) before we apply the transformation. Parameters: Name Type Description Default f function The transformation function. This transformation function must take as input a Polars DataFrame and output a Polars DataFrame. The transformation function must not have expectations on the length of its input. Similarly, the transformation function does not have to emit outputs of a specific size. The transformation function must produce the same output columns for every possible input. required new_schema list The names of the columns of the Polars DataFrame that the transformation function produces. required required_columns set The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. required foldable bool Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight functions generally benefit from being folded. Heavyweight functions or those whose efficiency improve with large input sizes might benefit from not being folded. True Return A new transformed DataStream with the supplied schema. Examples: Let's define a user defined function that takes in a Polars DataFrame with a single column \"text\", converts it to a Pyarrow table, and uses nice Pyarrow compute functions to perform the word count on this Polars DataFrame. Note 1) we have to convert it back to a Polars DataFrame afterwards, 2) the function works regardless of input length and 3) the output columns are the same regardless of the input. >>> def udf2 ( x ): >>> x = x . to_arrow () >>> da = compute . list_flatten ( compute . ascii_split_whitespace ( x [ \"text\" ])) >>> c = da . value_counts () . flatten () >>> return polars . from_arrow ( pa . Table . from_arrays ([ c [ 0 ], c [ 1 ]], names = [ \"word\" , \"count\" ])) This is a trick to read in text files, just use read_csv with a separator you know won't appear -- the result will just be DataStream with one column. >>> words = qc . read_csv ( \"random_words.txt\" , [ \"text\" ], sep = \"|\" ) Now transform words to counts. The result will be a DataStream with two columns, \"word\" and \"count\". >>> counted = words . transform ( udf2 , new_schema = [ \"word\" , \"count\" ], required_columns = { \"text\" }, foldable = True ) Source code in pyquokka/datastream.py 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 def transform ( self , f , new_schema : list , required_columns : set , foldable = True ): \"\"\" This is a rather Quokka-specific API that allows arbitrary transformations on a DataStream, similar to Spark RDD.map. Each batch in the DataStream is going to be transformed according to a user defined function, which can produce a new batch. The new batch can have completely different schema or even length as the original batch, and the original data is considered lost, or consumed by this transformation function. This could be used to implement user-defined-aggregation-functions (UDAFs). Note in cases where you are simply generating a new column from other columns for each row, i.e. UDF, you probably want to use the `with_columns` method instead. A DataStream is implemented as a stream of batches. In the runtime, your transformation function will be applied to each of those batches. However, there are no guarantees whatsoever on the sizes of these batches! You should probably make sure your logic is correct regardless of the sizes of the batches. For example, if your DataStream consists of a column of numbers, and you wish to compute the sum of those numbers, you could first transform the DataStream to return just the sum of each batch, and then hook this DataStream up to a stateful operator that adds up all the sums. You can use whatever libraries you have installed in your Python environment in this transformation function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the `utils` package in pyquokka to help you do this, in particular, check out `manager.install_python_package`. Note a transformation in the logical plan basically precludes any predicate pushdown or early projection past it, since the original columns are assumed to be lost, and we cannot directly establish correspendences between the input columns to a transformation and its output columns for the purposes of predicate pushdown or early projection. The user is required to supply a set or list of required columns, and we will select for those columns (which can be pushed down) before we apply the transformation. Args: f (function): The transformation function. This transformation function must take as input a Polars DataFrame and output a Polars DataFrame. The transformation function must not have expectations on the length of its input. Similarly, the transformation function does not have to emit outputs of a specific size. The transformation function must produce the same output columns for every possible input. new_schema (list): The names of the columns of the Polars DataFrame that the transformation function produces. required_columns (set): The names of the columns that are required for this transformation. This argument is made mandatory because it's often trivial to supply and can often greatly speed things up. foldable (bool): Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight functions generally benefit from being folded. Heavyweight functions or those whose efficiency improve with large input sizes might benefit from not being folded. Return: A new transformed DataStream with the supplied schema. Examples: Let's define a user defined function that takes in a Polars DataFrame with a single column \"text\", converts it to a Pyarrow table, and uses nice Pyarrow compute functions to perform the word count on this Polars DataFrame. Note 1) we have to convert it back to a Polars DataFrame afterwards, 2) the function works regardless of input length and 3) the output columns are the same regardless of the input. >>> def udf2(x): >>> x = x.to_arrow() >>> da = compute.list_flatten(compute.ascii_split_whitespace(x[\"text\"])) >>> c = da.value_counts().flatten() >>> return polars.from_arrow(pa.Table.from_arrays([c[0], c[1]], names=[\"word\",\"count\"])) This is a trick to read in text files, just use read_csv with a separator you know won't appear -- the result will just be DataStream with one column. >>> words = qc.read_csv(\"random_words.txt\", [\"text\"], sep = \"|\") Now transform words to counts. The result will be a DataStream with two columns, \"word\" and \"count\". >>> counted = words.transform( udf2, new_schema = [\"word\", \"count\"], required_columns = {\"text\"}, foldable=True) \"\"\" if type ( required_columns ) == list : required_columns = set ( required_columns ) assert type ( required_columns ) == set if self . materialized : df = self . _get_materialized_df () df = f ( df ) return self . quokka_context . from_polars ( df ) select_stream = self . select ( required_columns ) return self . quokka_context . new_stream ( sources = { 0 : select_stream }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = new_schema , schema_mapping = { col : { - 1 : col } for col in new_schema }, required_columns = { 0 : required_columns }, function = f , foldable = foldable ), schema = new_schema , )","title":"DataStream.transform"},{"location":"datastream/transform_sql/","text":"DataStream.transform_sql This is a SQL version of the transform method. It allows you to write SQL expressions that can be applied to each batch in the DataStream. This is the X in select X from Y . The Y is the DataStream. The X can be any SQL expression, including aggregation functions. Example sql expression \" sum(l_quantity) as sum_qty, sum(l_extendedprice) as sum_base_price, sum(l_extendedprice * (1 - l_discount)) as sum_disc_price, sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge\" You must supply an alias for each transformation. Any syntax that is supported by DuckDB can be used. You can optionally also specify groupby columns. This will apply the SQL expression select X from Y group by Z to each batch in the DataStream. Parameters: Name Type Description Default sql_expression str The SQL expression to apply to each batch in the DataStream. required groupby list The list of columns to group by. [] foldable bool Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Probably should be True. True Source code in pyquokka/datastream.py 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 def transform_sql ( self , sql_expression , groupby = [], foldable = True ): \"\"\" This is a SQL version of the `transform` method. It allows you to write SQL expressions that can be applied to each batch in the DataStream. This is the X in `select X from Y`. The Y is the DataStream. The X can be any SQL expression, including aggregation functions. Example sql expression: \" sum(l_quantity) as sum_qty, sum(l_extendedprice) as sum_base_price, sum(l_extendedprice * (1 - l_discount)) as sum_disc_price, sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge\" You must supply an alias for each transformation. Any syntax that is supported by DuckDB can be used. You can optionally also specify groupby columns. This will apply the SQL expression `select X from Y group by Z` to each batch in the DataStream. Args: sql_expression (str): The SQL expression to apply to each batch in the DataStream. groupby (list): The list of columns to group by. foldable (bool): Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Probably should be True. \"\"\" assert type ( groupby ) == list enhanced_exp = \"select \" + \",\" . join ( groupby ) + \", \" + sql_expression + \" from batch_arrow\" if len ( groupby ) > 0 : enhanced_exp = enhanced_exp + \" group by \" + \",\" . join ( groupby ) enhanced_exp = label_sample_table_names ( sqlglot . parse_one ( enhanced_exp ), 'batch_arrow' ) . sql () sqlglot_node = sqlglot . parse_one ( enhanced_exp ) required_columns = required_columns_from_exp ( sqlglot_node ) if len ( required_columns ) == 0 : # TODO: this is to make sure count works by picking a random column to download. required_columns = { self . schema [ 1 ]} for col in required_columns : assert col in self . schema , \"required column %s not in schema\" % col new_columns = [ i . alias for i in sqlglot_node . selects if i . name not in groupby ] assert '' not in new_columns , \"must provide alias for each computation\" assert type ( required_columns ) == set for column in new_columns : assert column not in self . schema , \"For now new columns cannot have same names as existing columns\" if self . materialized : batch_arrow = self . _get_materialized_df () . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % multiprocessing . cpu_count ()) df = polars . from_arrow ( con . execute ( enhanced_exp ) . arrow ()) return self . quokka_context . from_polars ( df ) def duckdb_func ( func , batch ): batch_arrow = batch . to_arrow () for i , ( col_name , type_ ) in enumerate ( zip ( batch_arrow . schema . names , batch_arrow . schema . types )): if pa . types . is_boolean ( type_ ): batch_arrow = batch_arrow . set_column ( i , col_name , compute . cast ( batch_arrow . column ( col_name ), pa . int32 ())) con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % multiprocessing . cpu_count ()) return polars . from_arrow ( con . execute ( func ) . arrow ()) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = groupby + new_columns , schema_mapping = { ** { new_column : { - 1 : new_column } for new_column in new_columns }, ** { col : { 0 : col } for col in self . schema }}, required_columns = { 0 : required_columns }, function = partial ( duckdb_func , enhanced_exp ), foldable = foldable ), schema = groupby + new_columns , sorted = self . sorted )","title":"DataStream.transform_sql"},{"location":"datastream/transform_sql/#datastreamtransform_sql","text":"This is a SQL version of the transform method. It allows you to write SQL expressions that can be applied to each batch in the DataStream. This is the X in select X from Y . The Y is the DataStream. The X can be any SQL expression, including aggregation functions. Example sql expression \" sum(l_quantity) as sum_qty, sum(l_extendedprice) as sum_base_price, sum(l_extendedprice * (1 - l_discount)) as sum_disc_price, sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge\" You must supply an alias for each transformation. Any syntax that is supported by DuckDB can be used. You can optionally also specify groupby columns. This will apply the SQL expression select X from Y group by Z to each batch in the DataStream. Parameters: Name Type Description Default sql_expression str The SQL expression to apply to each batch in the DataStream. required groupby list The list of columns to group by. [] foldable bool Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Probably should be True. True Source code in pyquokka/datastream.py 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 def transform_sql ( self , sql_expression , groupby = [], foldable = True ): \"\"\" This is a SQL version of the `transform` method. It allows you to write SQL expressions that can be applied to each batch in the DataStream. This is the X in `select X from Y`. The Y is the DataStream. The X can be any SQL expression, including aggregation functions. Example sql expression: \" sum(l_quantity) as sum_qty, sum(l_extendedprice) as sum_base_price, sum(l_extendedprice * (1 - l_discount)) as sum_disc_price, sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge\" You must supply an alias for each transformation. Any syntax that is supported by DuckDB can be used. You can optionally also specify groupby columns. This will apply the SQL expression `select X from Y group by Z` to each batch in the DataStream. Args: sql_expression (str): The SQL expression to apply to each batch in the DataStream. groupby (list): The list of columns to group by. foldable (bool): Whether or not the transformation can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Probably should be True. \"\"\" assert type ( groupby ) == list enhanced_exp = \"select \" + \",\" . join ( groupby ) + \", \" + sql_expression + \" from batch_arrow\" if len ( groupby ) > 0 : enhanced_exp = enhanced_exp + \" group by \" + \",\" . join ( groupby ) enhanced_exp = label_sample_table_names ( sqlglot . parse_one ( enhanced_exp ), 'batch_arrow' ) . sql () sqlglot_node = sqlglot . parse_one ( enhanced_exp ) required_columns = required_columns_from_exp ( sqlglot_node ) if len ( required_columns ) == 0 : # TODO: this is to make sure count works by picking a random column to download. required_columns = { self . schema [ 1 ]} for col in required_columns : assert col in self . schema , \"required column %s not in schema\" % col new_columns = [ i . alias for i in sqlglot_node . selects if i . name not in groupby ] assert '' not in new_columns , \"must provide alias for each computation\" assert type ( required_columns ) == set for column in new_columns : assert column not in self . schema , \"For now new columns cannot have same names as existing columns\" if self . materialized : batch_arrow = self . _get_materialized_df () . to_arrow () con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % multiprocessing . cpu_count ()) df = polars . from_arrow ( con . execute ( enhanced_exp ) . arrow ()) return self . quokka_context . from_polars ( df ) def duckdb_func ( func , batch ): batch_arrow = batch . to_arrow () for i , ( col_name , type_ ) in enumerate ( zip ( batch_arrow . schema . names , batch_arrow . schema . types )): if pa . types . is_boolean ( type_ ): batch_arrow = batch_arrow . set_column ( i , col_name , compute . cast ( batch_arrow . column ( col_name ), pa . int32 ())) con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % multiprocessing . cpu_count ()) return polars . from_arrow ( con . execute ( func ) . arrow ()) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = groupby + new_columns , schema_mapping = { ** { new_column : { - 1 : new_column } for new_column in new_columns }, ** { col : { 0 : col } for col in self . schema }}, required_columns = { 0 : required_columns }, function = partial ( duckdb_func , enhanced_exp ), foldable = foldable ), schema = groupby + new_columns , sorted = self . sorted )","title":"DataStream.transform_sql"},{"location":"datastream/union/","text":"DataStream.union The union of two streams is a stream that contains all the elements of both streams. The two streams must have the same schema. Note: since DataStreams are not ordered, you should not make any assumptions on the ordering of the rows in the result. Parameters: Name Type Description Default other DataStream another DataStream of the same schema. required Return A new DataStream of the same schema as the two streams, with rows from both. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) >>> d1 = qc . read_csv ( \"lineitem1.csv\" ) >>> d1 = d . union ( d1 ) Source code in pyquokka/datastream.py 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 def union ( self , other ): \"\"\" The union of two streams is a stream that contains all the elements of both streams. The two streams must have the same schema. Note: since DataStreams are not ordered, you should not make any assumptions on the ordering of the rows in the result. Args: other (DataStream): another DataStream of the same schema. Return: A new DataStream of the same schema as the two streams, with rows from both. Examples: >>> d = qc.read_csv(\"lineitem.csv\") >>> d1 = qc.read_csv(\"lineitem1.csv\") >>> d1 = d.union(d1) \"\"\" assert self . schema == other . schema assert self . sorted == other . sorted class UnionExecutor ( Executor ): def __init__ ( self ) -> None : self . state = None def execute ( self , batches , stream_id , executor_id ): return pa . concat_tables ( batches ) def done ( self , executor_id ): return executor = UnionExecutor () node = StatefulNode ( schema = self . schema , # cannot push through any predicates or projections! schema_mapping = { col : { 0 : col , 1 : col } for col in self . schema }, required_columns = { 0 : set (), 1 : set ()}, operator = executor ) return self . quokka_context . new_stream ( sources = { 0 : self , 1 : other }, partitioners = { 0 : PassThroughPartitioner (), 1 : PassThroughPartitioner ()}, node = node , schema = self . schema , sorted = None )","title":"DataStream.union"},{"location":"datastream/union/#datastreamunion","text":"The union of two streams is a stream that contains all the elements of both streams. The two streams must have the same schema. Note: since DataStreams are not ordered, you should not make any assumptions on the ordering of the rows in the result. Parameters: Name Type Description Default other DataStream another DataStream of the same schema. required Return A new DataStream of the same schema as the two streams, with rows from both. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) >>> d1 = qc . read_csv ( \"lineitem1.csv\" ) >>> d1 = d . union ( d1 ) Source code in pyquokka/datastream.py 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 def union ( self , other ): \"\"\" The union of two streams is a stream that contains all the elements of both streams. The two streams must have the same schema. Note: since DataStreams are not ordered, you should not make any assumptions on the ordering of the rows in the result. Args: other (DataStream): another DataStream of the same schema. Return: A new DataStream of the same schema as the two streams, with rows from both. Examples: >>> d = qc.read_csv(\"lineitem.csv\") >>> d1 = qc.read_csv(\"lineitem1.csv\") >>> d1 = d.union(d1) \"\"\" assert self . schema == other . schema assert self . sorted == other . sorted class UnionExecutor ( Executor ): def __init__ ( self ) -> None : self . state = None def execute ( self , batches , stream_id , executor_id ): return pa . concat_tables ( batches ) def done ( self , executor_id ): return executor = UnionExecutor () node = StatefulNode ( schema = self . schema , # cannot push through any predicates or projections! schema_mapping = { col : { 0 : col , 1 : col } for col in self . schema }, required_columns = { 0 : set (), 1 : set ()}, operator = executor ) return self . quokka_context . new_stream ( sources = { 0 : self , 1 : other }, partitioners = { 0 : PassThroughPartitioner (), 1 : PassThroughPartitioner ()}, node = node , schema = self . schema , sorted = None )","title":"DataStream.union"},{"location":"datastream/with_columns/","text":"DataStream.with_columns This will create new columns from certain columns in the dataframe. This is similar to Polars with_columns , Spark with_columns , etc. As usual, this function is not in-place, and will return a new DataStream, with the new column. This is a separate API from transform because the semantics allow for projection and predicate pushdown through this node, since the original columns are all preserved. Use this instead of transform if possible. The arguments are a bit different from Polars with_columns . You need to specify a dictionary where key is new column name and value is either a Quokka Expression or a Python function (lambda function or regular function). I think this is better than the Polars way and removes the possibility of having column names colliding. The preferred way is to supply Quokka Expressions for things that the Expression syntax supports. In case a function is supplied, it must assume a single input, which is a Polars DataFrame. Please look at the examples. A DataStream is implemented as a stream of batches. In the runtime, your function will be applied to each of those batches. The function must take as input a Polars DataFrame and produce a Polars DataFrame. This is a different mental model from Pandas or Polars df.apply , where the function is written for each row. The function's output must be a Polars Series (or DataFrame with one column)! Of course you can call Polars or Pandas apply inside of this function if you have to do things row by row. You can use whatever libraries you have installed in your Python environment in this function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the utils package in pyquokka to help you do this, in particular you can use mananger.install_python_package(cluster, package_name) . Importantly, your function can take full advantage of Polars' columnar APIs to make use of SIMD and other forms of speedy goodness. You can even use Polars LazyFrame abstractions inside of this function. Of course, for ultimate flexbility, you are more than welcome to convert the Polars DataFrame to a Pandas DataFrame and use df.apply . Just remember to convert it back to a Polars DataFrame with only the result column in the end! Parameters: Name Type Description Default new_columns dict A dictionary of column names to UDFs or Expressions. The UDFs must take as input a Polars DataFrame and output a Polars DataFrame. The UDFs must not have expectations on the length of its input. required required_columns list or set The names of the columns that are required for all your function. If this is not specified then Quokka assumes all the columns are required for your function. Early projection past this function becomes impossible. If you specify this and you got it wrong, you will get an error at runtime. This is only required for UDFs. If you use Quokka Expressions, then Quokka will automatically figure out the required columns. set() foldable bool Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight True Return A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) You can use Polars APIs inside of custom lambda functions: >>> d = d . with_columns ({ \"high\" : lambda x :( x [ \"o_orderpriority\" ] == \"1-URGENT\" ) | ( x [ \"o_orderpriority\" ] == \"2-HIGH\" ), \"low\" : lambda x :( x [ \"o_orderpriority\" ] == \"5-LOW\" ) | ( x [ \"o_orderpriority\" ] == \"4-NOT SPECIFIED\" )}, required_columns = { \"o_orderpriority\" }) You can also use Quokka Expressions. You don't need to specify required columns if you use only Quokka Expressions: >>> d = d . with_columns ({ \"high\" : ( d [ \"o_orderpriority\" ] == \"1-URGENT\" ) | ( d [ \"o_orderpriority\" ] == \"2-HIGH\" ), \"low\" : ( d [ \"o_orderpriority\" ] == \"5-LOW\" ) | ( d [ \"o_orderpriority\" ] == \"4-NOT SPECIFIED\" )}) Or mix the two. You then have to specify required columns again. It is the set of columns required for all your functions. >>> d = d . with_columns ({ \"high\" : ( lambda x :( x [ \"o_orderpriority\" ] == \"1-URGENT\" ) | ( x [ \"o_orderpriority\" ] == \"2-HIGH\" ), \"low\" : ( d [ \"o_orderpriority\" ] == \"5-LOW\" ) | ( d [ \"o_orderpriority\" ] == \"4-NOT SPECIFIED\" )}, required_columns = { \"o_orderpriority\" }) Source code in pyquokka/datastream.py 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 def with_columns ( self , new_columns : dict , required_columns = set (), foldable = True ): \"\"\" This will create new columns from certain columns in the dataframe. This is similar to Polars `with_columns`, Spark `with_columns`, etc. As usual, this function is not in-place, and will return a new DataStream, with the new column. This is a separate API from `transform` because the semantics allow for projection and predicate pushdown through this node, since the original columns are all preserved. Use this instead of `transform` if possible. The arguments are a bit different from Polars `with_columns`. You need to specify a dictionary where key is new column name and value is either a Quokka Expression or a Python function (lambda function or regular function). I think this is better than the Polars way and removes the possibility of having column names colliding. The preferred way is to supply Quokka Expressions for things that the Expression syntax supports. In case a function is supplied, it must assume a single input, which is a Polars DataFrame. Please look at the examples. A DataStream is implemented as a stream of batches. In the runtime, your function will be applied to each of those batches. The function must take as input a Polars DataFrame and produce a Polars DataFrame. This is a different mental model from Pandas or Polars `df.apply`, where the function is written for each row. **The function's output must be a Polars Series (or DataFrame with one column)! ** Of course you can call Polars or Pandas apply inside of this function if you have to do things row by row. You can use whatever libraries you have installed in your Python environment in this function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the `utils` package in pyquokka to help you do this, in particular you can use `mananger.install_python_package(cluster, package_name)`. Importantly, your function can take full advantage of Polars' columnar APIs to make use of SIMD and other forms of speedy goodness. You can even use Polars LazyFrame abstractions inside of this function. Of course, for ultimate flexbility, you are more than welcome to convert the Polars DataFrame to a Pandas DataFrame and use `df.apply`. Just remember to convert it back to a Polars DataFrame with only the result column in the end! Args: new_columns (dict): A dictionary of column names to UDFs or Expressions. The UDFs must take as input a Polars DataFrame and output a Polars DataFrame. The UDFs must not have expectations on the length of its input. required_columns (list or set): The names of the columns that are required for all your function. If this is not specified then Quokka assumes all the columns are required for your function. Early projection past this function becomes impossible. If you specify this and you got it wrong, you will get an error at runtime. This is only required for UDFs. If you use Quokka Expressions, then Quokka will automatically figure out the required columns. foldable (bool): Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight Return: A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc.read_csv(\"lineitem.csv\") You can use Polars APIs inside of custom lambda functions: >>> d = d.with_columns({\"high\": lambda x:(x[\"o_orderpriority\"] == \"1-URGENT\") | (x[\"o_orderpriority\"] == \"2-HIGH\"), \"low\": lambda x:(x[\"o_orderpriority\"] == \"5-LOW\") | (x[\"o_orderpriority\"] == \"4-NOT SPECIFIED\")}, required_columns = {\"o_orderpriority\"}) You can also use Quokka Expressions. You don't need to specify required columns if you use only Quokka Expressions: >>> d = d.with_columns({\"high\": (d[\"o_orderpriority\"] == \"1-URGENT\") | (d[\"o_orderpriority\"] == \"2-HIGH\"), \"low\": (d[\"o_orderpriority\"] == \"5-LOW\") | (d[\"o_orderpriority\"] == \"4-NOT SPECIFIED\")}) Or mix the two. You then have to specify required columns again. It is the set of columns required for *all* your functions. >>> d = d.with_columns({\"high\": (lambda x:(x[\"o_orderpriority\"] == \"1-URGENT\") | (x[\"o_orderpriority\"] == \"2-HIGH\"), \"low\": (d[\"o_orderpriority\"] == \"5-LOW\") | (d[\"o_orderpriority\"] == \"4-NOT SPECIFIED\")}, required_columns = {\"o_orderpriority\"}) \"\"\" assert type ( required_columns ) == set # fix the new column ordering new_column_names = list ( new_columns . keys ()) sql_statement = \"select *\" for new_column in new_columns : assert new_column not in self . schema , \"For now new columns cannot have same names as existing columns\" transform = new_columns [ new_column ] assert type ( transform ) == type ( lambda x : 1 ) or type ( transform ) == Expression , \"Transform must be a function or a Quokka Expression\" if type ( transform ) == Expression : required_columns = required_columns . union ( transform . required_columns ()) sql_statement += \", \" + transform . sql () + \" as \" + new_column else : # detected an arbitrary function. If required columns are not specified, assume all columns are required if len ( required_columns ) == 0 : required_columns = set ( self . schema ) def polars_func ( batch ): con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) if sql_statement != \"select *\" : # if there are any columns to add batch_arrow = batch . to_arrow () batch = polars . from_arrow ( con . execute ( sql_statement + \" from batch_arrow\" ) . arrow ()) batch = batch . with_columns ([ polars . Series ( name = column_name , values = new_columns [ column_name ]( batch )) for column_name in new_column_names if type ( new_columns [ column_name ]) == type ( lambda x : 1 )]) return batch return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = self . schema + new_column_names , schema_mapping = { ** { new_column : { - 1 : new_column } for new_column in new_column_names }, ** { col : { 0 : col } for col in self . schema }}, required_columns = { 0 : required_columns }, function = polars_func , foldable = foldable ), schema = self . schema + new_column_names , sorted = self . sorted )","title":"DataStream.with_columns"},{"location":"datastream/with_columns/#datastreamwith_columns","text":"This will create new columns from certain columns in the dataframe. This is similar to Polars with_columns , Spark with_columns , etc. As usual, this function is not in-place, and will return a new DataStream, with the new column. This is a separate API from transform because the semantics allow for projection and predicate pushdown through this node, since the original columns are all preserved. Use this instead of transform if possible. The arguments are a bit different from Polars with_columns . You need to specify a dictionary where key is new column name and value is either a Quokka Expression or a Python function (lambda function or regular function). I think this is better than the Polars way and removes the possibility of having column names colliding. The preferred way is to supply Quokka Expressions for things that the Expression syntax supports. In case a function is supplied, it must assume a single input, which is a Polars DataFrame. Please look at the examples. A DataStream is implemented as a stream of batches. In the runtime, your function will be applied to each of those batches. The function must take as input a Polars DataFrame and produce a Polars DataFrame. This is a different mental model from Pandas or Polars df.apply , where the function is written for each row. The function's output must be a Polars Series (or DataFrame with one column)! Of course you can call Polars or Pandas apply inside of this function if you have to do things row by row. You can use whatever libraries you have installed in your Python environment in this function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the utils package in pyquokka to help you do this, in particular you can use mananger.install_python_package(cluster, package_name) . Importantly, your function can take full advantage of Polars' columnar APIs to make use of SIMD and other forms of speedy goodness. You can even use Polars LazyFrame abstractions inside of this function. Of course, for ultimate flexbility, you are more than welcome to convert the Polars DataFrame to a Pandas DataFrame and use df.apply . Just remember to convert it back to a Polars DataFrame with only the result column in the end! Parameters: Name Type Description Default new_columns dict A dictionary of column names to UDFs or Expressions. The UDFs must take as input a Polars DataFrame and output a Polars DataFrame. The UDFs must not have expectations on the length of its input. required required_columns list or set The names of the columns that are required for all your function. If this is not specified then Quokka assumes all the columns are required for your function. Early projection past this function becomes impossible. If you specify this and you got it wrong, you will get an error at runtime. This is only required for UDFs. If you use Quokka Expressions, then Quokka will automatically figure out the required columns. set() foldable bool Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight True Return A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) You can use Polars APIs inside of custom lambda functions: >>> d = d . with_columns ({ \"high\" : lambda x :( x [ \"o_orderpriority\" ] == \"1-URGENT\" ) | ( x [ \"o_orderpriority\" ] == \"2-HIGH\" ), \"low\" : lambda x :( x [ \"o_orderpriority\" ] == \"5-LOW\" ) | ( x [ \"o_orderpriority\" ] == \"4-NOT SPECIFIED\" )}, required_columns = { \"o_orderpriority\" }) You can also use Quokka Expressions. You don't need to specify required columns if you use only Quokka Expressions: >>> d = d . with_columns ({ \"high\" : ( d [ \"o_orderpriority\" ] == \"1-URGENT\" ) | ( d [ \"o_orderpriority\" ] == \"2-HIGH\" ), \"low\" : ( d [ \"o_orderpriority\" ] == \"5-LOW\" ) | ( d [ \"o_orderpriority\" ] == \"4-NOT SPECIFIED\" )}) Or mix the two. You then have to specify required columns again. It is the set of columns required for all your functions. >>> d = d . with_columns ({ \"high\" : ( lambda x :( x [ \"o_orderpriority\" ] == \"1-URGENT\" ) | ( x [ \"o_orderpriority\" ] == \"2-HIGH\" ), \"low\" : ( d [ \"o_orderpriority\" ] == \"5-LOW\" ) | ( d [ \"o_orderpriority\" ] == \"4-NOT SPECIFIED\" )}, required_columns = { \"o_orderpriority\" }) Source code in pyquokka/datastream.py 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 def with_columns ( self , new_columns : dict , required_columns = set (), foldable = True ): \"\"\" This will create new columns from certain columns in the dataframe. This is similar to Polars `with_columns`, Spark `with_columns`, etc. As usual, this function is not in-place, and will return a new DataStream, with the new column. This is a separate API from `transform` because the semantics allow for projection and predicate pushdown through this node, since the original columns are all preserved. Use this instead of `transform` if possible. The arguments are a bit different from Polars `with_columns`. You need to specify a dictionary where key is new column name and value is either a Quokka Expression or a Python function (lambda function or regular function). I think this is better than the Polars way and removes the possibility of having column names colliding. The preferred way is to supply Quokka Expressions for things that the Expression syntax supports. In case a function is supplied, it must assume a single input, which is a Polars DataFrame. Please look at the examples. A DataStream is implemented as a stream of batches. In the runtime, your function will be applied to each of those batches. The function must take as input a Polars DataFrame and produce a Polars DataFrame. This is a different mental model from Pandas or Polars `df.apply`, where the function is written for each row. **The function's output must be a Polars Series (or DataFrame with one column)! ** Of course you can call Polars or Pandas apply inside of this function if you have to do things row by row. You can use whatever libraries you have installed in your Python environment in this function. If you are using this on a cloud cluster, you have to make sure the necessary libraries are installed on each machine. You can use the `utils` package in pyquokka to help you do this, in particular you can use `mananger.install_python_package(cluster, package_name)`. Importantly, your function can take full advantage of Polars' columnar APIs to make use of SIMD and other forms of speedy goodness. You can even use Polars LazyFrame abstractions inside of this function. Of course, for ultimate flexbility, you are more than welcome to convert the Polars DataFrame to a Pandas DataFrame and use `df.apply`. Just remember to convert it back to a Polars DataFrame with only the result column in the end! Args: new_columns (dict): A dictionary of column names to UDFs or Expressions. The UDFs must take as input a Polars DataFrame and output a Polars DataFrame. The UDFs must not have expectations on the length of its input. required_columns (list or set): The names of the columns that are required for all your function. If this is not specified then Quokka assumes all the columns are required for your function. Early projection past this function becomes impossible. If you specify this and you got it wrong, you will get an error at runtime. This is only required for UDFs. If you use Quokka Expressions, then Quokka will automatically figure out the required columns. foldable (bool): Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight Return: A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc.read_csv(\"lineitem.csv\") You can use Polars APIs inside of custom lambda functions: >>> d = d.with_columns({\"high\": lambda x:(x[\"o_orderpriority\"] == \"1-URGENT\") | (x[\"o_orderpriority\"] == \"2-HIGH\"), \"low\": lambda x:(x[\"o_orderpriority\"] == \"5-LOW\") | (x[\"o_orderpriority\"] == \"4-NOT SPECIFIED\")}, required_columns = {\"o_orderpriority\"}) You can also use Quokka Expressions. You don't need to specify required columns if you use only Quokka Expressions: >>> d = d.with_columns({\"high\": (d[\"o_orderpriority\"] == \"1-URGENT\") | (d[\"o_orderpriority\"] == \"2-HIGH\"), \"low\": (d[\"o_orderpriority\"] == \"5-LOW\") | (d[\"o_orderpriority\"] == \"4-NOT SPECIFIED\")}) Or mix the two. You then have to specify required columns again. It is the set of columns required for *all* your functions. >>> d = d.with_columns({\"high\": (lambda x:(x[\"o_orderpriority\"] == \"1-URGENT\") | (x[\"o_orderpriority\"] == \"2-HIGH\"), \"low\": (d[\"o_orderpriority\"] == \"5-LOW\") | (d[\"o_orderpriority\"] == \"4-NOT SPECIFIED\")}, required_columns = {\"o_orderpriority\"}) \"\"\" assert type ( required_columns ) == set # fix the new column ordering new_column_names = list ( new_columns . keys ()) sql_statement = \"select *\" for new_column in new_columns : assert new_column not in self . schema , \"For now new columns cannot have same names as existing columns\" transform = new_columns [ new_column ] assert type ( transform ) == type ( lambda x : 1 ) or type ( transform ) == Expression , \"Transform must be a function or a Quokka Expression\" if type ( transform ) == Expression : required_columns = required_columns . union ( transform . required_columns ()) sql_statement += \", \" + transform . sql () + \" as \" + new_column else : # detected an arbitrary function. If required columns are not specified, assume all columns are required if len ( required_columns ) == 0 : required_columns = set ( self . schema ) def polars_func ( batch ): con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) if sql_statement != \"select *\" : # if there are any columns to add batch_arrow = batch . to_arrow () batch = polars . from_arrow ( con . execute ( sql_statement + \" from batch_arrow\" ) . arrow ()) batch = batch . with_columns ([ polars . Series ( name = column_name , values = new_columns [ column_name ]( batch )) for column_name in new_column_names if type ( new_columns [ column_name ]) == type ( lambda x : 1 )]) return batch return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = self . schema + new_column_names , schema_mapping = { ** { new_column : { - 1 : new_column } for new_column in new_column_names }, ** { col : { 0 : col } for col in self . schema }}, required_columns = { 0 : required_columns }, function = polars_func , foldable = foldable ), schema = self . schema + new_column_names , sorted = self . sorted )","title":"DataStream.with_columns"},{"location":"datastream/with_columns_sql/","text":"DataStream.with_columns_sql This is the SQL analog of with_columns. Parameters: Name Type Description Default new_columns str A SQL expression X as in 'SELECT *, X from DataStream'. You can specify multiple columns by separating them with commas. You must provide an alias for each column. Please look at the examples. required foldable bool Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight True Return A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) Now create two columns high and low using SQL. >>> d = d . with_columns_sql ( 'o_orderpriority = \"1-URGENT\" or o_orderpriority = 2-HIGH as high, ... o_orderpriority = \"3-MEDIUM\" or o_orderpriority = 4 - NOT SPECIFIED \" as low') Another example. >>> d = d . with_columns_sql ( 'high + low as total' ) You must provide aliases for your columns, and separate the column defintiions with commas. Source code in pyquokka/datastream.py 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 def with_columns_sql ( self , new_columns : str , foldable = True ): \"\"\" This is the SQL analog of with_columns. Args: new_columns (str): A SQL expression X as in 'SELECT *, X from DataStream'. You can specify multiple columns by separating them with commas. You must provide an alias for each column. Please look at the examples. foldable (bool): Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight Return: A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc.read_csv(\"lineitem.csv\") Now create two columns high and low using SQL. >>> d = d.with_columns_sql('o_orderpriority = \"1-URGENT\" or o_orderpriority = 2-HIGH as high, ... o_orderpriority = \"3-MEDIUM\" or o_orderpriority = 4-NOT SPECIFIED\" as low') Another example. >>> d = d.with_columns_sql('high + low as total') You must provide aliases for your columns, and separate the column defintiions with commas. \"\"\" statements = new_columns . split ( \",\" ) sql_statement = \"select *, \" + new_columns + \" from batch_arrow\" new_column_names = [] required_columns = set () for statement in statements : node = sqlglot . parse_one ( statement ) assert type ( node ) == sqlglot . exp . Alias , \"must provide new name for each column: x1 as some_compute, x2 as some_compute, etc.\" new_column_names . append ( node . alias ) required_columns = required_columns . union ( required_columns_from_exp ( node . this )) def polars_func ( batch ): con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) batch_arrow = batch . to_arrow () return polars . from_arrow ( con . execute ( sql_statement ) . arrow ()) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = self . schema + new_column_names , schema_mapping = { ** { new_column : { - 1 : new_column } for new_column in new_column_names }, ** { col : { 0 : col } for col in self . schema }}, required_columns = { 0 : required_columns }, function = polars_func , foldable = foldable ), schema = self . schema + new_column_names , sorted = self . sorted )","title":"DataStream.with_columns_sql"},{"location":"datastream/with_columns_sql/#datastreamwith_columns_sql","text":"This is the SQL analog of with_columns. Parameters: Name Type Description Default new_columns str A SQL expression X as in 'SELECT *, X from DataStream'. You can specify multiple columns by separating them with commas. You must provide an alias for each column. Please look at the examples. required foldable bool Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight True Return A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc . read_csv ( \"lineitem.csv\" ) Now create two columns high and low using SQL. >>> d = d . with_columns_sql ( 'o_orderpriority = \"1-URGENT\" or o_orderpriority = 2-HIGH as high, ... o_orderpriority = \"3-MEDIUM\" or o_orderpriority = 4 - NOT SPECIFIED \" as low') Another example. >>> d = d . with_columns_sql ( 'high + low as total' ) You must provide aliases for your columns, and separate the column defintiions with commas. Source code in pyquokka/datastream.py 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 def with_columns_sql ( self , new_columns : str , foldable = True ): \"\"\" This is the SQL analog of with_columns. Args: new_columns (str): A SQL expression X as in 'SELECT *, X from DataStream'. You can specify multiple columns by separating them with commas. You must provide an alias for each column. Please look at the examples. foldable (bool): Whether or not the function can be executed as part of the batch post-processing of the previous operation in the execution graph. This is set to True by default. Correctly setting this flag requires some insight into how Quokka works. Lightweight Return: A new DataStream with new columns made by the user defined functions. Examples: >>> d = qc.read_csv(\"lineitem.csv\") Now create two columns high and low using SQL. >>> d = d.with_columns_sql('o_orderpriority = \"1-URGENT\" or o_orderpriority = 2-HIGH as high, ... o_orderpriority = \"3-MEDIUM\" or o_orderpriority = 4-NOT SPECIFIED\" as low') Another example. >>> d = d.with_columns_sql('high + low as total') You must provide aliases for your columns, and separate the column defintiions with commas. \"\"\" statements = new_columns . split ( \",\" ) sql_statement = \"select *, \" + new_columns + \" from batch_arrow\" new_column_names = [] required_columns = set () for statement in statements : node = sqlglot . parse_one ( statement ) assert type ( node ) == sqlglot . exp . Alias , \"must provide new name for each column: x1 as some_compute, x2 as some_compute, etc.\" new_column_names . append ( node . alias ) required_columns = required_columns . union ( required_columns_from_exp ( node . this )) def polars_func ( batch ): con = duckdb . connect () . execute ( 'PRAGMA threads= %d ' % 8 ) batch_arrow = batch . to_arrow () return polars . from_arrow ( con . execute ( sql_statement ) . arrow ()) return self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = MapNode ( schema = self . schema + new_column_names , schema_mapping = { ** { new_column : { - 1 : new_column } for new_column in new_column_names }, ** { col : { 0 : col } for col in self . schema }}, required_columns = { 0 : required_columns }, function = polars_func , foldable = foldable ), schema = self . schema + new_column_names , sorted = self . sorted )","title":"DataStream.with_columns_sql"},{"location":"datastream/write_csv/","text":"DataStream.write_csv This will write out the entire contents of the DataStream to a list of CSVs. Parameters: Name Type Description Default table_location str the root directory to write the output CSVs to. Similar to Spark, Quokka by default writes out a directory of CSVs instead of dumping all the results to a single CSV so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. required output_line_limit int how many rows each CSV in the output should have. The current implementation simply buffers this many rows in memory instead of using file appends, so you should have enough memory! 1000000 Return DataStream containing the filenames of the CSVs that were produced. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) >>> f = f . filter ( \"l_orderkey < 10 and l_partkey > 5\" ) >>> f . write_csv ( \"/home/user/test-out\" ) Make sure to create the directory first! This will write out a list of CSVs to /home/user/test-out. Source code in pyquokka/datastream.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def write_csv ( self , table_location , output_line_limit = 1000000 ): \"\"\" This will write out the entire contents of the DataStream to a list of CSVs. Args: table_location (str): the root directory to write the output CSVs to. Similar to Spark, Quokka by default writes out a directory of CSVs instead of dumping all the results to a single CSV so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. output_line_limit (int): how many rows each CSV in the output should have. The current implementation simply buffers this many rows in memory instead of using file appends, so you should have enough memory! Return: DataStream containing the filenames of the CSVs that were produced. Examples: >>> f = qc.read_csv(\"lineitem.csv\") >>> f = f.filter(\"l_orderkey < 10 and l_partkey > 5\") >>> f.write_csv(\"/home/user/test-out\") Make sure to create the directory first! This will write out a list of CSVs to /home/user/test-out. \"\"\" assert \"*\" not in table_location , \"* not supported, just supply the path.\" if self . materialized : df = self . _get_materialized_df () df . write_csv ( table_location ) return if table_location [: 5 ] == \"s3://\" : if type ( self . quokka_context . cluster ) == LocalCluster : print ( \"Warning: trying to write S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] try : client = boto3 . client ( \"s3\" ) region = client . get_bucket_location ( Bucket = bucket )[ \"LocationConstraint\" ] except : raise Exception ( \"Bucket does not exist.\" ) executor = OutputExecutor ( table_location , \"csv\" , region = region , row_group_size = output_line_limit ) else : if type ( self . quokka_context . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support wQuokkariting local dataset with S3 cluster. Must use S3 bucket.\" ) assert table_location [ 0 ] == \"/\" , \"You must supply absolute path to directory.\" assert os . path . isdir ( table_location ), \"Must supply an existing directory\" executor = OutputExecutor ( table_location , \"csv\" , region = \"local\" , row_group_size = output_line_limit ) name_stream = self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = StatefulNode ( schema = [ \"filename\" ], # this is a stateful node, but predicates and projections can be pushed down. schema_mapping = { \"filename\" : { - 1 : \"filename\" }}, required_columns = { 0 : set ( self . schema )}, operator = executor ), schema = [ \"filename\" ], ) return name_stream","title":"DataStream.write_csv"},{"location":"datastream/write_csv/#datastreamwrite_csv","text":"This will write out the entire contents of the DataStream to a list of CSVs. Parameters: Name Type Description Default table_location str the root directory to write the output CSVs to. Similar to Spark, Quokka by default writes out a directory of CSVs instead of dumping all the results to a single CSV so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. required output_line_limit int how many rows each CSV in the output should have. The current implementation simply buffers this many rows in memory instead of using file appends, so you should have enough memory! 1000000 Return DataStream containing the filenames of the CSVs that were produced. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) >>> f = f . filter ( \"l_orderkey < 10 and l_partkey > 5\" ) >>> f . write_csv ( \"/home/user/test-out\" ) Make sure to create the directory first! This will write out a list of CSVs to /home/user/test-out. Source code in pyquokka/datastream.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def write_csv ( self , table_location , output_line_limit = 1000000 ): \"\"\" This will write out the entire contents of the DataStream to a list of CSVs. Args: table_location (str): the root directory to write the output CSVs to. Similar to Spark, Quokka by default writes out a directory of CSVs instead of dumping all the results to a single CSV so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. output_line_limit (int): how many rows each CSV in the output should have. The current implementation simply buffers this many rows in memory instead of using file appends, so you should have enough memory! Return: DataStream containing the filenames of the CSVs that were produced. Examples: >>> f = qc.read_csv(\"lineitem.csv\") >>> f = f.filter(\"l_orderkey < 10 and l_partkey > 5\") >>> f.write_csv(\"/home/user/test-out\") Make sure to create the directory first! This will write out a list of CSVs to /home/user/test-out. \"\"\" assert \"*\" not in table_location , \"* not supported, just supply the path.\" if self . materialized : df = self . _get_materialized_df () df . write_csv ( table_location ) return if table_location [: 5 ] == \"s3://\" : if type ( self . quokka_context . cluster ) == LocalCluster : print ( \"Warning: trying to write S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] try : client = boto3 . client ( \"s3\" ) region = client . get_bucket_location ( Bucket = bucket )[ \"LocationConstraint\" ] except : raise Exception ( \"Bucket does not exist.\" ) executor = OutputExecutor ( table_location , \"csv\" , region = region , row_group_size = output_line_limit ) else : if type ( self . quokka_context . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support wQuokkariting local dataset with S3 cluster. Must use S3 bucket.\" ) assert table_location [ 0 ] == \"/\" , \"You must supply absolute path to directory.\" assert os . path . isdir ( table_location ), \"Must supply an existing directory\" executor = OutputExecutor ( table_location , \"csv\" , region = \"local\" , row_group_size = output_line_limit ) name_stream = self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = StatefulNode ( schema = [ \"filename\" ], # this is a stateful node, but predicates and projections can be pushed down. schema_mapping = { \"filename\" : { - 1 : \"filename\" }}, required_columns = { 0 : set ( self . schema )}, operator = executor ), schema = [ \"filename\" ], ) return name_stream","title":"DataStream.write_csv"},{"location":"datastream/write_parquet/","text":"DataStream.write_parquet This will write out the entire contents of the DataStream to a list of Parquets. Parameters: Name Type Description Default table_location str the root directory to write the output Parquets to. Similar to Spark, Quokka by default writes out a directory of Parquets instead of dumping all the results to a single Parquet so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. required output_line_limit int the row group size in each output file. 5000000 Return DataStream containing the filenames of the Parquets that were produced. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) >>> f = f . filter ( \"l_orderkey < 10 and l_partkey > 5\" ) >>> f . write_parquet ( \"/home/user/test-out\" ) You should create the directory before hand! This will write out a list of Parquets to /home/user/test-out. Source code in pyquokka/datastream.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def write_parquet ( self , table_location , output_line_limit = 5000000 ): \"\"\" This will write out the entire contents of the DataStream to a list of Parquets. Args: table_location (str): the root directory to write the output Parquets to. Similar to Spark, Quokka by default writes out a directory of Parquets instead of dumping all the results to a single Parquet so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. output_line_limit (int): the row group size in each output file. Return: DataStream containing the filenames of the Parquets that were produced. Examples: >>> f = qc.read_csv(\"lineitem.csv\") >>> f = f.filter(\"l_orderkey < 10 and l_partkey > 5\") >>> f.write_parquet(\"/home/user/test-out\") You should create the directory before hand! This will write out a list of Parquets to /home/user/test-out. \"\"\" if self . materialized : df = self . _get_materialized_df () df . write_parquet ( table_location ) return if table_location [: 5 ] == \"s3://\" : if type ( self . quokka_context . cluster ) == LocalCluster : print ( \"Warning: trying to write S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] try : client = boto3 . client ( \"s3\" ) region = client . get_bucket_location ( Bucket = bucket )[ \"LocationConstraint\" ] except : raise Exception ( \"Bucket does not exist.\" ) executor = OutputExecutor ( table_location , \"parquet\" , region = region , row_group_size = output_line_limit ) else : if type ( self . quokka_context . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support writing local dataset with S3 cluster. Must use S3 bucket.\" ) assert table_location [ 0 ] == \"/\" , \"You must supply absolute path to directory.\" executor = OutputExecutor ( table_location , \"parquet\" , region = \"local\" , row_group_size = output_line_limit ) name_stream = self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = StatefulNode ( schema = [ \"filename\" ], # this is a stateful node, but predicates and projections can be pushed down. schema_mapping = { \"filename\" : { - 1 : \"filename\" }}, required_columns = { 0 : set ( self . schema )}, operator = executor ), schema = [ \"filename\" ], ) return name_stream","title":"DataStream.write_parquet"},{"location":"datastream/write_parquet/#datastreamwrite_parquet","text":"This will write out the entire contents of the DataStream to a list of Parquets. Parameters: Name Type Description Default table_location str the root directory to write the output Parquets to. Similar to Spark, Quokka by default writes out a directory of Parquets instead of dumping all the results to a single Parquet so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. required output_line_limit int the row group size in each output file. 5000000 Return DataStream containing the filenames of the Parquets that were produced. Examples: >>> f = qc . read_csv ( \"lineitem.csv\" ) >>> f = f . filter ( \"l_orderkey < 10 and l_partkey > 5\" ) >>> f . write_parquet ( \"/home/user/test-out\" ) You should create the directory before hand! This will write out a list of Parquets to /home/user/test-out. Source code in pyquokka/datastream.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def write_parquet ( self , table_location , output_line_limit = 5000000 ): \"\"\" This will write out the entire contents of the DataStream to a list of Parquets. Args: table_location (str): the root directory to write the output Parquets to. Similar to Spark, Quokka by default writes out a directory of Parquets instead of dumping all the results to a single Parquet so the output can be done in parallel. If your dataset is small and you want a single file, you can adjust the output_line_limit parameter. Example table_locations: s3://bucket/prefix for cloud, absolute path /home/user/files for disk. output_line_limit (int): the row group size in each output file. Return: DataStream containing the filenames of the Parquets that were produced. Examples: >>> f = qc.read_csv(\"lineitem.csv\") >>> f = f.filter(\"l_orderkey < 10 and l_partkey > 5\") >>> f.write_parquet(\"/home/user/test-out\") You should create the directory before hand! This will write out a list of Parquets to /home/user/test-out. \"\"\" if self . materialized : df = self . _get_materialized_df () df . write_parquet ( table_location ) return if table_location [: 5 ] == \"s3://\" : if type ( self . quokka_context . cluster ) == LocalCluster : print ( \"Warning: trying to write S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] try : client = boto3 . client ( \"s3\" ) region = client . get_bucket_location ( Bucket = bucket )[ \"LocationConstraint\" ] except : raise Exception ( \"Bucket does not exist.\" ) executor = OutputExecutor ( table_location , \"parquet\" , region = region , row_group_size = output_line_limit ) else : if type ( self . quokka_context . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support writing local dataset with S3 cluster. Must use S3 bucket.\" ) assert table_location [ 0 ] == \"/\" , \"You must supply absolute path to directory.\" executor = OutputExecutor ( table_location , \"parquet\" , region = \"local\" , row_group_size = output_line_limit ) name_stream = self . quokka_context . new_stream ( sources = { 0 : self }, partitioners = { 0 : PassThroughPartitioner ()}, node = StatefulNode ( schema = [ \"filename\" ], # this is a stateful node, but predicates and projections can be pushed down. schema_mapping = { \"filename\" : { - 1 : \"filename\" }}, required_columns = { 0 : set ( self . schema )}, operator = executor ), schema = [ \"filename\" ], ) return name_stream","title":"DataStream.write_parquet"},{"location":"expression/dt_hour/","text":"ExprDateTimeNameSpace.hour Same as polars.Expr.dt.hour Source code in pyquokka/expression.py 293 294 295 296 297 298 299 def hour ( self ): \"\"\" Same as polars.Expr.dt.hour \"\"\" return Expression ( F . hour ( self . expr . sqlglot_expr ))","title":"Expression.dt.hour"},{"location":"expression/dt_hour/#exprdatetimenamespacehour","text":"Same as polars.Expr.dt.hour Source code in pyquokka/expression.py 293 294 295 296 297 298 299 def hour ( self ): \"\"\" Same as polars.Expr.dt.hour \"\"\" return Expression ( F . hour ( self . expr . sqlglot_expr ))","title":"ExprDateTimeNameSpace.hour"},{"location":"expression/dt_microsecond/","text":"ExprDateTimeNameSpace.microsecond Same as polars.Expr.dt.microsecond Source code in pyquokka/expression.py 325 326 327 328 329 330 331 def microsecond ( self ): \"\"\" Same as polars.Expr.dt.microsecond \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"microsecond\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"Expression.dt.microsecond"},{"location":"expression/dt_microsecond/#exprdatetimenamespacemicrosecond","text":"Same as polars.Expr.dt.microsecond Source code in pyquokka/expression.py 325 326 327 328 329 330 331 def microsecond ( self ): \"\"\" Same as polars.Expr.dt.microsecond \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"microsecond\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"ExprDateTimeNameSpace.microsecond"},{"location":"expression/dt_millisecond/","text":"ExprDateTimeNameSpace.millisecond Same as polars.Expr.dt.millisecond Source code in pyquokka/expression.py 317 318 319 320 321 322 323 def millisecond ( self ): \"\"\" Same as polars.Expr.dt.millisecond \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"millisecond\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"Expression.dt.millisecond"},{"location":"expression/dt_millisecond/#exprdatetimenamespacemillisecond","text":"Same as polars.Expr.dt.millisecond Source code in pyquokka/expression.py 317 318 319 320 321 322 323 def millisecond ( self ): \"\"\" Same as polars.Expr.dt.millisecond \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"millisecond\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"ExprDateTimeNameSpace.millisecond"},{"location":"expression/dt_minute/","text":"ExprDateTimeNameSpace.minute Same as polars.Expr.dt.minute Source code in pyquokka/expression.py 301 302 303 304 305 306 307 def minute ( self ): \"\"\" Same as polars.Expr.dt.minute \"\"\" return Expression ( F . minute ( self . expr . sqlglot_expr ))","title":"Expression.dt.minute"},{"location":"expression/dt_minute/#exprdatetimenamespaceminute","text":"Same as polars.Expr.dt.minute Source code in pyquokka/expression.py 301 302 303 304 305 306 307 def minute ( self ): \"\"\" Same as polars.Expr.dt.minute \"\"\" return Expression ( F . minute ( self . expr . sqlglot_expr ))","title":"ExprDateTimeNameSpace.minute"},{"location":"expression/dt_month/","text":"ExprDateTimeNameSpace.month Same as polars.Expr.dt.month Source code in pyquokka/expression.py 349 350 351 352 353 354 355 def month ( self ): \"\"\" Same as polars.Expr.dt.month \"\"\" return Expression ( F . month ( self . expr . sqlglot_expr ))","title":"Expression.dt.month"},{"location":"expression/dt_month/#exprdatetimenamespacemonth","text":"Same as polars.Expr.dt.month Source code in pyquokka/expression.py 349 350 351 352 353 354 355 def month ( self ): \"\"\" Same as polars.Expr.dt.month \"\"\" return Expression ( F . month ( self . expr . sqlglot_expr ))","title":"ExprDateTimeNameSpace.month"},{"location":"expression/dt_offset_by/","text":"ExprDateTimeNameSpace.offset_by Functionally same as polars.Expr.dt.offset_by. However arguments are different. Parameters: Name Type Description Default num int or float number of units to offset by required unit str unit of offset. One of \"ms\", \"s\", \"m\", \"h\", \"d\", \"M\", \"y\" required Source code in pyquokka/expression.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def offset_by ( self , num , unit ): \"\"\" Functionally same as polars.Expr.dt.offset_by. However arguments are different. Args: num (int or float): number of units to offset by unit (str): unit of offset. One of \"ms\", \"s\", \"m\", \"h\", \"d\", \"M\", \"y\" \"\"\" assert type ( unit ) == str and unit in { \"ms\" , \"s\" , \"m\" , \"h\" , \"d\" , \"M\" , \"y\" }, \"unit must be one of 'ms', 's', 'm', 'h', 'd', 'M', 'y'\" mapping = { \"ms\" : \"millisecond\" , \"s\" : \"second\" , \"m\" : \"minute\" , \"h\" : \"hour\" , \"d\" : \"day\" , \"M\" : \"month\" , \"y\" : \"year\" } if type ( num ) == int or type ( num ) == float : return Expression ( self . expr . sqlglot_expr + sqlglot . parse_one ( \"interval {} {} \" . format ( num , mapping [ unit ]))) else : raise Exception ( \"num must be an int or float. Offseting by a column is not supported yet\" )","title":"Expression.dt.offset_by"},{"location":"expression/dt_offset_by/#exprdatetimenamespaceoffset_by","text":"Functionally same as polars.Expr.dt.offset_by. However arguments are different. Parameters: Name Type Description Default num int or float number of units to offset by required unit str unit of offset. One of \"ms\", \"s\", \"m\", \"h\", \"d\", \"M\", \"y\" required Source code in pyquokka/expression.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def offset_by ( self , num , unit ): \"\"\" Functionally same as polars.Expr.dt.offset_by. However arguments are different. Args: num (int or float): number of units to offset by unit (str): unit of offset. One of \"ms\", \"s\", \"m\", \"h\", \"d\", \"M\", \"y\" \"\"\" assert type ( unit ) == str and unit in { \"ms\" , \"s\" , \"m\" , \"h\" , \"d\" , \"M\" , \"y\" }, \"unit must be one of 'ms', 's', 'm', 'h', 'd', 'M', 'y'\" mapping = { \"ms\" : \"millisecond\" , \"s\" : \"second\" , \"m\" : \"minute\" , \"h\" : \"hour\" , \"d\" : \"day\" , \"M\" : \"month\" , \"y\" : \"year\" } if type ( num ) == int or type ( num ) == float : return Expression ( self . expr . sqlglot_expr + sqlglot . parse_one ( \"interval {} {} \" . format ( num , mapping [ unit ]))) else : raise Exception ( \"num must be an int or float. Offseting by a column is not supported yet\" )","title":"ExprDateTimeNameSpace.offset_by"},{"location":"expression/dt_second/","text":"ExprDateTimeNameSpace.second Same as polars.Expr.dt.second Source code in pyquokka/expression.py 309 310 311 312 313 314 315 def second ( self ): \"\"\" Same as polars.Expr.dt.second \"\"\" return Expression ( F . second ( self . expr . sqlglot_expr ))","title":"Expression.dt.second"},{"location":"expression/dt_second/#exprdatetimenamespacesecond","text":"Same as polars.Expr.dt.second Source code in pyquokka/expression.py 309 310 311 312 313 314 315 def second ( self ): \"\"\" Same as polars.Expr.dt.second \"\"\" return Expression ( F . second ( self . expr . sqlglot_expr ))","title":"ExprDateTimeNameSpace.second"},{"location":"expression/dt_strftime/","text":"ExprDateTimeNameSpace.strftime Same as polars.Expr.dt.strftime, except no need to pass in a format string, because the only format accepted is \"%Y-%m-%d %H:%M:%S.%f\" Source code in pyquokka/expression.py 382 383 384 385 386 387 388 def strftime ( self ): \"\"\" Same as polars.Expr.dt.strftime, except no need to pass in a format string, because the only format accepted is \"%Y-%m-%d %H:%M:%S.%f\" \"\"\" return Expression ( self . expr . sqlglot_expr . cast ( \"string\" ))","title":"Expression.dt.strftime"},{"location":"expression/dt_strftime/#exprdatetimenamespacestrftime","text":"Same as polars.Expr.dt.strftime, except no need to pass in a format string, because the only format accepted is \"%Y-%m-%d %H:%M:%S.%f\" Source code in pyquokka/expression.py 382 383 384 385 386 387 388 def strftime ( self ): \"\"\" Same as polars.Expr.dt.strftime, except no need to pass in a format string, because the only format accepted is \"%Y-%m-%d %H:%M:%S.%f\" \"\"\" return Expression ( self . expr . sqlglot_expr . cast ( \"string\" ))","title":"ExprDateTimeNameSpace.strftime"},{"location":"expression/dt_week/","text":"ExprDateTimeNameSpace.week Same as polars.Expr.dt.week Source code in pyquokka/expression.py 341 342 343 344 345 346 347 def week ( self ): \"\"\" Same as polars.Expr.dt.week \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"weekofyear\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"Expression.dt.week"},{"location":"expression/dt_week/#exprdatetimenamespaceweek","text":"Same as polars.Expr.dt.week Source code in pyquokka/expression.py 341 342 343 344 345 346 347 def week ( self ): \"\"\" Same as polars.Expr.dt.week \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"weekofyear\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"ExprDateTimeNameSpace.week"},{"location":"expression/dt_weekday/","text":"ExprDateTimeNameSpace.weekday Same as polars.Expr.dt.weekday Source code in pyquokka/expression.py 333 334 335 336 337 338 339 def weekday ( self ): \"\"\" Same as polars.Expr.dt.weekday \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"dayofweek\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"Expression.dt.weekday"},{"location":"expression/dt_weekday/#exprdatetimenamespaceweekday","text":"Same as polars.Expr.dt.weekday Source code in pyquokka/expression.py 333 334 335 336 337 338 339 def weekday ( self ): \"\"\" Same as polars.Expr.dt.weekday \"\"\" return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . exp . Anonymous ( this = \"dayofweek\" , expressions = [ self . expr . sqlglot_expr . expression ])))","title":"ExprDateTimeNameSpace.weekday"},{"location":"expression/dt_year/","text":"ExprDateTimeNameSpace.year Same as polars.Expr.dt.year Source code in pyquokka/expression.py 357 358 359 360 361 362 363 def year ( self ): \"\"\" Same as polars.Expr.dt.year \"\"\" return Expression ( F . year ( self . expr . sqlglot_expr ))","title":"Expression.dt.year"},{"location":"expression/dt_year/#exprdatetimenamespaceyear","text":"Same as polars.Expr.dt.year Source code in pyquokka/expression.py 357 358 359 360 361 362 363 def year ( self ): \"\"\" Same as polars.Expr.dt.year \"\"\" return Expression ( F . year ( self . expr . sqlglot_expr ))","title":"ExprDateTimeNameSpace.year"},{"location":"expression/str_contains/","text":"ExprStringNameSpace.contains Same as polars.Expr.str.contains Parameters: Name Type Description Default s str The string to check if the expression contains required Source code in pyquokka/expression.py 208 209 210 211 212 213 214 215 216 217 218 def contains ( self , s ): \"\"\" Same as polars.Expr.str.contains Args: s (str): The string to check if the expression contains \"\"\" assert type ( s ) == str return Expression ( self . expr . sqlglot_expr . like ( \"* {} *\" . format ( s )))","title":"Expression.str.contains"},{"location":"expression/str_contains/#exprstringnamespacecontains","text":"Same as polars.Expr.str.contains Parameters: Name Type Description Default s str The string to check if the expression contains required Source code in pyquokka/expression.py 208 209 210 211 212 213 214 215 216 217 218 def contains ( self , s ): \"\"\" Same as polars.Expr.str.contains Args: s (str): The string to check if the expression contains \"\"\" assert type ( s ) == str return Expression ( self . expr . sqlglot_expr . like ( \"* {} *\" . format ( s )))","title":"ExprStringNameSpace.contains"},{"location":"expression/str_ends_with/","text":"ExprStringNameSpace.ends_with Same as polars.Expr.str.ends_with Parameters: Name Type Description Default s str The string to check if the expression ends with required Source code in pyquokka/expression.py 232 233 234 235 236 237 238 239 240 241 242 def ends_with ( self , s ): \"\"\" Same as polars.Expr.str.ends_with Args: s (str): The string to check if the expression ends with \"\"\" assert type ( s ) == str return Expression ( self . expr . sqlglot_expr . like ( \"* {} \" . format ( s )))","title":"Expression.str.ends_with"},{"location":"expression/str_ends_with/#exprstringnamespaceends_with","text":"Same as polars.Expr.str.ends_with Parameters: Name Type Description Default s str The string to check if the expression ends with required Source code in pyquokka/expression.py 232 233 234 235 236 237 238 239 240 241 242 def ends_with ( self , s ): \"\"\" Same as polars.Expr.str.ends_with Args: s (str): The string to check if the expression ends with \"\"\" assert type ( s ) == str return Expression ( self . expr . sqlglot_expr . like ( \"* {} \" . format ( s )))","title":"ExprStringNameSpace.ends_with"},{"location":"expression/str_hash/","text":"ExprStringNameSpace.hash Source code in pyquokka/expression.py 285 286 def hash ( self ): return Expression ( F . hash ( self . expr . sqlglot_expr ))","title":"Expression.str.hash"},{"location":"expression/str_hash/#exprstringnamespacehash","text":"Source code in pyquokka/expression.py 285 286 def hash ( self ): return Expression ( F . hash ( self . expr . sqlglot_expr ))","title":"ExprStringNameSpace.hash"},{"location":"expression/str_json_extract/","text":"ExprStringNameSpace.json_extract Similar polars.Expr.str.json_extract, however you can only extract one field! You must specify the field. If the field is not in the json, the value will be null. Parameters: Name Type Description Default field str The field to extract from the json required Examples: >>> qc = QuokkaContext () >>> a = polars . from_dict ({ \"a\" :[ 1 , 1 , 2 , 2 ], \"c\" : [ \"A\" , \"B\" , \"C\" , \"D\" ], \"b\" :[ '{\"my_field\": \"quack\"}' , '{\"my_field\": \"quack\"}' , '{\"my_field\": \"quack\"}' , '{\"my_field\": \"quack\"}' ]}) >>> a = qc . from_polars ( a ) >>> a . with_columns ({ \"my_field\" : a [ \"b\" ] . str . json_extract ( \"my_field\" )}) . collect () Source code in pyquokka/expression.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def json_extract ( self , field ): \"\"\" Similar polars.Expr.str.json_extract, however you can only extract one field! You must specify the field. If the field is not in the json, the value will be null. Args: field (str): The field to extract from the json Examples: >>> qc = QuokkaContext() >>> a = polars.from_dict({\"a\":[1,1,2,2], \"c\": [\"A\",\"B\",\"C\",\"D\"], \"b\":['{\"my_field\": \"quack\"}','{\"my_field\": \"quack\"}','{\"my_field\": \"quack\"}','{\"my_field\": \"quack\"}']}) >>> a = qc.from_polars(a) >>> a.with_columns({\"my_field\": a[\"b\"].str.json_extract(\"my_field\")}).collect() \"\"\" assert type ( self . expr . sqlglot_expr . expression ) == sqlglot . exp . Column , \"json_extract can only be applied to an untransformed column\" col_name = self . expr . sqlglot_expr . expression . name return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . parse_one ( \"json_extract_string( {} , ' {} ')\" . format ( col_name , field ))))","title":"Expression.str.json_extract"},{"location":"expression/str_json_extract/#exprstringnamespacejson_extract","text":"Similar polars.Expr.str.json_extract, however you can only extract one field! You must specify the field. If the field is not in the json, the value will be null. Parameters: Name Type Description Default field str The field to extract from the json required Examples: >>> qc = QuokkaContext () >>> a = polars . from_dict ({ \"a\" :[ 1 , 1 , 2 , 2 ], \"c\" : [ \"A\" , \"B\" , \"C\" , \"D\" ], \"b\" :[ '{\"my_field\": \"quack\"}' , '{\"my_field\": \"quack\"}' , '{\"my_field\": \"quack\"}' , '{\"my_field\": \"quack\"}' ]}) >>> a = qc . from_polars ( a ) >>> a . with_columns ({ \"my_field\" : a [ \"b\" ] . str . json_extract ( \"my_field\" )}) . collect () Source code in pyquokka/expression.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def json_extract ( self , field ): \"\"\" Similar polars.Expr.str.json_extract, however you can only extract one field! You must specify the field. If the field is not in the json, the value will be null. Args: field (str): The field to extract from the json Examples: >>> qc = QuokkaContext() >>> a = polars.from_dict({\"a\":[1,1,2,2], \"c\": [\"A\",\"B\",\"C\",\"D\"], \"b\":['{\"my_field\": \"quack\"}','{\"my_field\": \"quack\"}','{\"my_field\": \"quack\"}','{\"my_field\": \"quack\"}']}) >>> a = qc.from_polars(a) >>> a.with_columns({\"my_field\": a[\"b\"].str.json_extract(\"my_field\")}).collect() \"\"\" assert type ( self . expr . sqlglot_expr . expression ) == sqlglot . exp . Column , \"json_extract can only be applied to an untransformed column\" col_name = self . expr . sqlglot_expr . expression . name return Expression ( sqlglot . dataframe . sql . Column ( sqlglot . parse_one ( \"json_extract_string( {} , ' {} ')\" . format ( col_name , field ))))","title":"ExprStringNameSpace.json_extract"},{"location":"expression/str_length/","text":"ExprStringNameSpace.length Same as polars.Expr.str.length Source code in pyquokka/expression.py 244 245 246 247 248 249 250 def length ( self ): \"\"\" Same as polars.Expr.str.length \"\"\" return Expression ( F . length ( self . expr . sqlglot_expr ))","title":"Expression.str.length"},{"location":"expression/str_length/#exprstringnamespacelength","text":"Same as polars.Expr.str.length Source code in pyquokka/expression.py 244 245 246 247 248 249 250 def length ( self ): \"\"\" Same as polars.Expr.str.length \"\"\" return Expression ( F . length ( self . expr . sqlglot_expr ))","title":"ExprStringNameSpace.length"},{"location":"expression/str_starts_with/","text":"ExprStringNameSpace.starts_with Same as polars.Expr.str.starts_with Parameters: Name Type Description Default s str The string to check if the expression starts with required Source code in pyquokka/expression.py 220 221 222 223 224 225 226 227 228 229 230 def starts_with ( self , s ): \"\"\" Same as polars.Expr.str.starts_with Args: s (str): The string to check if the expression starts with \"\"\" assert type ( s ) == str return Expression ( self . expr . sqlglot_expr . like ( \" {} *\" . format ( s )))","title":"Expression.str.starts_with"},{"location":"expression/str_starts_with/#exprstringnamespacestarts_with","text":"Same as polars.Expr.str.starts_with Parameters: Name Type Description Default s str The string to check if the expression starts with required Source code in pyquokka/expression.py 220 221 222 223 224 225 226 227 228 229 230 def starts_with ( self , s ): \"\"\" Same as polars.Expr.str.starts_with Args: s (str): The string to check if the expression starts with \"\"\" assert type ( s ) == str return Expression ( self . expr . sqlglot_expr . like ( \" {} *\" . format ( s )))","title":"ExprStringNameSpace.starts_with"},{"location":"expression/str_strptime/","text":"ExprStringNameSpace.strptime Parse the string expression to a datetime/date/time type. The string must have format \"%Y-%m-%d %H:%M:%S.%f\" You should specify a time format, otherwise it will default to datetime. Parameters: Name Type Description Default format str \"datetime\" (default) | \"date\" | \"time\" 'datetime' Source code in pyquokka/expression.py 274 275 276 277 278 279 280 281 282 283 def strptime ( self , format = \"datetime\" ): \"\"\" Parse the string expression to a datetime/date/time type. The string must have format \"%Y-%m-%d %H:%M:%S.%f\" You should specify a time format, otherwise it will default to datetime. Args: format (str): \"datetime\" (default) | \"date\" | \"time\" \"\"\" return Expression ( self . expr . sqlglot_expr . cast ( format ))","title":"Expression.str.strptime"},{"location":"expression/str_strptime/#exprstringnamespacestrptime","text":"Parse the string expression to a datetime/date/time type. The string must have format \"%Y-%m-%d %H:%M:%S.%f\" You should specify a time format, otherwise it will default to datetime. Parameters: Name Type Description Default format str \"datetime\" (default) | \"date\" | \"time\" 'datetime' Source code in pyquokka/expression.py 274 275 276 277 278 279 280 281 282 283 def strptime ( self , format = \"datetime\" ): \"\"\" Parse the string expression to a datetime/date/time type. The string must have format \"%Y-%m-%d %H:%M:%S.%f\" You should specify a time format, otherwise it will default to datetime. Args: format (str): \"datetime\" (default) | \"date\" | \"time\" \"\"\" return Expression ( self . expr . sqlglot_expr . cast ( format ))","title":"ExprStringNameSpace.strptime"},{"location":"expression/str_to_lowercase/","text":"ExprStringNameSpace.to_lowercase Same as polars.Expr.str.to_lowercase Source code in pyquokka/expression.py 200 201 202 203 204 205 206 def to_lowercase ( self ): \"\"\" Same as polars.Expr.str.to_lowercase \"\"\" return Expression ( F . lower ( self . expr . sqlglot_expr ))","title":"Expression.str.to_lowercase"},{"location":"expression/str_to_lowercase/#exprstringnamespaceto_lowercase","text":"Same as polars.Expr.str.to_lowercase Source code in pyquokka/expression.py 200 201 202 203 204 205 206 def to_lowercase ( self ): \"\"\" Same as polars.Expr.str.to_lowercase \"\"\" return Expression ( F . lower ( self . expr . sqlglot_expr ))","title":"ExprStringNameSpace.to_lowercase"},{"location":"expression/str_to_uppercase/","text":"ExprStringNameSpace.to_uppercase Same as polars.Expr.str.to_uppercase Source code in pyquokka/expression.py 192 193 194 195 196 197 198 def to_uppercase ( self ): \"\"\" Same as polars.Expr.str.to_uppercase \"\"\" return Expression ( F . upper ( self . expr . sqlglot_expr ))","title":"Expression.str.to_uppercase"},{"location":"expression/str_to_uppercase/#exprstringnamespaceto_uppercase","text":"Same as polars.Expr.str.to_uppercase Source code in pyquokka/expression.py 192 193 194 195 196 197 198 def to_uppercase ( self ): \"\"\" Same as polars.Expr.str.to_uppercase \"\"\" return Expression ( F . upper ( self . expr . sqlglot_expr ))","title":"ExprStringNameSpace.to_uppercase"},{"location":"quokka_context/from_arrow/","text":"QuokkaContext.from_arrow Create a DataStream for a pyarrow Table. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Parameters: Name Type Description Default df PyArrow Table The polars DataFrame to create the DataStream from. required Returns: Name Type Description DataStream The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> df = pl . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) . to_arrow () >>> stream = qc . from_arrow ( df ) >>> stream . count () Source code in pyquokka/df.py 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 def from_arrow ( self , df ): \"\"\" Create a DataStream for a pyarrow Table. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Args: df (PyArrow Table): The polars DataFrame to create the DataStream from. Returns: DataStream: The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}).to_arrow() >>> stream = qc.from_arrow(df) >>> stream.count() \"\"\" self . nodes [ self . latest_node_id ] = InputPolarsNode ( polars . from_arrow ( df )) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True )","title":"QuokkaContext.from_arrow"},{"location":"quokka_context/from_arrow/#quokkacontextfrom_arrow","text":"Create a DataStream for a pyarrow Table. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Parameters: Name Type Description Default df PyArrow Table The polars DataFrame to create the DataStream from. required Returns: Name Type Description DataStream The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> df = pl . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) . to_arrow () >>> stream = qc . from_arrow ( df ) >>> stream . count () Source code in pyquokka/df.py 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 def from_arrow ( self , df ): \"\"\" Create a DataStream for a pyarrow Table. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Args: df (PyArrow Table): The polars DataFrame to create the DataStream from. Returns: DataStream: The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}).to_arrow() >>> stream = qc.from_arrow(df) >>> stream.count() \"\"\" self . nodes [ self . latest_node_id ] = InputPolarsNode ( polars . from_arrow ( df )) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True )","title":"QuokkaContext.from_arrow"},{"location":"quokka_context/from_pandas/","text":"QuokkaContext.from_pandas Create a DataStream from a pandas DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Parameters: Name Type Description Default df Pandas DataFrame The pandas DataFrame to create the DataStream from. required Returns: Name Type Description DataStream The DataStream created from the pandas DataFrame. Examples: >>> import pandas as pd >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) >>> stream = qc . from_pandas ( df ) >>> stream . count () Source code in pyquokka/df.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 def from_pandas ( self , df ): \"\"\" Create a DataStream from a pandas DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Args: df (Pandas DataFrame): The pandas DataFrame to create the DataStream from. Returns: DataStream: The DataStream created from the pandas DataFrame. Examples: >>> import pandas as pd >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) >>> stream = qc.from_pandas(df) >>> stream.count() \"\"\" self . nodes [ self . latest_node_id ] = InputPolarsNode ( polars . from_pandas ( df )) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True )","title":"QuokkaContext.from_pandas"},{"location":"quokka_context/from_pandas/#quokkacontextfrom_pandas","text":"Create a DataStream from a pandas DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Parameters: Name Type Description Default df Pandas DataFrame The pandas DataFrame to create the DataStream from. required Returns: Name Type Description DataStream The DataStream created from the pandas DataFrame. Examples: >>> import pandas as pd >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) >>> stream = qc . from_pandas ( df ) >>> stream . count () Source code in pyquokka/df.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 def from_pandas ( self , df ): \"\"\" Create a DataStream from a pandas DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Args: df (Pandas DataFrame): The pandas DataFrame to create the DataStream from. Returns: DataStream: The DataStream created from the pandas DataFrame. Examples: >>> import pandas as pd >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) >>> stream = qc.from_pandas(df) >>> stream.count() \"\"\" self . nodes [ self . latest_node_id ] = InputPolarsNode ( polars . from_pandas ( df )) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True )","title":"QuokkaContext.from_pandas"},{"location":"quokka_context/from_polars/","text":"QuokkaContext.from_polars Create a DataStream from a polars DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Parameters: Name Type Description Default df Polars DataFrame The polars DataFrame to create the DataStream from. required Returns: Name Type Description DataStream The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> df = pl . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) >>> stream = qc . from_polars ( df ) >>> stream . count () Source code in pyquokka/df.py 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 def from_polars ( self , df ): \"\"\" Create a DataStream from a polars DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Args: df (Polars DataFrame): The polars DataFrame to create the DataStream from. Returns: DataStream: The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) >>> stream = qc.from_polars(df) >>> stream.count() \"\"\" self . nodes [ self . latest_node_id ] = InputPolarsNode ( df ) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True )","title":"QuokkaContext.from_polars"},{"location":"quokka_context/from_polars/#quokkacontextfrom_polars","text":"Create a DataStream from a polars DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Parameters: Name Type Description Default df Polars DataFrame The polars DataFrame to create the DataStream from. required Returns: Name Type Description DataStream The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> df = pl . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) >>> stream = qc . from_polars ( df ) >>> stream . count () Source code in pyquokka/df.py 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 def from_polars ( self , df ): \"\"\" Create a DataStream from a polars DataFrame. The DataFrame will be materialized. If you don't know what this means, don't worry about it. Args: df (Polars DataFrame): The polars DataFrame to create the DataStream from. Returns: DataStream: The DataStream created from the polars DataFrame. Examples: >>> import polars as pl >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) >>> stream = qc.from_polars(df) >>> stream.count() \"\"\" self . nodes [ self . latest_node_id ] = InputPolarsNode ( df ) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True )","title":"QuokkaContext.from_polars"},{"location":"quokka_context/get_config/","text":"QuokkaContext.get_config Gets a config value for the entire cluster. Parameters: Name Type Description Default key str the key to get required Returns: Name Type Description any the value of the key Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext () >>> qc . get_config ( \"optimize_joins\" ) Source code in pyquokka/df.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_config ( self , key ): \"\"\" Gets a config value for the entire cluster. Args: key (str): the key to get Returns: any: the value of the key Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext() >>> qc.get_config(\"optimize_joins\") \"\"\" if key in self . sql_config : return self . sql_config [ key ] elif key in self . exec_config : return self . exec_config [ key ] else : raise Exception ( \"key not found in config\" )","title":"QuokkaContext.get_config"},{"location":"quokka_context/get_config/#quokkacontextget_config","text":"Gets a config value for the entire cluster. Parameters: Name Type Description Default key str the key to get required Returns: Name Type Description any the value of the key Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext () >>> qc . get_config ( \"optimize_joins\" ) Source code in pyquokka/df.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def get_config ( self , key ): \"\"\" Gets a config value for the entire cluster. Args: key (str): the key to get Returns: any: the value of the key Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext() >>> qc.get_config(\"optimize_joins\") \"\"\" if key in self . sql_config : return self . sql_config [ key ] elif key in self . exec_config : return self . exec_config [ key ] else : raise Exception ( \"key not found in config\" )","title":"QuokkaContext.get_config"},{"location":"quokka_context/quokka_context/","text":"QuokkaContext Creates a QuokkaContext object. Needed to do basically anything. Similar to SparkContext. Parameters: Name Type Description Default cluster Cluster Cluster object. If None, a LocalCluster will be created. None io_per_node int Number of IO nodes to launch per machine. Default is 2. This controls the number of IO thread pools to use per machine. 2 exec_per_node int Number of compute nodes to launch per machine. Default is 1. This controls the number of compute thread pools to use per machine. 1 Returns: Type Description None QuokkaContext object Examples: >>> from pyquokka.df import QuokkaContext This will create a QuokkaContext object with a LocalCluster to execute locally. Useful for testing and exploration. >>> qc = QuokkaContext () If you want to connect to a remote Ray cluster, you can do this. >>> manager = QuokkaClusterManager () >>> cluster = manager . get_cluster_from_ray ( \"my_cluster.yaml\" , aws_access_key , aws_access_id , requirements = [ \"numpy\" , \"pandas\" ], spill_dir = \"/data\" ) >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) Or you can spin up a new cluster: >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> qc = QuokkaContext ( cluster ) Or you can connect to a stopped cluster that was saved to json. >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . from_json ( \"my_cluster.json\" ) Source code in pyquokka/df.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , cluster = None , io_per_node = 2 , exec_per_node = 1 ) -> None : \"\"\" Creates a QuokkaContext object. Needed to do basically anything. Similar to SparkContext. Args: cluster (Cluster): Cluster object. If None, a LocalCluster will be created. io_per_node (int): Number of IO nodes to launch per machine. Default is 2. This controls the number of IO thread pools to use per machine. exec_per_node (int): Number of compute nodes to launch per machine. Default is 1. This controls the number of compute thread pools to use per machine. Returns: QuokkaContext object Examples: >>> from pyquokka.df import QuokkaContext This will create a QuokkaContext object with a LocalCluster to execute locally. Useful for testing and exploration. >>> qc = QuokkaContext() If you want to connect to a remote Ray cluster, you can do this. >>> manager = QuokkaClusterManager() >>> cluster = manager.get_cluster_from_ray(\"my_cluster.yaml\", aws_access_key, aws_access_id, requirements = [\"numpy\", \"pandas\"], spill_dir = \"/data\") >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) Or you can spin up a new cluster: >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> qc = QuokkaContext(cluster) Or you can connect to a stopped cluster that was saved to json. >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.from_json(\"my_cluster.json\") \"\"\" self . sql_config = { \"optimize_joins\" : True , \"s3_csv_materialize_threshold\" : 10 * 1048576 , \"disk_csv_materialize_threshold\" : 1048576 , \"s3_parquet_materialize_threshold\" : 10 * 1048576 , \"disk_parquet_materialize_threshold\" : 1048576 } self . exec_config = { \"hbq_path\" : \"/data/\" , \"fault_tolerance\" : False , \"memory_limit\" : 0.25 , \"max_pipeline_batches\" : 30 , \"checkpoint_interval\" : None , \"checkpoint_bucket\" : \"quokka-checkpoint\" , \"batch_attempt\" : 20 , \"max_pipeline\" : 3 } self . latest_node_id = 0 self . nodes = {} self . cluster = LocalCluster () if cluster is None else cluster if type ( self . cluster ) == LocalCluster : if self . exec_config [ \"fault_tolerance\" ]: print ( \"Fault tolerance is not supported in local mode, turning it off\" ) self . exec_config [ \"fault_tolerance\" ] = False self . io_per_node = io_per_node self . exec_per_node = exec_per_node self . coordinator = Coordinator . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + str ( self . cluster . leader_private_ip ): 0.001 }) . remote () self . catalog = Catalog . options ( num_cpus = 0.001 , resources = { \"node:\" + str ( self . cluster . leader_private_ip ): 0.001 }) . remote () self . dataset_manager = ArrowDataset . options ( num_cpus = 0.001 , resources = { \"node:\" + str ( self . cluster . leader_private_ip ): 0.001 }) . remote () self . task_managers = {} self . node_locs = {} self . io_nodes = set () self . compute_nodes = set () self . replay_nodes = set () count = 0 self . tag_io_nodes = { k : set () for k in self . cluster . tags } self . tag_compute_nodes = { k : set () for k in self . cluster . tags } self . leader_compute_nodes = [] self . leader_io_nodes = [] # default strategy launches two IO nodes and one compute node per machine private_ips = list ( self . cluster . private_ips . values ()) for ip in private_ips : for k in range ( 1 ): self . task_managers [ count ] = ReplayTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) self . replay_nodes . add ( count ) self . node_locs [ count ] = ip count += 1 for k in range ( io_per_node ): self . task_managers [ count ] = IOTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) self . io_nodes . add ( count ) if ip == self . cluster . leader_private_ip : self . leader_io_nodes . append ( count ) for tag in self . cluster . tags : if ip in self . cluster . tags [ tag ]: self . tag_io_nodes [ tag ] . add ( count ) self . node_locs [ count ] = ip count += 1 for k in range ( exec_per_node ): if type ( self . cluster ) == LocalCluster : self . task_managers [ count ] = ExecTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) elif type ( self . cluster ) == EC2Cluster : self . task_managers [ count ] = ExecTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) else : raise Exception if ip == self . cluster . leader_private_ip : self . leader_compute_nodes . append ( count ) for tag in self . cluster . tags : if ip in self . cluster . tags [ tag ]: self . tag_compute_nodes [ tag ] . add ( count ) self . compute_nodes . add ( count ) self . node_locs [ count ] = ip count += 1 ray . get ( self . coordinator . register_nodes . remote ( replay_nodes = { k : self . task_managers [ k ] for k in self . replay_nodes }, io_nodes = { k : self . task_managers [ k ] for k in self . io_nodes }, compute_nodes = { k : self . task_managers [ k ] for k in self . compute_nodes })) ray . get ( self . coordinator . register_node_ips . remote ( self . node_locs ))","title":"QuokkaContext"},{"location":"quokka_context/quokka_context/#quokkacontext","text":"Creates a QuokkaContext object. Needed to do basically anything. Similar to SparkContext. Parameters: Name Type Description Default cluster Cluster Cluster object. If None, a LocalCluster will be created. None io_per_node int Number of IO nodes to launch per machine. Default is 2. This controls the number of IO thread pools to use per machine. 2 exec_per_node int Number of compute nodes to launch per machine. Default is 1. This controls the number of compute thread pools to use per machine. 1 Returns: Type Description None QuokkaContext object Examples: >>> from pyquokka.df import QuokkaContext This will create a QuokkaContext object with a LocalCluster to execute locally. Useful for testing and exploration. >>> qc = QuokkaContext () If you want to connect to a remote Ray cluster, you can do this. >>> manager = QuokkaClusterManager () >>> cluster = manager . get_cluster_from_ray ( \"my_cluster.yaml\" , aws_access_key , aws_access_id , requirements = [ \"numpy\" , \"pandas\" ], spill_dir = \"/data\" ) >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) Or you can spin up a new cluster: >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> qc = QuokkaContext ( cluster ) Or you can connect to a stopped cluster that was saved to json. >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . from_json ( \"my_cluster.json\" ) Source code in pyquokka/df.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , cluster = None , io_per_node = 2 , exec_per_node = 1 ) -> None : \"\"\" Creates a QuokkaContext object. Needed to do basically anything. Similar to SparkContext. Args: cluster (Cluster): Cluster object. If None, a LocalCluster will be created. io_per_node (int): Number of IO nodes to launch per machine. Default is 2. This controls the number of IO thread pools to use per machine. exec_per_node (int): Number of compute nodes to launch per machine. Default is 1. This controls the number of compute thread pools to use per machine. Returns: QuokkaContext object Examples: >>> from pyquokka.df import QuokkaContext This will create a QuokkaContext object with a LocalCluster to execute locally. Useful for testing and exploration. >>> qc = QuokkaContext() If you want to connect to a remote Ray cluster, you can do this. >>> manager = QuokkaClusterManager() >>> cluster = manager.get_cluster_from_ray(\"my_cluster.yaml\", aws_access_key, aws_access_id, requirements = [\"numpy\", \"pandas\"], spill_dir = \"/data\") >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) Or you can spin up a new cluster: >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> qc = QuokkaContext(cluster) Or you can connect to a stopped cluster that was saved to json. >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.from_json(\"my_cluster.json\") \"\"\" self . sql_config = { \"optimize_joins\" : True , \"s3_csv_materialize_threshold\" : 10 * 1048576 , \"disk_csv_materialize_threshold\" : 1048576 , \"s3_parquet_materialize_threshold\" : 10 * 1048576 , \"disk_parquet_materialize_threshold\" : 1048576 } self . exec_config = { \"hbq_path\" : \"/data/\" , \"fault_tolerance\" : False , \"memory_limit\" : 0.25 , \"max_pipeline_batches\" : 30 , \"checkpoint_interval\" : None , \"checkpoint_bucket\" : \"quokka-checkpoint\" , \"batch_attempt\" : 20 , \"max_pipeline\" : 3 } self . latest_node_id = 0 self . nodes = {} self . cluster = LocalCluster () if cluster is None else cluster if type ( self . cluster ) == LocalCluster : if self . exec_config [ \"fault_tolerance\" ]: print ( \"Fault tolerance is not supported in local mode, turning it off\" ) self . exec_config [ \"fault_tolerance\" ] = False self . io_per_node = io_per_node self . exec_per_node = exec_per_node self . coordinator = Coordinator . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + str ( self . cluster . leader_private_ip ): 0.001 }) . remote () self . catalog = Catalog . options ( num_cpus = 0.001 , resources = { \"node:\" + str ( self . cluster . leader_private_ip ): 0.001 }) . remote () self . dataset_manager = ArrowDataset . options ( num_cpus = 0.001 , resources = { \"node:\" + str ( self . cluster . leader_private_ip ): 0.001 }) . remote () self . task_managers = {} self . node_locs = {} self . io_nodes = set () self . compute_nodes = set () self . replay_nodes = set () count = 0 self . tag_io_nodes = { k : set () for k in self . cluster . tags } self . tag_compute_nodes = { k : set () for k in self . cluster . tags } self . leader_compute_nodes = [] self . leader_io_nodes = [] # default strategy launches two IO nodes and one compute node per machine private_ips = list ( self . cluster . private_ips . values ()) for ip in private_ips : for k in range ( 1 ): self . task_managers [ count ] = ReplayTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) self . replay_nodes . add ( count ) self . node_locs [ count ] = ip count += 1 for k in range ( io_per_node ): self . task_managers [ count ] = IOTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) self . io_nodes . add ( count ) if ip == self . cluster . leader_private_ip : self . leader_io_nodes . append ( count ) for tag in self . cluster . tags : if ip in self . cluster . tags [ tag ]: self . tag_io_nodes [ tag ] . add ( count ) self . node_locs [ count ] = ip count += 1 for k in range ( exec_per_node ): if type ( self . cluster ) == LocalCluster : self . task_managers [ count ] = ExecTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) elif type ( self . cluster ) == EC2Cluster : self . task_managers [ count ] = ExecTaskManager . options ( num_cpus = 0.001 , max_concurrency = 2 , resources = { \"node:\" + ip : 0.001 }) . remote ( count , self . cluster . leader_public_ip , list ( self . cluster . private_ips . values ()), self . exec_config ) else : raise Exception if ip == self . cluster . leader_private_ip : self . leader_compute_nodes . append ( count ) for tag in self . cluster . tags : if ip in self . cluster . tags [ tag ]: self . tag_compute_nodes [ tag ] . add ( count ) self . compute_nodes . add ( count ) self . node_locs [ count ] = ip count += 1 ray . get ( self . coordinator . register_nodes . remote ( replay_nodes = { k : self . task_managers [ k ] for k in self . replay_nodes }, io_nodes = { k : self . task_managers [ k ] for k in self . io_nodes }, compute_nodes = { k : self . task_managers [ k ] for k in self . compute_nodes })) ray . get ( self . coordinator . register_node_ips . remote ( self . node_locs ))","title":"QuokkaContext"},{"location":"quokka_context/read_csv/","text":"QuokkaContext.read_csv Read in a CSV file or files from a table location. It can be a single CSV or a list of CSVs. It can be CSV(s) on disk or CSV(s) on S3. Currently other clouds are not supported. The CSVs can have a predefined schema using a list of column names in the schema argument, or you can specify the CSV has a header row and Quokka will read the schema from it. You should also specify the CSV's separator. Parameters: Name Type Description Default table_location str where the CSV(s) are. This mostly mimics Spark behavior. Look at the examples. required schema list you can provide a list of column names, it's kinda like polars.read_csv(new_columns=...) None has_header bool is there a header row. If the schema is not provided, this should be True. If the schema IS provided, this can still be True. Quokka will just ignore the header row. False sep str default to ',' but could be something else, like '|' for TPC-H tables ',' Return A DataStream. Examples: Read a single CSV. It's better always to specify the absolute path. >>> lineitem = qc . read_csv ( \"/home/ubuntu/tpch/lineitem.csv\" ) Read a directory of CSVs >>> lineitem = qc . read_csv ( \"/home/ubuntu/tpch/lineitem/*\" ) Read a single CSV from S3 >>> lineitem = qc . read_csv ( \"s3://tpc-h-csv/lineitem/lineitem.tbl.1\" ) Read CSVs from S3 bucket with prefix >>> lineitem = qc . read_csv ( \"s3://tpc-h-csv/lineitem/*\" ) ~~~ Source code in pyquokka/df.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def read_csv ( self , table_location : str , schema = None , has_header = False , sep = \",\" ): \"\"\" Read in a CSV file or files from a table location. It can be a single CSV or a list of CSVs. It can be CSV(s) on disk or CSV(s) on S3. Currently other clouds are not supported. The CSVs can have a predefined schema using a list of column names in the schema argument, or you can specify the CSV has a header row and Quokka will read the schema from it. You should also specify the CSV's separator. Args: table_location (str): where the CSV(s) are. This mostly mimics Spark behavior. Look at the examples. schema (list): you can provide a list of column names, it's kinda like polars.read_csv(new_columns=...) has_header (bool): is there a header row. If the schema is not provided, this should be True. If the schema IS provided, this can still be True. Quokka will just ignore the header row. sep (str): default to ',' but could be something else, like '|' for TPC-H tables Return: A DataStream. Examples: Read a single CSV. It's better always to specify the absolute path. >>> lineitem = qc.read_csv(\"/home/ubuntu/tpch/lineitem.csv\") Read a directory of CSVs >>> lineitem = qc.read_csv(\"/home/ubuntu/tpch/lineitem/*\") Read a single CSV from S3 >>> lineitem = qc.read_csv(\"s3://tpc-h-csv/lineitem/lineitem.tbl.1\") Read CSVs from S3 bucket with prefix >>> lineitem = qc.read_csv(\"s3://tpc-h-csv/lineitem/*\") ~~~ \"\"\" def get_schema_from_bytes ( resp ): first_newline = resp . find ( bytes ( ' \\n ' , 'utf-8' )) if first_newline == - 1 : raise Exception ( \"could not detect the first line break with first 4 kb\" ) resp = resp [: first_newline ] if resp [ - 1 ] == sep : resp = resp [: - 1 ] schema = resp . decode ( \"utf-8\" ) . split ( sep ) return schema def return_materialized_stream ( where , my_schema = None ): if has_header : df = polars . read_csv ( where , has_header = True , separator = sep ) else : df = polars . read_csv ( where , new_columns = my_schema , has_header = False , separator = sep ) self . nodes [ self . latest_node_id ] = InputPolarsNode ( df ) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True ) if schema is None : assert has_header , \"if not provide schema, must have header.\" if schema is not None and has_header : print ( \"You specified a schema as well as a header. Quokka should use your schema and ignore the names in the header.\" ) if table_location [: 5 ] == \"s3://\" : if type ( self . cluster ) == LocalCluster : print ( \"Warning: trying to read S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] if \"*\" in table_location : assert table_location [ - 1 ] == \"*\" , \"wildcard can only be the last character in address string\" prefix = \"/\" . join ( table_location [: - 1 ] . split ( \"/\" )[ 1 :]) s3 = boto3 . client ( 's3' ) z = s3 . list_objects_v2 ( Bucket = bucket , Prefix = prefix ) if 'Contents' not in z : raise Exception ( \"Wrong S3 path\" ) files = [ i [ 'Key' ] for i in z [ 'Contents' ]] sizes = [ i [ 'Size' ] for i in z [ 'Contents' ]] assert 'NextContinuationToken' not in z , \"too many files in S3 bucket\" assert len ( files ) > 0 , \"no files under prefix\" if schema is None : resp = s3 . get_object ( Bucket = bucket , Key = files [ 0 ], Range = 'bytes= {} - {} ' . format ( 0 , 4096 ))[ 'Body' ] . read () schema = get_schema_from_bytes ( resp ) # if there are more than one files, there could be header problems. just do one file right now if len ( files ) == 1 and sizes [ 0 ] < self . sql_config [ \"s3_csv_materialize_threshold\" ]: return return_materialized_stream ( \"s3://\" + bucket + \"/\" + files [ 0 ], schema ) token = ray . get ( self . catalog . register_s3_csv_source . remote ( bucket , files [ 0 ], schema , sep , sum ( sizes ))) self . nodes [ self . latest_node_id ] = InputS3CSVNode ( bucket , prefix , None , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : key = \"/\" . join ( table_location . split ( \"/\" )[ 1 :]) s3 = boto3 . client ( 's3' ) try : response = s3 . head_object ( Bucket = bucket , Key = key ) size = response [ 'ContentLength' ] except : raise Exception ( \"CSV not found in S3!\" ) if schema is None : resp = s3 . get_object ( Bucket = bucket , Key = key , Range = 'bytes= {} - {} ' . format ( 0 , 4096 ))[ 'Body' ] . read () schema = get_schema_from_bytes ( resp ) if size < 10 * 1048576 : return return_materialized_stream ( \"s3://\" + table_location , schema ) token = ray . get ( self . catalog . register_s3_csv_source . remote ( bucket , key , schema , sep , size )) self . nodes [ self . latest_node_id ] = InputS3CSVNode ( bucket , None , key , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : if type ( self . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support reading local dataset with S3 cluster. Must use S3 bucket.\" ) if \"*\" in table_location : table_location = table_location [: - 1 ] assert table_location [ - 1 ] == \"/\" , \"must specify * with entire directory, doesn't support prefixes yet\" try : files = [ i for i in os . listdir ( table_location )] except : raise Exception ( \"Tried to get list of files at \" , table_location , \" failed. Make sure specify absolute path\" ) assert len ( files ) > 0 if schema is None : resp = open ( table_location + files [ 0 ], \"rb\" ) . read ( 1024 * 4 ) schema = get_schema_from_bytes ( resp ) if len ( files ) == 1 and os . path . getsize ( table_location + files [ 0 ]) < self . sql_config [ \"disk_csv_materialize_threshold\" ]: return return_materialized_stream ( table_location + files [ 0 ], schema ) token = ray . get ( self . catalog . register_disk_csv_source . remote ( table_location , schema , sep )) self . nodes [ self . latest_node_id ] = InputDiskCSVNode ( table_location , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : size = os . path . getsize ( table_location ) if schema is None : resp = open ( table_location , \"rb\" ) . read ( 1024 * 4 ) schema = get_schema_from_bytes ( resp ) if size < self . sql_config [ \"disk_csv_materialize_threshold\" ]: return return_materialized_stream ( table_location , schema ) token = ray . get ( self . catalog . register_disk_csv_source . remote ( table_location , schema , sep )) self . nodes [ self . latest_node_id ] = InputDiskCSVNode ( table_location , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) self . latest_node_id += 1 return DataStream ( self , schema , self . latest_node_id - 1 )","title":"QuokkaContext.read_csv"},{"location":"quokka_context/read_csv/#quokkacontextread_csv","text":"Read in a CSV file or files from a table location. It can be a single CSV or a list of CSVs. It can be CSV(s) on disk or CSV(s) on S3. Currently other clouds are not supported. The CSVs can have a predefined schema using a list of column names in the schema argument, or you can specify the CSV has a header row and Quokka will read the schema from it. You should also specify the CSV's separator. Parameters: Name Type Description Default table_location str where the CSV(s) are. This mostly mimics Spark behavior. Look at the examples. required schema list you can provide a list of column names, it's kinda like polars.read_csv(new_columns=...) None has_header bool is there a header row. If the schema is not provided, this should be True. If the schema IS provided, this can still be True. Quokka will just ignore the header row. False sep str default to ',' but could be something else, like '|' for TPC-H tables ',' Return A DataStream. Examples: Read a single CSV. It's better always to specify the absolute path. >>> lineitem = qc . read_csv ( \"/home/ubuntu/tpch/lineitem.csv\" ) Read a directory of CSVs >>> lineitem = qc . read_csv ( \"/home/ubuntu/tpch/lineitem/*\" ) Read a single CSV from S3 >>> lineitem = qc . read_csv ( \"s3://tpc-h-csv/lineitem/lineitem.tbl.1\" ) Read CSVs from S3 bucket with prefix >>> lineitem = qc . read_csv ( \"s3://tpc-h-csv/lineitem/*\" ) ~~~ Source code in pyquokka/df.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def read_csv ( self , table_location : str , schema = None , has_header = False , sep = \",\" ): \"\"\" Read in a CSV file or files from a table location. It can be a single CSV or a list of CSVs. It can be CSV(s) on disk or CSV(s) on S3. Currently other clouds are not supported. The CSVs can have a predefined schema using a list of column names in the schema argument, or you can specify the CSV has a header row and Quokka will read the schema from it. You should also specify the CSV's separator. Args: table_location (str): where the CSV(s) are. This mostly mimics Spark behavior. Look at the examples. schema (list): you can provide a list of column names, it's kinda like polars.read_csv(new_columns=...) has_header (bool): is there a header row. If the schema is not provided, this should be True. If the schema IS provided, this can still be True. Quokka will just ignore the header row. sep (str): default to ',' but could be something else, like '|' for TPC-H tables Return: A DataStream. Examples: Read a single CSV. It's better always to specify the absolute path. >>> lineitem = qc.read_csv(\"/home/ubuntu/tpch/lineitem.csv\") Read a directory of CSVs >>> lineitem = qc.read_csv(\"/home/ubuntu/tpch/lineitem/*\") Read a single CSV from S3 >>> lineitem = qc.read_csv(\"s3://tpc-h-csv/lineitem/lineitem.tbl.1\") Read CSVs from S3 bucket with prefix >>> lineitem = qc.read_csv(\"s3://tpc-h-csv/lineitem/*\") ~~~ \"\"\" def get_schema_from_bytes ( resp ): first_newline = resp . find ( bytes ( ' \\n ' , 'utf-8' )) if first_newline == - 1 : raise Exception ( \"could not detect the first line break with first 4 kb\" ) resp = resp [: first_newline ] if resp [ - 1 ] == sep : resp = resp [: - 1 ] schema = resp . decode ( \"utf-8\" ) . split ( sep ) return schema def return_materialized_stream ( where , my_schema = None ): if has_header : df = polars . read_csv ( where , has_header = True , separator = sep ) else : df = polars . read_csv ( where , new_columns = my_schema , has_header = False , separator = sep ) self . nodes [ self . latest_node_id ] = InputPolarsNode ( df ) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True ) if schema is None : assert has_header , \"if not provide schema, must have header.\" if schema is not None and has_header : print ( \"You specified a schema as well as a header. Quokka should use your schema and ignore the names in the header.\" ) if table_location [: 5 ] == \"s3://\" : if type ( self . cluster ) == LocalCluster : print ( \"Warning: trying to read S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] if \"*\" in table_location : assert table_location [ - 1 ] == \"*\" , \"wildcard can only be the last character in address string\" prefix = \"/\" . join ( table_location [: - 1 ] . split ( \"/\" )[ 1 :]) s3 = boto3 . client ( 's3' ) z = s3 . list_objects_v2 ( Bucket = bucket , Prefix = prefix ) if 'Contents' not in z : raise Exception ( \"Wrong S3 path\" ) files = [ i [ 'Key' ] for i in z [ 'Contents' ]] sizes = [ i [ 'Size' ] for i in z [ 'Contents' ]] assert 'NextContinuationToken' not in z , \"too many files in S3 bucket\" assert len ( files ) > 0 , \"no files under prefix\" if schema is None : resp = s3 . get_object ( Bucket = bucket , Key = files [ 0 ], Range = 'bytes= {} - {} ' . format ( 0 , 4096 ))[ 'Body' ] . read () schema = get_schema_from_bytes ( resp ) # if there are more than one files, there could be header problems. just do one file right now if len ( files ) == 1 and sizes [ 0 ] < self . sql_config [ \"s3_csv_materialize_threshold\" ]: return return_materialized_stream ( \"s3://\" + bucket + \"/\" + files [ 0 ], schema ) token = ray . get ( self . catalog . register_s3_csv_source . remote ( bucket , files [ 0 ], schema , sep , sum ( sizes ))) self . nodes [ self . latest_node_id ] = InputS3CSVNode ( bucket , prefix , None , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : key = \"/\" . join ( table_location . split ( \"/\" )[ 1 :]) s3 = boto3 . client ( 's3' ) try : response = s3 . head_object ( Bucket = bucket , Key = key ) size = response [ 'ContentLength' ] except : raise Exception ( \"CSV not found in S3!\" ) if schema is None : resp = s3 . get_object ( Bucket = bucket , Key = key , Range = 'bytes= {} - {} ' . format ( 0 , 4096 ))[ 'Body' ] . read () schema = get_schema_from_bytes ( resp ) if size < 10 * 1048576 : return return_materialized_stream ( \"s3://\" + table_location , schema ) token = ray . get ( self . catalog . register_s3_csv_source . remote ( bucket , key , schema , sep , size )) self . nodes [ self . latest_node_id ] = InputS3CSVNode ( bucket , None , key , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : if type ( self . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support reading local dataset with S3 cluster. Must use S3 bucket.\" ) if \"*\" in table_location : table_location = table_location [: - 1 ] assert table_location [ - 1 ] == \"/\" , \"must specify * with entire directory, doesn't support prefixes yet\" try : files = [ i for i in os . listdir ( table_location )] except : raise Exception ( \"Tried to get list of files at \" , table_location , \" failed. Make sure specify absolute path\" ) assert len ( files ) > 0 if schema is None : resp = open ( table_location + files [ 0 ], \"rb\" ) . read ( 1024 * 4 ) schema = get_schema_from_bytes ( resp ) if len ( files ) == 1 and os . path . getsize ( table_location + files [ 0 ]) < self . sql_config [ \"disk_csv_materialize_threshold\" ]: return return_materialized_stream ( table_location + files [ 0 ], schema ) token = ray . get ( self . catalog . register_disk_csv_source . remote ( table_location , schema , sep )) self . nodes [ self . latest_node_id ] = InputDiskCSVNode ( table_location , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : size = os . path . getsize ( table_location ) if schema is None : resp = open ( table_location , \"rb\" ) . read ( 1024 * 4 ) schema = get_schema_from_bytes ( resp ) if size < self . sql_config [ \"disk_csv_materialize_threshold\" ]: return return_materialized_stream ( table_location , schema ) token = ray . get ( self . catalog . register_disk_csv_source . remote ( table_location , schema , sep )) self . nodes [ self . latest_node_id ] = InputDiskCSVNode ( table_location , schema , sep , has_header ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) self . latest_node_id += 1 return DataStream ( self , schema , self . latest_node_id - 1 )","title":"QuokkaContext.read_csv"},{"location":"quokka_context/read_dataset/","text":"QuokkaContext.read_dataset Convert a DataSet back to a DataStream to process it. Parameters: Name Type Description Default dataset DataSet The dataset to read from. Note this is a Quokka DataSet, not a Ray Data dataset! required Returns: Name Type Description DataStream The data stream. Examples: >>> ds = qc . read_parquet ( \"s3://my-bucket/my-data.parquet\" ) . compute () ds will be a DataSet, i.e. a collection of Ray ObjectRefs. >>> ds = qc . read_dataset ( ds ) ds will now be a DataStream. Source code in pyquokka/df.py 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 def read_dataset ( self , dataset ): \"\"\" Convert a DataSet back to a DataStream to process it. Args: dataset (DataSet): The dataset to read from. Note this is a Quokka DataSet, not a Ray Data dataset! Returns: DataStream: The data stream. Examples: >>> ds = qc.read_parquet(\"s3://my-bucket/my-data.parquet\").compute() `ds` will be a DataSet, i.e. a collection of Ray ObjectRefs. >>> ds = qc.read_dataset(ds) `ds` will now be a DataStream. \"\"\" self . nodes [ self . latest_node_id ] = InputRayDatasetNode ( dataset ) self . latest_node_id += 1 return DataStream ( self , dataset . schema , self . latest_node_id - 1 )","title":"QuokkaContext.read_dataset"},{"location":"quokka_context/read_dataset/#quokkacontextread_dataset","text":"Convert a DataSet back to a DataStream to process it. Parameters: Name Type Description Default dataset DataSet The dataset to read from. Note this is a Quokka DataSet, not a Ray Data dataset! required Returns: Name Type Description DataStream The data stream. Examples: >>> ds = qc . read_parquet ( \"s3://my-bucket/my-data.parquet\" ) . compute () ds will be a DataSet, i.e. a collection of Ray ObjectRefs. >>> ds = qc . read_dataset ( ds ) ds will now be a DataStream. Source code in pyquokka/df.py 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 def read_dataset ( self , dataset ): \"\"\" Convert a DataSet back to a DataStream to process it. Args: dataset (DataSet): The dataset to read from. Note this is a Quokka DataSet, not a Ray Data dataset! Returns: DataStream: The data stream. Examples: >>> ds = qc.read_parquet(\"s3://my-bucket/my-data.parquet\").compute() `ds` will be a DataSet, i.e. a collection of Ray ObjectRefs. >>> ds = qc.read_dataset(ds) `ds` will now be a DataStream. \"\"\" self . nodes [ self . latest_node_id ] = InputRayDatasetNode ( dataset ) self . latest_node_id += 1 return DataStream ( self , dataset . schema , self . latest_node_id - 1 )","title":"QuokkaContext.read_dataset"},{"location":"quokka_context/read_delta/","text":"QuokkaContext.read_delta API under construction, help wanted. In the mean time try read_iceberg !","title":"QuokkaContext.read_delta"},{"location":"quokka_context/read_delta/#quokkacontextread_delta","text":"API under construction, help wanted. In the mean time try read_iceberg !","title":"QuokkaContext.read_delta"},{"location":"quokka_context/read_iceberg/","text":"QuokkaContext.read_iceberg Must have pyiceberg installed on the client. This is a new API. This only supports AWS Glue catalog so far. Parameters: Name Type Description Default table str The table name to read from. This is the full table name, not just the table name. For example, if the table is in the database \"default\" and the table name is \"my_table\", then the table name should be \"default.my_table\". required snapshot int The snapshot ID to read from. If this is not specified, the latest snapshot will be read from. None Returns: Name Type Description DataStream The DataStream created from the iceberg table. Examples: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> stream = qc . read_iceberg ( \"default.my_table\" ) Source code in pyquokka/df.py 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 def read_iceberg ( self , table , snapshot = None ): \"\"\" Must have pyiceberg installed on the client. This is a new API. This only supports AWS Glue catalog so far. Args: table (str): The table name to read from. This is the full table name, not just the table name. For example, if the table is in the database \"default\" and the table name is \"my_table\", then the table name should be \"default.my_table\". snapshot (int): The snapshot ID to read from. If this is not specified, the latest snapshot will be read from. Returns: DataStream: The DataStream created from the iceberg table. Examples: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> stream = qc.read_iceberg(\"default.my_table\") \"\"\" from pyiceberg.catalog import load_catalog catalog = load_catalog ( \"glue\" , ** { \"type\" : \"glue\" }) try : table = catalog . load_table ( table ) except : raise Exception ( \"table not found\" ) if snapshot is not None : assert snapshot in [ i . snapshot_id for i in table . metadata . snapshots ], \"snapshot not found\" self . nodes [ self . latest_node_id ] = InputS3IcebergNode ( table , snapshot ) self . latest_node_id += 1 return DataStream ( self , [ i . name for i in table . schema () . fields ], self . latest_node_id - 1 )","title":"QuokkaContext.read_iceberg"},{"location":"quokka_context/read_iceberg/#quokkacontextread_iceberg","text":"Must have pyiceberg installed on the client. This is a new API. This only supports AWS Glue catalog so far. Parameters: Name Type Description Default table str The table name to read from. This is the full table name, not just the table name. For example, if the table is in the database \"default\" and the table name is \"my_table\", then the table name should be \"default.my_table\". required snapshot int The snapshot ID to read from. If this is not specified, the latest snapshot will be read from. None Returns: Name Type Description DataStream The DataStream created from the iceberg table. Examples: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () >>> stream = qc . read_iceberg ( \"default.my_table\" ) Source code in pyquokka/df.py 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 def read_iceberg ( self , table , snapshot = None ): \"\"\" Must have pyiceberg installed on the client. This is a new API. This only supports AWS Glue catalog so far. Args: table (str): The table name to read from. This is the full table name, not just the table name. For example, if the table is in the database \"default\" and the table name is \"my_table\", then the table name should be \"default.my_table\". snapshot (int): The snapshot ID to read from. If this is not specified, the latest snapshot will be read from. Returns: DataStream: The DataStream created from the iceberg table. Examples: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() >>> stream = qc.read_iceberg(\"default.my_table\") \"\"\" from pyiceberg.catalog import load_catalog catalog = load_catalog ( \"glue\" , ** { \"type\" : \"glue\" }) try : table = catalog . load_table ( table ) except : raise Exception ( \"table not found\" ) if snapshot is not None : assert snapshot in [ i . snapshot_id for i in table . metadata . snapshots ], \"snapshot not found\" self . nodes [ self . latest_node_id ] = InputS3IcebergNode ( table , snapshot ) self . latest_node_id += 1 return DataStream ( self , [ i . name for i in table . schema () . fields ], self . latest_node_id - 1 )","title":"QuokkaContext.read_iceberg"},{"location":"quokka_context/read_parquet/","text":"QuokkaContext.read_parquet Read Parquet. It can be a single Parquet or a list of Parquets. It can be Parquet(s) on disk or Parquet(s) on S3. Currently other clouds are not supported. Parameters: Name Type Description Default table_location str where the Parquet(s) are. This mostly mimics Spark behavior. Look at the examples. required Return DataStream. Examples: Read a single Parquet. It's better always to specify the absolute path. >>> lineitem = qc . read_parquet ( \"/home/ubuntu/tpch/lineitem.parquet\" ) Read a directory of Parquets >>> lineitem = qc . read_parquet ( \"/home/ubuntu/tpch/lineitem/*\" ) Read a single Parquet from S3 >>> lineitem = qc . read_parquet ( \"s3://tpc-h-parquet/lineitem.parquet\" ) Read Parquets from S3 bucket with prefix >>> lineitem = qc . read_parquet ( \"s3://tpc-h-parquet/lineitem.parquet/*\" ) Source code in pyquokka/df.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 def read_parquet ( self , table_location : str ): \"\"\" Read Parquet. It can be a single Parquet or a list of Parquets. It can be Parquet(s) on disk or Parquet(s) on S3. Currently other clouds are not supported. Args: table_location (str): where the Parquet(s) are. This mostly mimics Spark behavior. Look at the examples. Return: DataStream. Examples: Read a single Parquet. It's better always to specify the absolute path. >>> lineitem = qc.read_parquet(\"/home/ubuntu/tpch/lineitem.parquet\") Read a directory of Parquets >>> lineitem = qc.read_parquet(\"/home/ubuntu/tpch/lineitem/*\") Read a single Parquet from S3 >>> lineitem = qc.read_parquet(\"s3://tpc-h-parquet/lineitem.parquet\") Read Parquets from S3 bucket with prefix >>> lineitem = qc.read_parquet(\"s3://tpc-h-parquet/lineitem.parquet/*\") \"\"\" def return_materialized_stream ( df ): self . nodes [ self . latest_node_id ] = InputPolarsNode ( df ) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True ) s3 = boto3 . client ( 's3' ) if table_location [: 5 ] == \"s3://\" : if type ( self . cluster ) == LocalCluster : print ( \"Warning: trying to read S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] if \"*\" in table_location : assert \"*\" not in table_location [: - 1 ], \"wildcard can only be the last character in address string\" table_location = table_location [: - 1 ] prefix = \"/\" . join ( table_location [: - 1 ] . split ( \"/\" )[ 1 :]) z = s3 . list_objects_v2 ( Bucket = bucket , Prefix = prefix ) if 'Contents' not in z : raise Exception ( \"Wrong S3 path\" ) files = [ bucket + \"/\" + i [ 'Key' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( \".parquet\" )] sizes = [ i [ 'Size' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( '.parquet' )] while 'NextContinuationToken' in z . keys (): z = s3 . list_objects_v2 ( Bucket = bucket , Prefix = prefix , ContinuationToken = z [ 'NextContinuationToken' ]) files . extend ([ bucket + \"/\" + i [ 'Key' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( \".parquet\" )]) sizes . extend ([ i [ 'Size' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( '.parquet' )]) assert len ( files ) > 0 , \"could not find any parquet files. make sure they end with .parquet\" if sum ( sizes ) < self . sql_config [ \"s3_parquet_materialize_threshold\" ] and len ( files ) == 1 : df = polars . from_arrow ( pq . read_table ( files [ 0 ], filesystem = S3FileSystem ())) return return_materialized_stream ( df ) try : f = pq . ParquetFile ( S3FileSystem () . open_input_file ( files [ 0 ])) schema = [ k . name for k in f . schema_arrow ] except : raise Exception ( \"schema discovery failed for Parquet dataset at location {} . Please raise Github issue.\" . format ( table_location )) token = ray . get ( self . catalog . register_s3_parquet_source . remote ( files [ 0 ], len ( sizes ))) self . nodes [ self . latest_node_id ] = InputS3ParquetNode ( files , schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : try : f = pq . ParquetFile ( S3FileSystem () . open_input_file ( table_location )) schema = [ k . name for k in f . schema_arrow ] except : raise Exception ( \"\"\"schema discovery failed for Parquet dataset at location {} . Note if you are specifying a prefix to many parquet files, must use asterix. E.g. qc.read_parquet(\"s3://rottnest/happy.parquet/*\")\"\"\" . format ( table_location )) key = \"/\" . join ( table_location . split ( \"/\" )[ 1 :]) response = s3 . head_object ( Bucket = bucket , Key = key ) size = response [ 'ContentLength' ] if size < self . sql_config [ \"s3_parquet_materialize_threshold\" ]: df = polars . from_arrow ( pq . read_table ( table_location , filesystem = S3FileSystem ())) return return_materialized_stream ( df ) token = ray . get ( self . catalog . register_s3_parquet_source . remote ( bucket + \"/\" + key , 1 )) self . nodes [ self . latest_node_id ] = InputS3ParquetNode ([ table_location ], schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) # self.nodes[self.latest_node_id].set_placement_strategy(CustomChannelsStrategy(2)) else : if type ( self . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support reading local dataset with S3 cluster. Must use S3 bucket.\" ) if \"*\" in table_location : table_location = table_location [: - 1 ] assert table_location [ - 1 ] == \"/\" , \"must specify * with entire directory, doesn't support prefixes yet\" try : files = [ i for i in os . listdir ( table_location ) if i . endswith ( \".parquet\" )] except : raise Exception ( \"Tried to get list of parquet files at \" , table_location , \" failed. Make sure specify absolute path and filenames end with .parquet\" ) assert len ( files ) > 0 f = pq . ParquetFile ( table_location + files [ 0 ]) schema = [ k . name for k in f . schema_arrow ] if len ( files ) == 1 and os . path . getsize ( table_location + files [ 0 ]) < self . sql_config [ \"disk_parquet_materialize_threshold\" ]: df = polars . read_parquet ( table_location + files [ 0 ]) return return_materialized_stream ( df ) token = ray . get ( self . catalog . register_disk_parquet_source . remote ( table_location )) self . nodes [ self . latest_node_id ] = InputDiskParquetNode ( table_location , schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : try : size = os . path . getsize ( table_location ) except : raise Exception ( \"could not find the parquet file at \" , table_location ) if size < self . sql_config [ \"disk_parquet_materialize_threshold\" ]: df = polars . read_parquet ( table_location ) return return_materialized_stream ( df ) f = pq . ParquetFile ( table_location ) schema = [ k . name for k in f . schema_arrow ] token = ray . get ( self . catalog . register_disk_parquet_source . remote ( table_location )) self . nodes [ self . latest_node_id ] = InputDiskParquetNode ( table_location , schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) self . latest_node_id += 1 return DataStream ( self , schema , self . latest_node_id - 1 )","title":"QuokkaContext.read_parquet"},{"location":"quokka_context/read_parquet/#quokkacontextread_parquet","text":"Read Parquet. It can be a single Parquet or a list of Parquets. It can be Parquet(s) on disk or Parquet(s) on S3. Currently other clouds are not supported. Parameters: Name Type Description Default table_location str where the Parquet(s) are. This mostly mimics Spark behavior. Look at the examples. required Return DataStream. Examples: Read a single Parquet. It's better always to specify the absolute path. >>> lineitem = qc . read_parquet ( \"/home/ubuntu/tpch/lineitem.parquet\" ) Read a directory of Parquets >>> lineitem = qc . read_parquet ( \"/home/ubuntu/tpch/lineitem/*\" ) Read a single Parquet from S3 >>> lineitem = qc . read_parquet ( \"s3://tpc-h-parquet/lineitem.parquet\" ) Read Parquets from S3 bucket with prefix >>> lineitem = qc . read_parquet ( \"s3://tpc-h-parquet/lineitem.parquet/*\" ) Source code in pyquokka/df.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 def read_parquet ( self , table_location : str ): \"\"\" Read Parquet. It can be a single Parquet or a list of Parquets. It can be Parquet(s) on disk or Parquet(s) on S3. Currently other clouds are not supported. Args: table_location (str): where the Parquet(s) are. This mostly mimics Spark behavior. Look at the examples. Return: DataStream. Examples: Read a single Parquet. It's better always to specify the absolute path. >>> lineitem = qc.read_parquet(\"/home/ubuntu/tpch/lineitem.parquet\") Read a directory of Parquets >>> lineitem = qc.read_parquet(\"/home/ubuntu/tpch/lineitem/*\") Read a single Parquet from S3 >>> lineitem = qc.read_parquet(\"s3://tpc-h-parquet/lineitem.parquet\") Read Parquets from S3 bucket with prefix >>> lineitem = qc.read_parquet(\"s3://tpc-h-parquet/lineitem.parquet/*\") \"\"\" def return_materialized_stream ( df ): self . nodes [ self . latest_node_id ] = InputPolarsNode ( df ) self . latest_node_id += 1 return DataStream ( self , df . columns , self . latest_node_id - 1 , materialized = True ) s3 = boto3 . client ( 's3' ) if table_location [: 5 ] == \"s3://\" : if type ( self . cluster ) == LocalCluster : print ( \"Warning: trying to read S3 dataset on local machine. This assumes high network bandwidth.\" ) table_location = table_location [ 5 :] bucket = table_location . split ( \"/\" )[ 0 ] if \"*\" in table_location : assert \"*\" not in table_location [: - 1 ], \"wildcard can only be the last character in address string\" table_location = table_location [: - 1 ] prefix = \"/\" . join ( table_location [: - 1 ] . split ( \"/\" )[ 1 :]) z = s3 . list_objects_v2 ( Bucket = bucket , Prefix = prefix ) if 'Contents' not in z : raise Exception ( \"Wrong S3 path\" ) files = [ bucket + \"/\" + i [ 'Key' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( \".parquet\" )] sizes = [ i [ 'Size' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( '.parquet' )] while 'NextContinuationToken' in z . keys (): z = s3 . list_objects_v2 ( Bucket = bucket , Prefix = prefix , ContinuationToken = z [ 'NextContinuationToken' ]) files . extend ([ bucket + \"/\" + i [ 'Key' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( \".parquet\" )]) sizes . extend ([ i [ 'Size' ] for i in z [ 'Contents' ] if i [ 'Key' ] . endswith ( '.parquet' )]) assert len ( files ) > 0 , \"could not find any parquet files. make sure they end with .parquet\" if sum ( sizes ) < self . sql_config [ \"s3_parquet_materialize_threshold\" ] and len ( files ) == 1 : df = polars . from_arrow ( pq . read_table ( files [ 0 ], filesystem = S3FileSystem ())) return return_materialized_stream ( df ) try : f = pq . ParquetFile ( S3FileSystem () . open_input_file ( files [ 0 ])) schema = [ k . name for k in f . schema_arrow ] except : raise Exception ( \"schema discovery failed for Parquet dataset at location {} . Please raise Github issue.\" . format ( table_location )) token = ray . get ( self . catalog . register_s3_parquet_source . remote ( files [ 0 ], len ( sizes ))) self . nodes [ self . latest_node_id ] = InputS3ParquetNode ( files , schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : try : f = pq . ParquetFile ( S3FileSystem () . open_input_file ( table_location )) schema = [ k . name for k in f . schema_arrow ] except : raise Exception ( \"\"\"schema discovery failed for Parquet dataset at location {} . Note if you are specifying a prefix to many parquet files, must use asterix. E.g. qc.read_parquet(\"s3://rottnest/happy.parquet/*\")\"\"\" . format ( table_location )) key = \"/\" . join ( table_location . split ( \"/\" )[ 1 :]) response = s3 . head_object ( Bucket = bucket , Key = key ) size = response [ 'ContentLength' ] if size < self . sql_config [ \"s3_parquet_materialize_threshold\" ]: df = polars . from_arrow ( pq . read_table ( table_location , filesystem = S3FileSystem ())) return return_materialized_stream ( df ) token = ray . get ( self . catalog . register_s3_parquet_source . remote ( bucket + \"/\" + key , 1 )) self . nodes [ self . latest_node_id ] = InputS3ParquetNode ([ table_location ], schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) # self.nodes[self.latest_node_id].set_placement_strategy(CustomChannelsStrategy(2)) else : if type ( self . cluster ) == EC2Cluster : raise NotImplementedError ( \"Does not support reading local dataset with S3 cluster. Must use S3 bucket.\" ) if \"*\" in table_location : table_location = table_location [: - 1 ] assert table_location [ - 1 ] == \"/\" , \"must specify * with entire directory, doesn't support prefixes yet\" try : files = [ i for i in os . listdir ( table_location ) if i . endswith ( \".parquet\" )] except : raise Exception ( \"Tried to get list of parquet files at \" , table_location , \" failed. Make sure specify absolute path and filenames end with .parquet\" ) assert len ( files ) > 0 f = pq . ParquetFile ( table_location + files [ 0 ]) schema = [ k . name for k in f . schema_arrow ] if len ( files ) == 1 and os . path . getsize ( table_location + files [ 0 ]) < self . sql_config [ \"disk_parquet_materialize_threshold\" ]: df = polars . read_parquet ( table_location + files [ 0 ]) return return_materialized_stream ( df ) token = ray . get ( self . catalog . register_disk_parquet_source . remote ( table_location )) self . nodes [ self . latest_node_id ] = InputDiskParquetNode ( table_location , schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) else : try : size = os . path . getsize ( table_location ) except : raise Exception ( \"could not find the parquet file at \" , table_location ) if size < self . sql_config [ \"disk_parquet_materialize_threshold\" ]: df = polars . read_parquet ( table_location ) return return_materialized_stream ( df ) f = pq . ParquetFile ( table_location ) schema = [ k . name for k in f . schema_arrow ] token = ray . get ( self . catalog . register_disk_parquet_source . remote ( table_location )) self . nodes [ self . latest_node_id ] = InputDiskParquetNode ( table_location , schema ) self . nodes [ self . latest_node_id ] . set_catalog_id ( token ) self . latest_node_id += 1 return DataStream ( self , schema , self . latest_node_id - 1 )","title":"QuokkaContext.read_parquet"},{"location":"quokka_context/read_ray_dataset/","text":"QuokkaContext.read_ray_dataset Under construction.","title":"QuokkaContext.read_ray_dataset"},{"location":"quokka_context/read_ray_dataset/#quokkacontextread_ray_dataset","text":"Under construction.","title":"QuokkaContext.read_ray_dataset"},{"location":"quokka_context/set_config/","text":"QuokkaContext.set_config This sets a config value for the entire cluster. You should do this at the very start of your program generally speaking. The following keys are supported: optimize_joins: bool, whether to optimize joins based on cardinality estimates. Default to True s3_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file in S3 disk_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file on disk s3_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file in S3 disk_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file on disk hbq_path: str, the disk spill directory. Default to \"/data\" fault_tolerance: bool, whether to enable fault tolerance. Default to False Parameters: Name Type Description Default key str the key to set required value any the value to set required Returns: Type Description None Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext () Turn on join order optimization. >>> qc . set_config ( \"optimize_joins\" , True ) Turn off fault tolerance. >>> qc . set_config ( \"fault_tolerance\" , False ) Source code in pyquokka/df.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def set_config ( self , key , value ): \"\"\" This sets a config value for the entire cluster. You should do this at the very start of your program generally speaking. The following keys are supported: 1. optimize_joins: bool, whether to optimize joins based on cardinality estimates. Default to True 2. s3_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file in S3 3. disk_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file on disk 4. s3_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file in S3 5. disk_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file on disk 6. hbq_path: str, the disk spill directory. Default to \"/data\" 7. fault_tolerance: bool, whether to enable fault tolerance. Default to False Args: key (str): the key to set value (any): the value to set Returns: None Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext() Turn on join order optimization. >>> qc.set_config(\"optimize_joins\", True) Turn off fault tolerance. >>> qc.set_config(\"fault_tolerance\", False) \"\"\" if key in self . sql_config : self . sql_config [ key ] = value elif key in self . exec_config : self . exec_config [ key ] = value assert all ( ray . get ([ task_manager . set_config . remote ( key , value ) for task_manager in self . task_managers . values ()])) else : raise Exception ( \"key not found in config\" )","title":"QuokkaContext.set_config"},{"location":"quokka_context/set_config/#quokkacontextset_config","text":"This sets a config value for the entire cluster. You should do this at the very start of your program generally speaking. The following keys are supported: optimize_joins: bool, whether to optimize joins based on cardinality estimates. Default to True s3_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file in S3 disk_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file on disk s3_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file in S3 disk_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file on disk hbq_path: str, the disk spill directory. Default to \"/data\" fault_tolerance: bool, whether to enable fault tolerance. Default to False Parameters: Name Type Description Default key str the key to set required value any the value to set required Returns: Type Description None Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext () Turn on join order optimization. >>> qc . set_config ( \"optimize_joins\" , True ) Turn off fault tolerance. >>> qc . set_config ( \"fault_tolerance\" , False ) Source code in pyquokka/df.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def set_config ( self , key , value ): \"\"\" This sets a config value for the entire cluster. You should do this at the very start of your program generally speaking. The following keys are supported: 1. optimize_joins: bool, whether to optimize joins based on cardinality estimates. Default to True 2. s3_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file in S3 3. disk_csv_materialize_threshold: int, the threshold in bytes for when to materialize a CSV file on disk 4. s3_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file in S3 5. disk_parquet_materialize_threshold: int, the threshold in bytes for when to materialize a Parquet file on disk 6. hbq_path: str, the disk spill directory. Default to \"/data\" 7. fault_tolerance: bool, whether to enable fault tolerance. Default to False Args: key (str): the key to set value (any): the value to set Returns: None Examples: >>> from pyquokka.df import * >>> qc = QuokkaContext() Turn on join order optimization. >>> qc.set_config(\"optimize_joins\", True) Turn off fault tolerance. >>> qc.set_config(\"fault_tolerance\", False) \"\"\" if key in self . sql_config : self . sql_config [ key ] = value elif key in self . exec_config : self . exec_config [ key ] = value assert all ( ray . get ([ task_manager . set_config . remote ( key , value ) for task_manager in self . task_managers . values ()])) else : raise Exception ( \"key not found in config\" )","title":"QuokkaContext.set_config"},{"location":"utils/create_cluster/","text":"QuokkaClusterManager.create_cluster Create a Ray cluster configured to run Quokka applications. Parameters: Name Type Description Default aws_access_key str AWS access key. required aws_access_id str AWS access id. required num_instances int Number of instances to create. required instance_type str Instance type to use, defaults to i3.2xlarge. 'i3.2xlarge' ami str AMI to use, defaults to \"ami-0530ca8899fac469f\", which is us-west-2 ubuntu 20.04. Please change accordingly for your region and OS. 'ami-0530ca8899fac469f' requirements list List of requirements to install on cluster, defaults to empty list. [] spill_dir str Directory to use for spill directory, defaults to \"/data\". Quokka will detect if your instance have NVME SSD and mount it to this directory. '/data' Return EC2Cluster object. See EC2Cluster for more details. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) >>> df = qc . read_csv ( \"s3://my_bucket/my_file.csv\" ) Source code in pyquokka/utils.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def create_cluster ( self , aws_access_key , aws_access_id , num_instances , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [], spill_dir = \"/data\" , volume_size = 8 ): \"\"\" Create a Ray cluster configured to run Quokka applications. Args: aws_access_key (str): AWS access key. aws_access_id (str): AWS access id. num_instances (int): Number of instances to create. instance_type (str): Instance type to use, defaults to i3.2xlarge. ami (str): AMI to use, defaults to \"ami-0530ca8899fac469f\", which is us-west-2 ubuntu 20.04. Please change accordingly for your region and OS. requirements (list): List of requirements to install on cluster, defaults to empty list. spill_dir (str): Directory to use for spill directory, defaults to \"/data\". Quokka will detect if your instance have NVME SSD and mount it to this directory. Return: EC2Cluster object. See EC2Cluster for more details. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) >>> df = qc.read_csv(\"s3://my_bucket/my_file.csv\") \"\"\" start_time = time . time () ec2 = boto3 . client ( \"ec2\" ) vcpu_per_node = ec2 . describe_instance_types ( InstanceTypes = [ instance_type ])[ 'InstanceTypes' ][ 0 ][ 'VCpuInfo' ][ 'DefaultVCpus' ] waiter = ec2 . get_waiter ( 'instance_running' ) res = ec2 . run_instances ( BlockDeviceMappings = [ { 'DeviceName' : ec2 . describe_images ( ImageIds = [ ami ])[ 'Images' ][ 0 ][ 'RootDeviceName' ], 'Ebs' : { 'DeleteOnTermination' : True , 'VolumeSize' : volume_size , 'VolumeType' : 'gp3' } } ], ImageId = ami , InstanceType = instance_type , SecurityGroupIds = [ self . security_group ], KeyName = self . key_name , MaxCount = num_instances , MinCount = num_instances ) instance_ids = [ res [ 'Instances' ][ i ][ 'InstanceId' ] for i in range ( num_instances )] waiter . wait ( InstanceIds = instance_ids ) a = ec2 . describe_instances ( InstanceIds = instance_ids ) public_ips = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'PublicIpAddress' ] for i in range ( num_instances )] private_ips = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'PrivateIpAddress' ] for i in range ( num_instances )] self . check_instance_alive ( public_ips ) self . set_up_envs ( public_ips , requirements , aws_access_key , aws_access_id ) self . launch_all ( \"sudo mkdir {} \" . format ( spill_dir ), public_ips , \"failed to make temp spill directory\" ) self . _initialize_instances ( instance_ids , spill_dir ) print ( \"Launching of Quokka cluster used: \" , time . time () - start_time ) return EC2Cluster ( public_ips , private_ips , instance_ids , vcpu_per_node , spill_dir )","title":"QuokkaClusterManager.create_cluster"},{"location":"utils/create_cluster/#quokkaclustermanagercreate_cluster","text":"Create a Ray cluster configured to run Quokka applications. Parameters: Name Type Description Default aws_access_key str AWS access key. required aws_access_id str AWS access id. required num_instances int Number of instances to create. required instance_type str Instance type to use, defaults to i3.2xlarge. 'i3.2xlarge' ami str AMI to use, defaults to \"ami-0530ca8899fac469f\", which is us-west-2 ubuntu 20.04. Please change accordingly for your region and OS. 'ami-0530ca8899fac469f' requirements list List of requirements to install on cluster, defaults to empty list. [] spill_dir str Directory to use for spill directory, defaults to \"/data\". Quokka will detect if your instance have NVME SSD and mount it to this directory. '/data' Return EC2Cluster object. See EC2Cluster for more details. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) >>> df = qc . read_csv ( \"s3://my_bucket/my_file.csv\" ) Source code in pyquokka/utils.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def create_cluster ( self , aws_access_key , aws_access_id , num_instances , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [], spill_dir = \"/data\" , volume_size = 8 ): \"\"\" Create a Ray cluster configured to run Quokka applications. Args: aws_access_key (str): AWS access key. aws_access_id (str): AWS access id. num_instances (int): Number of instances to create. instance_type (str): Instance type to use, defaults to i3.2xlarge. ami (str): AMI to use, defaults to \"ami-0530ca8899fac469f\", which is us-west-2 ubuntu 20.04. Please change accordingly for your region and OS. requirements (list): List of requirements to install on cluster, defaults to empty list. spill_dir (str): Directory to use for spill directory, defaults to \"/data\". Quokka will detect if your instance have NVME SSD and mount it to this directory. Return: EC2Cluster object. See EC2Cluster for more details. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) >>> df = qc.read_csv(\"s3://my_bucket/my_file.csv\") \"\"\" start_time = time . time () ec2 = boto3 . client ( \"ec2\" ) vcpu_per_node = ec2 . describe_instance_types ( InstanceTypes = [ instance_type ])[ 'InstanceTypes' ][ 0 ][ 'VCpuInfo' ][ 'DefaultVCpus' ] waiter = ec2 . get_waiter ( 'instance_running' ) res = ec2 . run_instances ( BlockDeviceMappings = [ { 'DeviceName' : ec2 . describe_images ( ImageIds = [ ami ])[ 'Images' ][ 0 ][ 'RootDeviceName' ], 'Ebs' : { 'DeleteOnTermination' : True , 'VolumeSize' : volume_size , 'VolumeType' : 'gp3' } } ], ImageId = ami , InstanceType = instance_type , SecurityGroupIds = [ self . security_group ], KeyName = self . key_name , MaxCount = num_instances , MinCount = num_instances ) instance_ids = [ res [ 'Instances' ][ i ][ 'InstanceId' ] for i in range ( num_instances )] waiter . wait ( InstanceIds = instance_ids ) a = ec2 . describe_instances ( InstanceIds = instance_ids ) public_ips = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'PublicIpAddress' ] for i in range ( num_instances )] private_ips = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'PrivateIpAddress' ] for i in range ( num_instances )] self . check_instance_alive ( public_ips ) self . set_up_envs ( public_ips , requirements , aws_access_key , aws_access_id ) self . launch_all ( \"sudo mkdir {} \" . format ( spill_dir ), public_ips , \"failed to make temp spill directory\" ) self . _initialize_instances ( instance_ids , spill_dir ) print ( \"Launching of Quokka cluster used: \" , time . time () - start_time ) return EC2Cluster ( public_ips , private_ips , instance_ids , vcpu_per_node , spill_dir )","title":"QuokkaClusterManager.create_cluster"},{"location":"utils/ec2cluster_to_json/","text":"EC2Cluster.to_json Creates JSON representation of this cluster that can be used to connect to the cluster again. Parameters: Name Type Description Default output str Path to output file. Defaults to \"cluster.json\". 'cluster.json' Return None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . from_json ( \"my_cluster.json\" ) Source code in pyquokka/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def to_json ( self , output = \"cluster.json\" ): \"\"\" Creates JSON representation of this cluster that can be used to connect to the cluster again. Args: output (str, optional): Path to output file. Defaults to \"cluster.json\". Return: None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.from_json(\"my_cluster.json\") \"\"\" json . dump ({ \"instance_ids\" : self . instance_ids , \"cpu_count_per_instance\" : self . cpu_count , \"spill_dir\" : self . spill_dir , \"tags\" : self . tags }, open ( output , \"w\" ))","title":"EC2Cluster.to_json"},{"location":"utils/ec2cluster_to_json/#ec2clusterto_json","text":"Creates JSON representation of this cluster that can be used to connect to the cluster again. Parameters: Name Type Description Default output str Path to output file. Defaults to \"cluster.json\". 'cluster.json' Return None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . from_json ( \"my_cluster.json\" ) Source code in pyquokka/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def to_json ( self , output = \"cluster.json\" ): \"\"\" Creates JSON representation of this cluster that can be used to connect to the cluster again. Args: output (str, optional): Path to output file. Defaults to \"cluster.json\". Return: None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.from_json(\"my_cluster.json\") \"\"\" json . dump ({ \"instance_ids\" : self . instance_ids , \"cpu_count_per_instance\" : self . cpu_count , \"spill_dir\" : self . spill_dir , \"tags\" : self . tags }, open ( output , \"w\" ))","title":"EC2Cluster.to_json"},{"location":"utils/get_cluster_from_json/","text":"QuokkaClusterManager.get_cluster_from_json Get an EC2Cluster object from a json file. The json file must have been created by EC2Cluster.to_json . This will restart the cluster if all the instances have been stopped and set up the Quokka runtime. If the cluster is running, the Quokka runtime will not be set up again. So this will break if you manually turned on the instances. Parameters: Name Type Description Default json_file str Path to json file, must have been created by EC2Cluster.to_json . You can also manually create this json based on the format of the json file created by EC2Cluster.to_json , but this is not recommended. required Return EC2Cluster: Cluster object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . get_cluster_from_json ( \"my_cluster.json\" ) >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) >>> df = qc . read_csv ( \"s3://my_bucket/my_file.csv\" ) Source code in pyquokka/utils.py 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 def get_cluster_from_json ( self , json_file ): \"\"\" Get an EC2Cluster object from a json file. The json file must have been created by `EC2Cluster.to_json`. This will restart the cluster if all the instances have been stopped and set up the Quokka runtime. If the cluster is running, the Quokka runtime will not be set up again. So this will break if you manually turned on the instances. Args: json_file (str): Path to json file, must have been created by `EC2Cluster.to_json`. You can also manually create this json based on the format of the json file created by `EC2Cluster.to_json`, but this is not recommended. Return: EC2Cluster: Cluster object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.get_cluster_from_json(\"my_cluster.json\") >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) >>> df = qc.read_csv(\"s3://my_bucket/my_file.csv\") \"\"\" ec2 = boto3 . client ( \"ec2\" ) stuff = json . load ( open ( json_file , \"r\" )) cpu_count = int ( stuff [ \"cpu_count_per_instance\" ]) spill_dir = stuff [ \"spill_dir\" ] instance_ids = self . str_key_to_int ( stuff [ \"instance_ids\" ]) instance_ids = [ instance_ids [ i ] for i in range ( len ( instance_ids ))] a = ec2 . describe_instances ( InstanceIds = instance_ids ) states = [ k [ 'State' ][ 'Name' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] if sum ([ i == \"stopped\" for i in states ]) == len ( states ): print ( \"Cluster is stopped. Run quokkactl start_cluster to start the cluster.\" ) if sum ([ i == \"running\" for i in states ]) == len ( states ): request_instance_ids = [ k [ 'InstanceId' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] public_ips = [ k [ 'PublicIpAddress' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] private_ips = [ k [ 'PrivateIpAddress' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] # figure out where in request_instance_ids is instance_ids[0] leader_index = request_instance_ids . index ( instance_ids [ 0 ]) assert leader_index != - 1 , \"Leader instance not found in request_instance_ids\" public_ips = public_ips [ leader_index :] + public_ips [: leader_index ] private_ips = private_ips [ leader_index :] + private_ips [: leader_index ] return EC2Cluster ( public_ips , private_ips , instance_ids , cpu_count , spill_dir , tags = stuff [ 'tags' ] if 'tags' in stuff else {}) else : print ( \"Cluster in an inconsistent state. Either only some machines are running or some machines have been terminated.\" ) return False","title":"QuokkaClusterManager.get_cluster_from_json"},{"location":"utils/get_cluster_from_json/#quokkaclustermanagerget_cluster_from_json","text":"Get an EC2Cluster object from a json file. The json file must have been created by EC2Cluster.to_json . This will restart the cluster if all the instances have been stopped and set up the Quokka runtime. If the cluster is running, the Quokka runtime will not be set up again. So this will break if you manually turned on the instances. Parameters: Name Type Description Default json_file str Path to json file, must have been created by EC2Cluster.to_json . You can also manually create this json based on the format of the json file created by EC2Cluster.to_json , but this is not recommended. required Return EC2Cluster: Cluster object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . get_cluster_from_json ( \"my_cluster.json\" ) >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) >>> df = qc . read_csv ( \"s3://my_bucket/my_file.csv\" ) Source code in pyquokka/utils.py 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 def get_cluster_from_json ( self , json_file ): \"\"\" Get an EC2Cluster object from a json file. The json file must have been created by `EC2Cluster.to_json`. This will restart the cluster if all the instances have been stopped and set up the Quokka runtime. If the cluster is running, the Quokka runtime will not be set up again. So this will break if you manually turned on the instances. Args: json_file (str): Path to json file, must have been created by `EC2Cluster.to_json`. You can also manually create this json based on the format of the json file created by `EC2Cluster.to_json`, but this is not recommended. Return: EC2Cluster: Cluster object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.get_cluster_from_json(\"my_cluster.json\") >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) >>> df = qc.read_csv(\"s3://my_bucket/my_file.csv\") \"\"\" ec2 = boto3 . client ( \"ec2\" ) stuff = json . load ( open ( json_file , \"r\" )) cpu_count = int ( stuff [ \"cpu_count_per_instance\" ]) spill_dir = stuff [ \"spill_dir\" ] instance_ids = self . str_key_to_int ( stuff [ \"instance_ids\" ]) instance_ids = [ instance_ids [ i ] for i in range ( len ( instance_ids ))] a = ec2 . describe_instances ( InstanceIds = instance_ids ) states = [ k [ 'State' ][ 'Name' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] if sum ([ i == \"stopped\" for i in states ]) == len ( states ): print ( \"Cluster is stopped. Run quokkactl start_cluster to start the cluster.\" ) if sum ([ i == \"running\" for i in states ]) == len ( states ): request_instance_ids = [ k [ 'InstanceId' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] public_ips = [ k [ 'PublicIpAddress' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] private_ips = [ k [ 'PrivateIpAddress' ] for reservation in a [ 'Reservations' ] for k in reservation [ 'Instances' ]] # figure out where in request_instance_ids is instance_ids[0] leader_index = request_instance_ids . index ( instance_ids [ 0 ]) assert leader_index != - 1 , \"Leader instance not found in request_instance_ids\" public_ips = public_ips [ leader_index :] + public_ips [: leader_index ] private_ips = private_ips [ leader_index :] + private_ips [: leader_index ] return EC2Cluster ( public_ips , private_ips , instance_ids , cpu_count , spill_dir , tags = stuff [ 'tags' ] if 'tags' in stuff else {}) else : print ( \"Cluster in an inconsistent state. Either only some machines are running or some machines have been terminated.\" ) return False","title":"QuokkaClusterManager.get_cluster_from_json"},{"location":"utils/localcluster/","text":"LocalCluster Source code in pyquokka/utils.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class LocalCluster : def __init__ ( self ) -> None : \"\"\" Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because `QuokkaContext` will automatically make one for you. Return: LocalCluster: A LocalCluster object. Examples: >>> from pyquokka.utils import * >>> cluster = LocalCluster() >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) But the following is equivalent: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() \"\"\" print ( \"Initializing local Quokka cluster.\" ) self . num_node = 1 self . tags = {} self . cpu_count = multiprocessing . cpu_count () pyquokka_loc = pyquokka . __file__ . replace ( \"__init__.py\" , \"\" ) # we assume you have pyquokka installed, and we are going to spin up a ray cluster locally ray . init ( ignore_reinit_error = True ) flight_file = pyquokka_loc + \"/flight.py\" self . flight_process = None self . redis_process = None os . system ( \"export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288\" ) port5005 = os . popen ( \"lsof -i:5005\" ) . read () if \"python\" in port5005 : raise Exception ( \"Port 5005 is already in use. Kill the process that is using it first.\" ) try : self . flight_process = subprocess . Popen ([ \"python3\" , flight_file ], preexec_fn = preexec_function ) except : raise Exception ( \"Could not start flight server properly. Check if there is already something using port 5005, kill it if necessary. Use lsof -i:5005\" ) self . redis_process = subprocess . Popen ([ \"redis-server\" , pyquokka_loc + \"redis.conf\" , \"--port 6800\" , \"--protected-mode no\" ], preexec_fn = preexec_function ) self . leader_public_ip = \"localhost\" self . leader_private_ip = ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ] self . public_ips = { 0 : \"localhost\" } self . private_ips = { 0 : ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ]} print ( \"Finished setting up local Quokka cluster.\" ) def __del__ ( self ): # we need to join the process that is running the flight server! if self . flight_process is not None : self . flight_process . kill () if self . redis_process is not None : self . redis_process . kill () __init__ () Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because QuokkaContext will automatically make one for you. Return LocalCluster: A LocalCluster object. Examples: >>> from pyquokka.utils import * >>> cluster = LocalCluster () >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) But the following is equivalent: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () Source code in pyquokka/utils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self ) -> None : \"\"\" Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because `QuokkaContext` will automatically make one for you. Return: LocalCluster: A LocalCluster object. Examples: >>> from pyquokka.utils import * >>> cluster = LocalCluster() >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) But the following is equivalent: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() \"\"\" print ( \"Initializing local Quokka cluster.\" ) self . num_node = 1 self . tags = {} self . cpu_count = multiprocessing . cpu_count () pyquokka_loc = pyquokka . __file__ . replace ( \"__init__.py\" , \"\" ) # we assume you have pyquokka installed, and we are going to spin up a ray cluster locally ray . init ( ignore_reinit_error = True ) flight_file = pyquokka_loc + \"/flight.py\" self . flight_process = None self . redis_process = None os . system ( \"export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288\" ) port5005 = os . popen ( \"lsof -i:5005\" ) . read () if \"python\" in port5005 : raise Exception ( \"Port 5005 is already in use. Kill the process that is using it first.\" ) try : self . flight_process = subprocess . Popen ([ \"python3\" , flight_file ], preexec_fn = preexec_function ) except : raise Exception ( \"Could not start flight server properly. Check if there is already something using port 5005, kill it if necessary. Use lsof -i:5005\" ) self . redis_process = subprocess . Popen ([ \"redis-server\" , pyquokka_loc + \"redis.conf\" , \"--port 6800\" , \"--protected-mode no\" ], preexec_fn = preexec_function ) self . leader_public_ip = \"localhost\" self . leader_private_ip = ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ] self . public_ips = { 0 : \"localhost\" } self . private_ips = { 0 : ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ]} print ( \"Finished setting up local Quokka cluster.\" )","title":"LocalCluster"},{"location":"utils/localcluster/#localcluster","text":"Source code in pyquokka/utils.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class LocalCluster : def __init__ ( self ) -> None : \"\"\" Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because `QuokkaContext` will automatically make one for you. Return: LocalCluster: A LocalCluster object. Examples: >>> from pyquokka.utils import * >>> cluster = LocalCluster() >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) But the following is equivalent: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() \"\"\" print ( \"Initializing local Quokka cluster.\" ) self . num_node = 1 self . tags = {} self . cpu_count = multiprocessing . cpu_count () pyquokka_loc = pyquokka . __file__ . replace ( \"__init__.py\" , \"\" ) # we assume you have pyquokka installed, and we are going to spin up a ray cluster locally ray . init ( ignore_reinit_error = True ) flight_file = pyquokka_loc + \"/flight.py\" self . flight_process = None self . redis_process = None os . system ( \"export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288\" ) port5005 = os . popen ( \"lsof -i:5005\" ) . read () if \"python\" in port5005 : raise Exception ( \"Port 5005 is already in use. Kill the process that is using it first.\" ) try : self . flight_process = subprocess . Popen ([ \"python3\" , flight_file ], preexec_fn = preexec_function ) except : raise Exception ( \"Could not start flight server properly. Check if there is already something using port 5005, kill it if necessary. Use lsof -i:5005\" ) self . redis_process = subprocess . Popen ([ \"redis-server\" , pyquokka_loc + \"redis.conf\" , \"--port 6800\" , \"--protected-mode no\" ], preexec_fn = preexec_function ) self . leader_public_ip = \"localhost\" self . leader_private_ip = ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ] self . public_ips = { 0 : \"localhost\" } self . private_ips = { 0 : ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ]} print ( \"Finished setting up local Quokka cluster.\" ) def __del__ ( self ): # we need to join the process that is running the flight server! if self . flight_process is not None : self . flight_process . kill () if self . redis_process is not None : self . redis_process . kill ()","title":"LocalCluster"},{"location":"utils/localcluster/#pyquokka.utils.LocalCluster.__init__","text":"Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because QuokkaContext will automatically make one for you. Return LocalCluster: A LocalCluster object. Examples: >>> from pyquokka.utils import * >>> cluster = LocalCluster () >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext ( cluster ) But the following is equivalent: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext () Source code in pyquokka/utils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self ) -> None : \"\"\" Creates a local cluster on your machine. This is useful for testing purposes. This not should be necessary because `QuokkaContext` will automatically make one for you. Return: LocalCluster: A LocalCluster object. Examples: >>> from pyquokka.utils import * >>> cluster = LocalCluster() >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext(cluster) But the following is equivalent: >>> from pyquokka.df import QuokkaContext >>> qc = QuokkaContext() \"\"\" print ( \"Initializing local Quokka cluster.\" ) self . num_node = 1 self . tags = {} self . cpu_count = multiprocessing . cpu_count () pyquokka_loc = pyquokka . __file__ . replace ( \"__init__.py\" , \"\" ) # we assume you have pyquokka installed, and we are going to spin up a ray cluster locally ray . init ( ignore_reinit_error = True ) flight_file = pyquokka_loc + \"/flight.py\" self . flight_process = None self . redis_process = None os . system ( \"export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288\" ) port5005 = os . popen ( \"lsof -i:5005\" ) . read () if \"python\" in port5005 : raise Exception ( \"Port 5005 is already in use. Kill the process that is using it first.\" ) try : self . flight_process = subprocess . Popen ([ \"python3\" , flight_file ], preexec_fn = preexec_function ) except : raise Exception ( \"Could not start flight server properly. Check if there is already something using port 5005, kill it if necessary. Use lsof -i:5005\" ) self . redis_process = subprocess . Popen ([ \"redis-server\" , pyquokka_loc + \"redis.conf\" , \"--port 6800\" , \"--protected-mode no\" ], preexec_fn = preexec_function ) self . leader_public_ip = \"localhost\" self . leader_private_ip = ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ] self . public_ips = { 0 : \"localhost\" } self . private_ips = { 0 : ray . get_runtime_context () . gcs_address . split ( \":\" )[ 0 ]} print ( \"Finished setting up local Quokka cluster.\" )","title":"__init__()"},{"location":"utils/quokka_cluster_manager/","text":"QuokkaClusterManager Create a QuokkaClusterManager object. This object is used to create Ray clusters on AWS EC2 configured with Quokka or connecting to existing Ray clusters. This requires you to have an AWS key pair for logging into instances. Parameters: Name Type Description Default key_name str The name of the key pair to use. This is a required argument if you want to call create_cluster . None key_location str The location of the key pair to use. You must specify this argument. None security_group str The security group to use. None Return QuokkaClusterManager: A QuokkaClusterManager object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . from_json ( \"my_cluster.json\" ) Source code in pyquokka/utils.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def __init__ ( self , key_name = None , key_location = None , security_group = None ) -> None : \"\"\" Create a QuokkaClusterManager object. This object is used to create Ray clusters on AWS EC2 configured with Quokka or connecting to existing Ray clusters. This requires you to have an AWS key pair for logging into instances. Args: key_name (str, optional): The name of the key pair to use. This is a required argument if you want to call `create_cluster`. key_location (str, optional): The location of the key pair to use. You must specify this argument. security_group (str, optional): The security group to use. Return: QuokkaClusterManager: A QuokkaClusterManager object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.from_json(\"my_cluster.json\") \"\"\" assert key_location is not None self . key_name = key_name self . key_location = key_location self . security_group = security_group","title":"QuokkaClusterManager"},{"location":"utils/quokka_cluster_manager/#quokkaclustermanager","text":"Create a QuokkaClusterManager object. This object is used to create Ray clusters on AWS EC2 configured with Quokka or connecting to existing Ray clusters. This requires you to have an AWS key pair for logging into instances. Parameters: Name Type Description Default key_name str The name of the key pair to use. This is a required argument if you want to call create_cluster . None key_location str The location of the key pair to use. You must specify this argument. None security_group str The security group to use. None Return QuokkaClusterManager: A QuokkaClusterManager object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager ( key_name = \"my_key\" , key_location = \"/home/ubuntu/.ssh/my_key.pem\" , security_group = \"my_security_group\" ) >>> cluster = manager . from_json ( \"my_cluster.json\" ) Source code in pyquokka/utils.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def __init__ ( self , key_name = None , key_location = None , security_group = None ) -> None : \"\"\" Create a QuokkaClusterManager object. This object is used to create Ray clusters on AWS EC2 configured with Quokka or connecting to existing Ray clusters. This requires you to have an AWS key pair for logging into instances. Args: key_name (str, optional): The name of the key pair to use. This is a required argument if you want to call `create_cluster`. key_location (str, optional): The location of the key pair to use. You must specify this argument. security_group (str, optional): The security group to use. Return: QuokkaClusterManager: A QuokkaClusterManager object. Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") You can now close this Python session. In a new Python session you can connect to this cluster by doing: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager(key_name = \"my_key\", key_location = \"/home/ubuntu/.ssh/my_key.pem\", security_group = \"my_security_group\") >>> cluster = manager.from_json(\"my_cluster.json\") \"\"\" assert key_location is not None self . key_name = key_name self . key_location = key_location self . security_group = security_group","title":"QuokkaClusterManager"},{"location":"utils/start_cluster/","text":"QuokkaClusterManager.stop_cluster Source code in pyquokka/utils.py 370 371 372 373 374 375 376 377 def start_cluster ( self , cluster_json ): ec2 = boto3 . client ( \"ec2\" ) with open ( cluster_json , \"r\" ) as f : cluster_info = json . load ( f ) instance_ids = list ( cluster_info [ 'instance_ids' ] . values ()) ec2 . start_instances ( InstanceIds = instance_ids ) self . _initialize_instances ( instance_ids , cluster_info [ \"spill_dir\" ])","title":"QuokkaClusterManager.stop_cluster"},{"location":"utils/start_cluster/#quokkaclustermanagerstop_cluster","text":"Source code in pyquokka/utils.py 370 371 372 373 374 375 376 377 def start_cluster ( self , cluster_json ): ec2 = boto3 . client ( \"ec2\" ) with open ( cluster_json , \"r\" ) as f : cluster_info = json . load ( f ) instance_ids = list ( cluster_info [ 'instance_ids' ] . values ()) ec2 . start_instances ( InstanceIds = instance_ids ) self . _initialize_instances ( instance_ids , cluster_info [ \"spill_dir\" ])","title":"QuokkaClusterManager.stop_cluster"},{"location":"utils/stop_cluster/","text":"stop_cluster Stops a cluster, does not terminate it. If the cluster had been saved to json, can use get_cluster_from_json to restart the cluster. Parameters: Name Type Description Default quokka_cluster EC2Cluster Cluster to stop. required Return None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) >>> manager . stop_cluster ( cluster ) Source code in pyquokka/utils.py 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def stop_cluster ( cluster_json ): \"\"\" Stops a cluster, does not terminate it. If the cluster had been saved to json, can use `get_cluster_from_json` to restart the cluster. Args: quokka_cluster (EC2Cluster): Cluster to stop. Return: None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") >>> manager.stop_cluster(cluster) \"\"\" ec2 = boto3 . client ( \"ec2\" ) with open ( cluster_json , \"r\" ) as f : cluster_info = json . load ( f ) instance_ids = list ( cluster_info [ 'instance_ids' ] . values ()) ec2 . stop_instances ( InstanceIds = instance_ids ) while True : time . sleep ( 0.1 ) a = ec2 . describe_instances ( InstanceIds = instance_ids ) states = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'State' ][ 'Name' ] for i in range ( len ( instance_ids ))] if \"running\" in states : continue else : break","title":"QuokkaClusterManager.stop_cluster"},{"location":"utils/stop_cluster/#stop_cluster","text":"Stops a cluster, does not terminate it. If the cluster had been saved to json, can use get_cluster_from_json to restart the cluster. Parameters: Name Type Description Default quokka_cluster EC2Cluster Cluster to stop. required Return None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> cluster . to_json ( \"my_cluster.json\" ) >>> manager . stop_cluster ( cluster ) Source code in pyquokka/utils.py 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def stop_cluster ( cluster_json ): \"\"\" Stops a cluster, does not terminate it. If the cluster had been saved to json, can use `get_cluster_from_json` to restart the cluster. Args: quokka_cluster (EC2Cluster): Cluster to stop. Return: None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> cluster.to_json(\"my_cluster.json\") >>> manager.stop_cluster(cluster) \"\"\" ec2 = boto3 . client ( \"ec2\" ) with open ( cluster_json , \"r\" ) as f : cluster_info = json . load ( f ) instance_ids = list ( cluster_info [ 'instance_ids' ] . values ()) ec2 . stop_instances ( InstanceIds = instance_ids ) while True : time . sleep ( 0.1 ) a = ec2 . describe_instances ( InstanceIds = instance_ids ) states = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'State' ][ 'Name' ] for i in range ( len ( instance_ids ))] if \"running\" in states : continue else : break","title":"stop_cluster"},{"location":"utils/terminate_cluster/","text":"terminate_cluster Terminate a cluster. Parameters: Name Type Description Default quokka_cluster EC2Cluster Cluster to terminate. required Return None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> manager . terminate_cluster ( cluster ) Source code in pyquokka/utils.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 def terminate_cluster ( cluster_json ): \"\"\" Terminate a cluster. Args: quokka_cluster (EC2Cluster): Cluster to terminate. Return: None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> manager.terminate_cluster(cluster) \"\"\" ec2 = boto3 . client ( \"ec2\" ) with open ( cluster_json , \"r\" ) as f : cluster_info = json . load ( f ) instance_ids = list ( cluster_info [ 'instance_ids' ] . values ()) ec2 . terminate_instances ( InstanceIds = instance_ids ) ec2 . terminate_instances ( InstanceIds = instance_ids ) while True : time . sleep ( 0.1 ) a = ec2 . describe_instances ( InstanceIds = instance_ids ) states = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'State' ][ 'Name' ] for i in range ( len ( instance_ids ))] if \"running\" in states : continue else : break os . remove ( cluster_json )","title":"QuokkaClusterManager.terminate_cluster"},{"location":"utils/terminate_cluster/#terminate_cluster","text":"Terminate a cluster. Parameters: Name Type Description Default quokka_cluster EC2Cluster Cluster to terminate. required Return None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager () >>> cluster = manager . create_cluster ( aws_access_key , aws_access_id , num_instances = 2 , instance_type = \"i3.2xlarge\" , ami = \"ami-0530ca8899fac469f\" , requirements = [ \"numpy\" , \"pandas\" ]) >>> manager . terminate_cluster ( cluster ) Source code in pyquokka/utils.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 def terminate_cluster ( cluster_json ): \"\"\" Terminate a cluster. Args: quokka_cluster (EC2Cluster): Cluster to terminate. Return: None Examples: >>> from pyquokka.utils import * >>> manager = QuokkaClusterManager() >>> cluster = manager.create_cluster(aws_access_key, aws_access_id, num_instances = 2, instance_type = \"i3.2xlarge\", ami=\"ami-0530ca8899fac469f\", requirements = [\"numpy\", \"pandas\"]) >>> manager.terminate_cluster(cluster) \"\"\" ec2 = boto3 . client ( \"ec2\" ) with open ( cluster_json , \"r\" ) as f : cluster_info = json . load ( f ) instance_ids = list ( cluster_info [ 'instance_ids' ] . values ()) ec2 . terminate_instances ( InstanceIds = instance_ids ) ec2 . terminate_instances ( InstanceIds = instance_ids ) while True : time . sleep ( 0.1 ) a = ec2 . describe_instances ( InstanceIds = instance_ids ) states = [ a [ 'Reservations' ][ 0 ][ 'Instances' ][ i ][ 'State' ][ 'Name' ] for i in range ( len ( instance_ids ))] if \"running\" in states : continue else : break os . remove ( cluster_json )","title":"terminate_cluster"}]}